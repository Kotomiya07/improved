{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081f93d1-7aac-4d6b-8fbf-da227f442a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "\n",
    "args = [\"--dataset\", \"cifar10\", \"--exp\", \"test\", \"--num_channels\", \"4\", \"--num_channels_dae\", \"128\", \"--num_timesteps\", \"4\", \n",
    "\t\t\t\"--num_res_blocks\", \"2\", \"--batch_size\", \"256\", \"--num_epoch\", \"2000\", \"--ngf\", \"64\", \"--nz\", \"50\", \"--z_emb_dim\", \"256\", \"--n_mlp\", \"4\", \"--embedding_type\", \"positional\", \n",
    "\t\t\t\"--use_ema\", \"--ema_decay\", \"0.9999\", \"--r1_gamma\", \"0.02\", \"--lr_d\", \"1.25e-4\", \"--lr_g\", \"1.6e-4\", \"--lazy_reg\", \"15\", \n",
    "\t\t\t\"--ch_mult\", \"1\", \"2\", \"2\", \"--save_content\", \"--datadir\", \"./data/cifar-10\", \n",
    "\t\t\t\"--master_port\", \"6084\", \"--num_process_per_node\", \"1\", \"--save_ckpt_every\", \"25\", \n",
    "\t\t\t\"--current_resolution\", \"16\", \"--attn_resolutions\", \"32\", \"--num_disc_layers\", \"3\",  \"--scale_factor\", \"105.0\", \n",
    "\t\t\t\"--no_lr_decay\", \n",
    "            \"--AutoEncoder_config\", \"autoencoder/config/kl-f2.yaml\", \n",
    "            \"--AutoEncoder_ckpt\", \"autoencoder/weight/kl-f2.ckpt\", \n",
    "\t\t\t\"--rec_loss\",\n",
    "\t\t\t\"--sigmoid_learning\", \n",
    "       ]\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2f41f1-e44e-40ce-84fa-99e55ae10403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "\n",
    "args = [\"--dataset\", \"afhq_cat\", \"--image_size\", \"256\", \"--exp\", \"test2\", \"--num_channels\", \"3\", \"--num_channels_dae\", \"128\", \"--num_timesteps\", \"4\", \n",
    "\t\t\t\"--num_res_blocks\", \"2\", \"--batch_size\", \"32\", \"--num_epoch\", \"2000\", \"--ngf\", \"64\", \"--nz\", \"50\", \"--z_emb_dim\", \"256\", \"--n_mlp\", \"4\", \"--embedding_type\", \"positional\", \n",
    "\t\t\t\"--use_ema\", \"--ema_decay\", \"0.9999\", \"--r1_gamma\", \"0.02\", \"--lr_d\", \"1.25e-4\", \"--lr_g\", \"1.6e-4\", \"--lazy_reg\", \"15\", \n",
    "\t\t\t\"--ch_mult\", \"1\", \"2\", \"2\", \"--save_content\", \"--datadir\", \"./data/afhq\", \n",
    "\t\t\t\"--master_port\", \"6085\", \"--num_process_per_node\", \"1\", \"--save_ckpt_every\", \"25\", \n",
    "\t\t\t\"--current_resolution\", \"64\", \"--attn_resolutions\", \"32\", \"--num_disc_layers\", \"3\",  \"--scale_factor\", \"105.0\", \n",
    "\t\t\t\"--no_lr_decay\", \n",
    "            \"--AutoEncoder_config\", \"autoencoder/config/kl-f4.yaml\", \n",
    "            \"--AutoEncoder_ckpt\", \"autoencoder/weight/kl-f4.ckpt\", \n",
    "\t\t\t\"--rec_loss\",\n",
    "\t\t\t\"--sigmoid_learning\", \n",
    "       ]\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e150ff-a647-4d14-b4d5-ab6d46fe22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"64\",\n",
    "\"--exp\",\"test3\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"4\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"2000\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"50\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.9999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.25e-4\",\n",
    "\"--lr_g\",\"1.6e-4\",\n",
    "\"--lazy_reg\",\"15\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6086\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"32\",\n",
    "\"--num_disc_layers\", \"3\",\n",
    "\"--scale_factor\", \"60.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/kl-f2.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/kl-f2.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414e48d4-68ae-49af-a4ba-86470b9cf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10', 'afhq_cat']:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.sample().detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior.sample())\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17debf4-678d-48e4-94c3-015065223e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 4, 16, 16) = 1024 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration0, G Loss: 0.1910160928964615, D Loss: 1.5979480743408203, alpha: 0.9975273768433652\n",
      "epoch 0 iteration100, G Loss: 2.4215950965881348, D Loss: 1.791154146194458, alpha: 0.9975273768433652\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 1 iteration0, G Loss: 0.9172810912132263, D Loss: 1.4077460765838623, alpha: 0.9975125335223958\n",
      "epoch 1 iteration100, G Loss: 1.7371193170547485, D Loss: 1.665456771850586, alpha: 0.9975125335223958\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 2 iteration0, G Loss: 0.6650135517120361, D Loss: 1.6448270082473755, alpha: 0.997497601319515\n",
      "epoch 2 iteration100, G Loss: 0.8998571634292603, D Loss: 1.4480290412902832, alpha: 0.997497601319515\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 3 iteration0, G Loss: 0.8345134854316711, D Loss: 1.3610022068023682, alpha: 0.9974825797051893\n",
      "epoch 3 iteration100, G Loss: 0.9882712960243225, D Loss: 1.3328356742858887, alpha: 0.9974825797051893\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 4 iteration0, G Loss: 2.6389365196228027, D Loss: 1.4761029481887817, alpha: 0.9974674681467623\n",
      "epoch 4 iteration100, G Loss: 1.1714154481887817, D Loss: 1.3947322368621826, alpha: 0.9974674681467623\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 5 iteration0, G Loss: 0.9958224296569824, D Loss: 1.3462533950805664, alpha: 0.9974522661084374\n",
      "epoch 5 iteration100, G Loss: 1.202440619468689, D Loss: 1.9458179473876953, alpha: 0.9974522661084374\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 6 iteration0, G Loss: 1.4020193815231323, D Loss: 1.4039428234100342, alpha: 0.9974369730512593\n",
      "epoch 6 iteration100, G Loss: 0.7660014033317566, D Loss: 1.3961520195007324, alpha: 0.9974369730512593\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 7 iteration0, G Loss: 0.5437386631965637, D Loss: 1.7483556270599365, alpha: 0.9974215884330963\n",
      "epoch 7 iteration100, G Loss: 1.2861526012420654, D Loss: 1.4825798273086548, alpha: 0.9974215884330963\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 8 iteration0, G Loss: 1.1497281789779663, D Loss: 1.308937907218933, alpha: 0.997406111708621\n",
      "epoch 8 iteration100, G Loss: 1.0756261348724365, D Loss: 1.339104175567627, alpha: 0.997406111708621\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 9 iteration0, G Loss: 0.7745867967605591, D Loss: 1.3474200963974, alpha: 0.997390542329293\n",
      "epoch 9 iteration100, G Loss: 0.8335154056549072, D Loss: 1.336227297782898, alpha: 0.997390542329293\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 10 iteration0, G Loss: 0.9177977442741394, D Loss: 1.4216160774230957, alpha: 0.9973748797433398\n",
      "epoch 10 iteration100, G Loss: 1.1068170070648193, D Loss: 1.399266242980957, alpha: 0.9973748797433398\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 11 iteration0, G Loss: 0.4571862816810608, D Loss: 1.4767029285430908, alpha: 0.9973591233957381\n",
      "epoch 11 iteration100, G Loss: 1.4422847032546997, D Loss: 1.4093513488769531, alpha: 0.9973591233957381\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 12 iteration0, G Loss: 1.0996185541152954, D Loss: 1.3369531631469727, alpha: 0.9973432727281952\n",
      "epoch 12 iteration100, G Loss: 0.7847529053688049, D Loss: 1.3248622417449951, alpha: 0.9973432727281952\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 13 iteration0, G Loss: 1.0988597869873047, D Loss: 1.2947735786437988, alpha: 0.9973273271791304\n",
      "epoch 13 iteration100, G Loss: 1.5022526979446411, D Loss: 1.4686174392700195, alpha: 0.9973273271791304\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 14 iteration0, G Loss: 0.929639458656311, D Loss: 1.7608470916748047, alpha: 0.9973112861836558\n",
      "epoch 14 iteration100, G Loss: 0.9039775729179382, D Loss: 1.4347827434539795, alpha: 0.9973112861836558\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 15 iteration0, G Loss: 1.6734973192214966, D Loss: 1.5423511266708374, alpha: 0.9972951491735572\n",
      "epoch 15 iteration100, G Loss: 0.9121431112289429, D Loss: 1.3298672437667847, alpha: 0.9972951491735572\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 16 iteration0, G Loss: 0.861204206943512, D Loss: 1.460544228553772, alpha: 0.9972789155772751\n",
      "epoch 16 iteration100, G Loss: 1.6068217754364014, D Loss: 1.3465650081634521, alpha: 0.9972789155772751\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 17 iteration0, G Loss: 2.076809883117676, D Loss: 1.5352526903152466, alpha: 0.9972625848198857\n",
      "epoch 17 iteration100, G Loss: 0.8732393383979797, D Loss: 1.349963903427124, alpha: 0.9972625848198857\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 18 iteration0, G Loss: 0.9824247360229492, D Loss: 1.3368420600891113, alpha: 0.997246156323081\n",
      "epoch 18 iteration100, G Loss: 0.7193225622177124, D Loss: 1.4359767436981201, alpha: 0.997246156323081\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 19 iteration0, G Loss: 0.49199753999710083, D Loss: 2.178065299987793, alpha: 0.9972296295051497\n",
      "epoch 19 iteration100, G Loss: 1.0384141206741333, D Loss: 1.486140251159668, alpha: 0.9972296295051497\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 20 iteration0, G Loss: 0.987459123134613, D Loss: 1.3597922325134277, alpha: 0.9972130037809577\n",
      "epoch 20 iteration100, G Loss: 1.047744870185852, D Loss: 1.386771321296692, alpha: 0.9972130037809577\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 21 iteration0, G Loss: 1.337325930595398, D Loss: 1.305467128753662, alpha: 0.9971962785619283\n",
      "epoch 21 iteration100, G Loss: 1.1739907264709473, D Loss: 1.2384881973266602, alpha: 0.9971962785619283\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 22 iteration0, G Loss: 0.9257129430770874, D Loss: 1.2958168983459473, alpha: 0.9971794532560223\n",
      "epoch 22 iteration100, G Loss: 0.9292540550231934, D Loss: 1.3247802257537842, alpha: 0.9971794532560223\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 23 iteration0, G Loss: 1.67599618434906, D Loss: 1.3304154872894287, alpha: 0.9971625272677183\n",
      "epoch 23 iteration100, G Loss: 1.1365784406661987, D Loss: 1.5478748083114624, alpha: 0.9971625272677183\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 24 iteration0, G Loss: 1.10249924659729, D Loss: 1.2739765644073486, alpha: 0.9971454999979927\n",
      "epoch 24 iteration100, G Loss: 1.994053840637207, D Loss: 1.3223309516906738, alpha: 0.9971454999979927\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 25 iteration0, G Loss: 1.4881285429000854, D Loss: 1.6153671741485596, alpha: 0.9971283708442996\n",
      "epoch 25 iteration100, G Loss: 0.9992223978042603, D Loss: 1.3514496088027954, alpha: 0.9971283708442996\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 26 iteration0, G Loss: 1.3848562240600586, D Loss: 1.3383185863494873, alpha: 0.9971111392005504\n",
      "epoch 26 iteration100, G Loss: 1.0326989889144897, D Loss: 1.4763548374176025, alpha: 0.9971111392005504\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 27 iteration0, G Loss: 0.9847499132156372, D Loss: 1.3932626247406006, alpha: 0.9970938044570938\n",
      "epoch 27 iteration100, G Loss: 1.258052945137024, D Loss: 1.3475308418273926, alpha: 0.9970938044570938\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 28 iteration0, G Loss: 0.8723728656768799, D Loss: 1.3777170181274414, alpha: 0.9970763660006952\n",
      "epoch 28 iteration100, G Loss: 0.9540896415710449, D Loss: 1.2799460887908936, alpha: 0.9970763660006952\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 29 iteration0, G Loss: 0.8472197651863098, D Loss: 1.366830587387085, alpha: 0.9970588232145158\n",
      "epoch 29 iteration100, G Loss: 0.9366998076438904, D Loss: 1.3813225030899048, alpha: 0.9970588232145158\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 30 iteration0, G Loss: 1.0758881568908691, D Loss: 1.303957462310791, alpha: 0.9970411754780928\n",
      "epoch 30 iteration100, G Loss: 0.8702118396759033, D Loss: 1.4372773170471191, alpha: 0.9970411754780928\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 31 iteration0, G Loss: 1.013394832611084, D Loss: 1.3415284156799316, alpha: 0.9970234221673178\n",
      "epoch 31 iteration100, G Loss: 0.863053023815155, D Loss: 1.4516980648040771, alpha: 0.9970234221673178\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 32 iteration0, G Loss: 1.0201112031936646, D Loss: 1.353471040725708, alpha: 0.9970055626544164\n",
      "epoch 32 iteration100, G Loss: 1.2193706035614014, D Loss: 1.304002046585083, alpha: 0.9970055626544164\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 33 iteration0, G Loss: 0.9178076386451721, D Loss: 1.4295275211334229, alpha: 0.9969875963079272\n",
      "epoch 33 iteration100, G Loss: 1.916161298751831, D Loss: 1.3493542671203613, alpha: 0.9969875963079272\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 34 iteration0, G Loss: 0.9205597043037415, D Loss: 1.4125474691390991, alpha: 0.9969695224926802\n",
      "epoch 34 iteration100, G Loss: 1.8607840538024902, D Loss: 1.3938391208648682, alpha: 0.9969695224926802\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 35 iteration0, G Loss: 1.530400037765503, D Loss: 1.6608163118362427, alpha: 0.9969513405697763\n",
      "epoch 35 iteration100, G Loss: 1.380983591079712, D Loss: 1.4414212703704834, alpha: 0.9969513405697763\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 36 iteration0, G Loss: 0.7257581353187561, D Loss: 1.341064214706421, alpha: 0.9969330498965654\n",
      "epoch 36 iteration100, G Loss: 0.9044972658157349, D Loss: 1.4812827110290527, alpha: 0.9969330498965654\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 37 iteration0, G Loss: 1.4569015502929688, D Loss: 1.3720595836639404, alpha: 0.9969146498266254\n",
      "epoch 37 iteration100, G Loss: 0.7710740566253662, D Loss: 1.3603246212005615, alpha: 0.9969146498266254\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 38 iteration0, G Loss: 1.6943200826644897, D Loss: 1.3443615436553955, alpha: 0.9968961397097401\n",
      "epoch 38 iteration100, G Loss: 1.109071969985962, D Loss: 1.4199786186218262, alpha: 0.9968961397097401\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 39 iteration0, G Loss: 1.0074046850204468, D Loss: 1.4074405431747437, alpha: 0.9968775188918781\n",
      "epoch 39 iteration100, G Loss: 0.9621743559837341, D Loss: 1.2851243019104004, alpha: 0.9968775188918781\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 40 iteration0, G Loss: 2.2170023918151855, D Loss: 1.6286637783050537, alpha: 0.9968587867151706\n",
      "epoch 40 iteration100, G Loss: 0.9403570890426636, D Loss: 1.3078796863555908, alpha: 0.9968587867151706\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 41 iteration0, G Loss: 0.8008682131767273, D Loss: 1.3067477941513062, alpha: 0.9968399425178895\n",
      "epoch 41 iteration100, G Loss: 0.9983688592910767, D Loss: 1.4445726871490479, alpha: 0.9968399425178895\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 42 iteration0, G Loss: 0.7798043489456177, D Loss: 1.5525007247924805, alpha: 0.9968209856344259\n",
      "epoch 42 iteration100, G Loss: 1.0519498586654663, D Loss: 1.3206731081008911, alpha: 0.9968209856344259\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 43 iteration0, G Loss: 0.9878890514373779, D Loss: 1.3291807174682617, alpha: 0.9968019153952671\n",
      "epoch 43 iteration100, G Loss: 0.983988881111145, D Loss: 1.6300314664840698, alpha: 0.9968019153952671\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 44 iteration0, G Loss: 1.0220986604690552, D Loss: 1.2385568618774414, alpha: 0.9967827311269749\n",
      "epoch 44 iteration100, G Loss: 0.6128252744674683, D Loss: 1.3993314504623413, alpha: 0.9967827311269749\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 45 iteration0, G Loss: 1.0450385808944702, D Loss: 1.3868521451950073, alpha: 0.9967634321521631\n",
      "epoch 45 iteration100, G Loss: 1.5298014879226685, D Loss: 1.3290767669677734, alpha: 0.9967634321521631\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 46 iteration0, G Loss: 1.2629855871200562, D Loss: 1.314028263092041, alpha: 0.9967440177894752\n",
      "epoch 46 iteration100, G Loss: 0.8717383742332458, D Loss: 1.4108591079711914, alpha: 0.9967440177894752\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 47 iteration0, G Loss: 0.9991406798362732, D Loss: 1.3763790130615234, alpha: 0.996724487353561\n",
      "epoch 47 iteration100, G Loss: 1.2156744003295898, D Loss: 1.3380753993988037, alpha: 0.996724487353561\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 48 iteration0, G Loss: 0.9678480625152588, D Loss: 1.383742094039917, alpha: 0.9967048401550548\n",
      "epoch 48 iteration100, G Loss: 1.057064414024353, D Loss: 1.3769830465316772, alpha: 0.9967048401550548\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 49 iteration0, G Loss: 1.0389786958694458, D Loss: 1.309625267982483, alpha: 0.9966850755005522\n",
      "epoch 49 iteration100, G Loss: 0.7682986259460449, D Loss: 1.9982507228851318, alpha: 0.9966850755005522\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 50 iteration0, G Loss: 0.7705880999565125, D Loss: 1.6762888431549072, alpha: 0.9966651926925867\n",
      "epoch 50 iteration100, G Loss: 1.299242615699768, D Loss: 1.5635485649108887, alpha: 0.9966651926925867\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 51 iteration0, G Loss: 1.4012033939361572, D Loss: 1.5815929174423218, alpha: 0.996645191029607\n",
      "epoch 51 iteration100, G Loss: 2.1758949756622314, D Loss: 1.3093105554580688, alpha: 0.996645191029607\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 52 iteration0, G Loss: 0.6870719790458679, D Loss: 1.3317809104919434, alpha: 0.9966250698059539\n",
      "epoch 52 iteration100, G Loss: 0.7624169588088989, D Loss: 1.272734522819519, alpha: 0.9966250698059539\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 53 iteration0, G Loss: 1.3948142528533936, D Loss: 1.3876893520355225, alpha: 0.9966048283118364\n",
      "epoch 53 iteration100, G Loss: 0.9989495873451233, D Loss: 1.373335361480713, alpha: 0.9966048283118364\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 54 iteration0, G Loss: 0.8769404888153076, D Loss: 1.4102628231048584, alpha: 0.9965844658333086\n",
      "epoch 54 iteration100, G Loss: 0.7562993168830872, D Loss: 1.3481659889221191, alpha: 0.9965844658333086\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 55 iteration0, G Loss: 0.9161584377288818, D Loss: 1.46720290184021, alpha: 0.9965639816522461\n",
      "epoch 55 iteration100, G Loss: 2.512873888015747, D Loss: 1.112054467201233, alpha: 0.9965639816522461\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 56 iteration0, G Loss: 0.893752932548523, D Loss: 1.3207257986068726, alpha: 0.9965433750463222\n",
      "epoch 56 iteration100, G Loss: 1.2995141744613647, D Loss: 1.4536335468292236, alpha: 0.9965433750463222\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 57 iteration0, G Loss: 1.1227420568466187, D Loss: 1.4062288999557495, alpha: 0.9965226452889837\n",
      "epoch 57 iteration100, G Loss: 0.867809534072876, D Loss: 1.3817694187164307, alpha: 0.9965226452889837\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 58 iteration0, G Loss: 1.2552136182785034, D Loss: 1.4076573848724365, alpha: 0.9965017916494276\n",
      "epoch 58 iteration100, G Loss: 0.9683583974838257, D Loss: 1.339376449584961, alpha: 0.9965017916494276\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 59 iteration0, G Loss: 1.9142366647720337, D Loss: 1.2184325456619263, alpha: 0.9964808133925762\n",
      "epoch 59 iteration100, G Loss: 1.3679835796356201, D Loss: 1.3061093091964722, alpha: 0.9964808133925762\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 60 iteration0, G Loss: 0.9834100008010864, D Loss: 1.4380784034729004, alpha: 0.9964597097790535\n",
      "epoch 60 iteration100, G Loss: 0.9421017169952393, D Loss: 1.291295051574707, alpha: 0.9964597097790535\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 61 iteration0, G Loss: 0.9534580111503601, D Loss: 1.3837138414382935, alpha: 0.9964384800651604\n",
      "epoch 61 iteration100, G Loss: 0.8541001081466675, D Loss: 1.4024238586425781, alpha: 0.9964384800651604\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 62 iteration0, G Loss: 1.1276718378067017, D Loss: 1.340299129486084, alpha: 0.9964171235028505\n",
      "epoch 62 iteration100, G Loss: 1.1639858484268188, D Loss: 1.5257056951522827, alpha: 0.9964171235028505\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 63 iteration0, G Loss: 0.8638678789138794, D Loss: 1.3086292743682861, alpha: 0.9963956393397052\n",
      "epoch 63 iteration100, G Loss: 0.9521434307098389, D Loss: 1.415266990661621, alpha: 0.9963956393397052\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 64 iteration0, G Loss: 1.0700199604034424, D Loss: 1.2935166358947754, alpha: 0.9963740268189091\n",
      "epoch 64 iteration100, G Loss: 1.198035478591919, D Loss: 1.4175313711166382, alpha: 0.9963740268189091\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 65 iteration0, G Loss: 0.9956461191177368, D Loss: 1.39602792263031, alpha: 0.996352285179225\n",
      "epoch 65 iteration100, G Loss: 1.0899282693862915, D Loss: 1.2131214141845703, alpha: 0.996352285179225\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 66 iteration0, G Loss: 1.6830253601074219, D Loss: 1.6674443483352661, alpha: 0.9963304136549692\n",
      "epoch 66 iteration100, G Loss: 1.2472089529037476, D Loss: 1.3660027980804443, alpha: 0.9963304136549692\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 67 iteration0, G Loss: 0.9758114814758301, D Loss: 1.1109404563903809, alpha: 0.9963084114759856\n",
      "epoch 67 iteration100, G Loss: 0.844146192073822, D Loss: 1.163015365600586, alpha: 0.9963084114759856\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 68 iteration0, G Loss: 1.1020300388336182, D Loss: 1.2471630573272705, alpha: 0.9962862778676213\n",
      "epoch 68 iteration100, G Loss: 1.1096810102462769, D Loss: 1.334596037864685, alpha: 0.9962862778676213\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 69 iteration0, G Loss: 0.7500141859054565, D Loss: 1.2868971824645996, alpha: 0.9962640120507004\n",
      "epoch 69 iteration100, G Loss: 1.0156913995742798, D Loss: 1.274876594543457, alpha: 0.9962640120507004\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 70 iteration0, G Loss: 1.4628052711486816, D Loss: 1.3482615947723389, alpha: 0.9962416132414992\n",
      "epoch 70 iteration100, G Loss: 1.9421734809875488, D Loss: 1.2884770631790161, alpha: 0.9962416132414992\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 71 iteration0, G Loss: 1.2219496965408325, D Loss: 1.3393230438232422, alpha: 0.9962190806517198\n",
      "epoch 71 iteration100, G Loss: 1.236377239227295, D Loss: 1.2159249782562256, alpha: 0.9962190806517198\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 72 iteration0, G Loss: 1.5650731325149536, D Loss: 1.325643539428711, alpha: 0.9961964134884649\n",
      "epoch 72 iteration100, G Loss: 1.2142447233200073, D Loss: 1.3197412490844727, alpha: 0.9961964134884649\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 73 iteration0, G Loss: 1.3336765766143799, D Loss: 1.2608546018600464, alpha: 0.9961736109542111\n",
      "epoch 73 iteration100, G Loss: 1.4447184801101685, D Loss: 1.279843807220459, alpha: 0.9961736109542111\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 74 iteration0, G Loss: 1.2821367979049683, D Loss: 1.2318439483642578, alpha: 0.9961506722467834\n",
      "epoch 74 iteration100, G Loss: 1.3098870515823364, D Loss: 1.3486613035202026, alpha: 0.9961506722467834\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 75 iteration0, G Loss: 1.6049606800079346, D Loss: 1.309981107711792, alpha: 0.9961275965593289\n",
      "epoch 75 iteration100, G Loss: 1.403041958808899, D Loss: 1.2200343608856201, alpha: 0.9961275965593289\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 76 iteration0, G Loss: 0.9299681186676025, D Loss: 1.2219771146774292, alpha: 0.9961043830802904\n",
      "epoch 76 iteration100, G Loss: 1.245540976524353, D Loss: 1.298771858215332, alpha: 0.9961043830802904\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 77 iteration0, G Loss: 1.2151066064834595, D Loss: 1.2732288837432861, alpha: 0.9960810309933794\n",
      "epoch 77 iteration100, G Loss: 0.9827873706817627, D Loss: 1.3576350212097168, alpha: 0.9960810309933794\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 78 iteration0, G Loss: 1.2407190799713135, D Loss: 1.169468879699707, alpha: 0.9960575394775504\n",
      "epoch 78 iteration100, G Loss: 0.7378765344619751, D Loss: 1.5085164308547974, alpha: 0.9960575394775504\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 79 iteration0, G Loss: 1.1136672496795654, D Loss: 1.3361271619796753, alpha: 0.996033907706973\n",
      "epoch 79 iteration100, G Loss: 1.5316362380981445, D Loss: 1.5486019849777222, alpha: 0.996033907706973\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 80 iteration0, G Loss: 1.1087361574172974, D Loss: 1.276216745376587, alpha: 0.9960101348510059\n",
      "epoch 80 iteration100, G Loss: 2.1065800189971924, D Loss: 1.2663532495498657, alpha: 0.9960101348510059\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 81 iteration0, G Loss: 0.832314670085907, D Loss: 1.3458770513534546, alpha: 0.9959862200741696\n",
      "epoch 81 iteration100, G Loss: 1.362358808517456, D Loss: 1.3541133403778076, alpha: 0.9959862200741696\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 82 iteration0, G Loss: 1.0522936582565308, D Loss: 1.1885919570922852, alpha: 0.9959621625361187\n",
      "epoch 82 iteration100, G Loss: 1.02851140499115, D Loss: 1.275001049041748, alpha: 0.9959621625361187\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 83 iteration0, G Loss: 0.849270761013031, D Loss: 1.3668553829193115, alpha: 0.9959379613916154\n",
      "epoch 83 iteration100, G Loss: 0.4946569502353668, D Loss: 1.263048529624939, alpha: 0.9959379613916154\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 84 iteration0, G Loss: 1.0442596673965454, D Loss: 1.1019365787506104, alpha: 0.9959136157905011\n",
      "epoch 84 iteration100, G Loss: 0.735486626625061, D Loss: 1.2122467756271362, alpha: 0.9959136157905011\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 85 iteration0, G Loss: 1.2510758638381958, D Loss: 1.3164258003234863, alpha: 0.9958891248776693\n",
      "epoch 85 iteration100, G Loss: 1.3770772218704224, D Loss: 1.2131576538085938, alpha: 0.9958891248776693\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 86 iteration0, G Loss: 0.749917209148407, D Loss: 1.1726109981536865, alpha: 0.9958644877930382\n",
      "epoch 86 iteration100, G Loss: 0.8622560501098633, D Loss: 1.9823551177978516, alpha: 0.9958644877930382\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 87 iteration0, G Loss: 1.8499634265899658, D Loss: 1.7234078645706177, alpha: 0.9958397036715217\n",
      "epoch 87 iteration100, G Loss: 1.1828984022140503, D Loss: 1.426867961883545, alpha: 0.9958397036715217\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 88 iteration0, G Loss: 1.3778131008148193, D Loss: 1.2251359224319458, alpha: 0.9958147716430024\n",
      "epoch 88 iteration100, G Loss: 0.943977952003479, D Loss: 1.2849845886230469, alpha: 0.9958147716430024\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 89 iteration0, G Loss: 0.8413410186767578, D Loss: 1.3889012336730957, alpha: 0.9957896908323025\n",
      "epoch 89 iteration100, G Loss: 1.093980312347412, D Loss: 1.098962426185608, alpha: 0.9957896908323025\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 90 iteration0, G Loss: 1.2698101997375488, D Loss: 1.243577480316162, alpha: 0.9957644603591566\n",
      "epoch 90 iteration100, G Loss: 0.6527689099311829, D Loss: 1.2966275215148926, alpha: 0.9957644603591566\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 91 iteration0, G Loss: 1.7416356801986694, D Loss: 1.0336055755615234, alpha: 0.9957390793381818\n",
      "epoch 91 iteration100, G Loss: 0.6961186528205872, D Loss: 1.247093915939331, alpha: 0.9957390793381818\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 92 iteration0, G Loss: 0.7112804055213928, D Loss: 1.3578530550003052, alpha: 0.9957135468788503\n",
      "epoch 92 iteration100, G Loss: 1.3598837852478027, D Loss: 1.272364616394043, alpha: 0.9957135468788503\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 93 iteration0, G Loss: 1.892794132232666, D Loss: 1.3485913276672363, alpha: 0.9956878620854597\n",
      "epoch 93 iteration100, G Loss: 1.1044965982437134, D Loss: 1.2348685264587402, alpha: 0.9956878620854597\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 94 iteration0, G Loss: 1.0548895597457886, D Loss: 1.4632846117019653, alpha: 0.9956620240571046\n",
      "epoch 94 iteration100, G Loss: 0.7194807529449463, D Loss: 1.44798743724823, alpha: 0.9956620240571046\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 95 iteration0, G Loss: 0.9329891204833984, D Loss: 1.3071051836013794, alpha: 0.9956360318876475\n",
      "epoch 95 iteration100, G Loss: 1.187981128692627, D Loss: 1.167540431022644, alpha: 0.9956360318876475\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 96 iteration0, G Loss: 0.6534550786018372, D Loss: 1.4950346946716309, alpha: 0.995609884665689\n",
      "epoch 96 iteration100, G Loss: 1.0529547929763794, D Loss: 1.3444623947143555, alpha: 0.995609884665689\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 97 iteration0, G Loss: 1.9246898889541626, D Loss: 1.0946402549743652, alpha: 0.9955835814745394\n",
      "epoch 97 iteration100, G Loss: 0.9761373996734619, D Loss: 1.4017146825790405, alpha: 0.9955835814745394\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 98 iteration0, G Loss: 1.2359938621520996, D Loss: 1.2001664638519287, alpha: 0.9955571213921881\n",
      "epoch 98 iteration100, G Loss: 0.9036486148834229, D Loss: 1.3714206218719482, alpha: 0.9955571213921881\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 99 iteration0, G Loss: 0.9525555968284607, D Loss: 1.2100774049758911, alpha: 0.9955305034912747\n",
      "epoch 99 iteration100, G Loss: 0.6191349029541016, D Loss: 1.3819235563278198, alpha: 0.9955305034912747\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 100 iteration0, G Loss: 1.2891955375671387, D Loss: 1.3755159378051758, alpha: 0.9955037268390589\n",
      "epoch 100 iteration100, G Loss: 0.9599674940109253, D Loss: 1.872506022453308, alpha: 0.9955037268390589\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 101 iteration0, G Loss: 1.0887434482574463, D Loss: 1.2605621814727783, alpha: 0.99547679049739\n",
      "epoch 101 iteration100, G Loss: 1.334281086921692, D Loss: 1.324193000793457, alpha: 0.99547679049739\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 102 iteration0, G Loss: 1.2880712747573853, D Loss: 1.0804879665374756, alpha: 0.9954496935226781\n",
      "epoch 102 iteration100, G Loss: 0.9760153293609619, D Loss: 1.2252893447875977, alpha: 0.9954496935226781\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 103 iteration0, G Loss: 1.1480597257614136, D Loss: 1.3765597343444824, alpha: 0.9954224349658624\n",
      "epoch 103 iteration100, G Loss: 1.2515498399734497, D Loss: 1.2429330348968506, alpha: 0.9954224349658624\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 104 iteration0, G Loss: 1.231377363204956, D Loss: 1.290461778640747, alpha: 0.9953950138723812\n",
      "epoch 104 iteration100, G Loss: 0.9631686806678772, D Loss: 1.3143906593322754, alpha: 0.9953950138723812\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 105 iteration0, G Loss: 1.6940598487854004, D Loss: 1.429856538772583, alpha: 0.9953674292821418\n",
      "epoch 105 iteration100, G Loss: 1.5079580545425415, D Loss: 1.4883582592010498, alpha: 0.9953674292821418\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 106 iteration0, G Loss: 1.2226938009262085, D Loss: 1.189838171005249, alpha: 0.9953396802294893\n",
      "epoch 106 iteration100, G Loss: 0.9327266216278076, D Loss: 1.4211844205856323, alpha: 0.9953396802294893\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 107 iteration0, G Loss: 1.5083458423614502, D Loss: 1.458986759185791, alpha: 0.9953117657431754\n",
      "epoch 107 iteration100, G Loss: 1.0220065116882324, D Loss: 1.248381495475769, alpha: 0.9953117657431754\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 108 iteration0, G Loss: 0.6926584243774414, D Loss: 1.2773265838623047, alpha: 0.9952836848463282\n",
      "epoch 108 iteration100, G Loss: 1.423586368560791, D Loss: 1.261828064918518, alpha: 0.9952836848463282\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 109 iteration0, G Loss: 1.5201997756958008, D Loss: 1.3979510068893433, alpha: 0.99525543655642\n",
      "epoch 109 iteration100, G Loss: 0.7775574326515198, D Loss: 1.1902179718017578, alpha: 0.99525543655642\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 110 iteration0, G Loss: 1.2385714054107666, D Loss: 1.2411653995513916, alpha: 0.9952270198852368\n",
      "epoch 110 iteration100, G Loss: 1.110825777053833, D Loss: 1.230257272720337, alpha: 0.9952270198852368\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 111 iteration0, G Loss: 0.9365814924240112, D Loss: 1.3089592456817627, alpha: 0.995198433838846\n",
      "epoch 111 iteration100, G Loss: 1.4876121282577515, D Loss: 1.2761582136154175, alpha: 0.995198433838846\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 112 iteration0, G Loss: 1.2359803915023804, D Loss: 1.397479772567749, alpha: 0.9951696774175651\n",
      "epoch 112 iteration100, G Loss: 1.414320945739746, D Loss: 1.1308009624481201, alpha: 0.9951696774175651\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 113 iteration0, G Loss: 1.0409035682678223, D Loss: 1.2200371026992798, alpha: 0.9951407496159302\n",
      "epoch 113 iteration100, G Loss: 1.4113270044326782, D Loss: 1.522934913635254, alpha: 0.9951407496159302\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 114 iteration0, G Loss: 1.1077721118927002, D Loss: 1.0771645307540894, alpha: 0.9951116494226631\n",
      "epoch 114 iteration100, G Loss: 1.1374351978302002, D Loss: 1.182259440422058, alpha: 0.9951116494226631\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 115 iteration0, G Loss: 2.580092191696167, D Loss: 2.2031965255737305, alpha: 0.9950823758206396\n",
      "epoch 115 iteration100, G Loss: 1.1133173704147339, D Loss: 1.1663625240325928, alpha: 0.9950823758206396\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 116 iteration0, G Loss: 1.6981712579727173, D Loss: 0.8805440664291382, alpha: 0.9950529277868578\n",
      "epoch 116 iteration100, G Loss: 2.2495388984680176, D Loss: 1.070338249206543, alpha: 0.9950529277868578\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 117 iteration0, G Loss: 1.1961675882339478, D Loss: 1.2197530269622803, alpha: 0.9950233042924043\n",
      "epoch 117 iteration100, G Loss: 1.276694893836975, D Loss: 1.2734037637710571, alpha: 0.9950233042924043\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 118 iteration0, G Loss: 1.2069737911224365, D Loss: 1.2674338817596436, alpha: 0.9949935043024226\n",
      "epoch 118 iteration100, G Loss: 1.3182387351989746, D Loss: 1.351619005203247, alpha: 0.9949935043024226\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 119 iteration0, G Loss: 0.97005295753479, D Loss: 1.3708442449569702, alpha: 0.9949635267760798\n",
      "epoch 119 iteration100, G Loss: 1.3709897994995117, D Loss: 1.205488681793213, alpha: 0.9949635267760798\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 120 iteration0, G Loss: 0.8457266092300415, D Loss: 1.360309362411499, alpha: 0.9949333706665338\n",
      "epoch 120 iteration100, G Loss: 0.9962816834449768, D Loss: 1.1673078536987305, alpha: 0.9949333706665338\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 121 iteration0, G Loss: 0.9226139783859253, D Loss: 1.3187155723571777, alpha: 0.9949030349208999\n",
      "epoch 121 iteration100, G Loss: 0.8440066576004028, D Loss: 1.1554611921310425, alpha: 0.9949030349208999\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 122 iteration0, G Loss: 1.7108039855957031, D Loss: 1.3003413677215576, alpha: 0.9948725184802181\n",
      "epoch 122 iteration100, G Loss: 1.197716236114502, D Loss: 1.199812650680542, alpha: 0.9948725184802181\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 123 iteration0, G Loss: 1.192132592201233, D Loss: 1.1836304664611816, alpha: 0.9948418202794187\n",
      "epoch 123 iteration100, G Loss: 2.285090684890747, D Loss: 1.2724534273147583, alpha: 0.9948418202794187\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 124 iteration0, G Loss: 1.0597543716430664, D Loss: 1.3357566595077515, alpha: 0.9948109392472895\n",
      "epoch 124 iteration100, G Loss: 1.307647466659546, D Loss: 1.3379840850830078, alpha: 0.9948109392472895\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 125 iteration0, G Loss: 2.4593260288238525, D Loss: 1.1911842823028564, alpha: 0.9947798743064415\n",
      "epoch 125 iteration100, G Loss: 1.5697376728057861, D Loss: 1.316082239151001, alpha: 0.9947798743064415\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 126 iteration0, G Loss: 1.411118984222412, D Loss: 1.2972681522369385, alpha: 0.9947486243732755\n",
      "epoch 126 iteration100, G Loss: 2.380340814590454, D Loss: 1.0373430252075195, alpha: 0.9947486243732755\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 127 iteration0, G Loss: 1.31471586227417, D Loss: 1.11911940574646, alpha: 0.994717188357947\n",
      "epoch 127 iteration100, G Loss: 1.82383394241333, D Loss: 1.6415406465530396, alpha: 0.994717188357947\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 128 iteration0, G Loss: 1.4838443994522095, D Loss: 1.5562670230865479, alpha: 0.9946855651643326\n",
      "epoch 128 iteration100, G Loss: 0.9910386800765991, D Loss: 1.4508225917816162, alpha: 0.9946855651643326\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 129 iteration0, G Loss: 1.5371477603912354, D Loss: 0.9115701913833618, alpha: 0.9946537536899958\n",
      "epoch 129 iteration100, G Loss: 1.2772296667099, D Loss: 1.194309949874878, alpha: 0.9946537536899958\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 130 iteration0, G Loss: 1.254160761833191, D Loss: 1.239776611328125, alpha: 0.9946217528261514\n",
      "epoch 130 iteration100, G Loss: 1.443385124206543, D Loss: 1.323879599571228, alpha: 0.9946217528261514\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 131 iteration0, G Loss: 1.2259382009506226, D Loss: 1.1015887260437012, alpha: 0.9945895614576316\n",
      "epoch 131 iteration100, G Loss: 1.3210268020629883, D Loss: 1.609612226486206, alpha: 0.9945895614576316\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 132 iteration0, G Loss: 1.6774535179138184, D Loss: 1.3797271251678467, alpha: 0.9945571784628505\n",
      "epoch 132 iteration100, G Loss: 1.2853776216506958, D Loss: 1.00877046585083, alpha: 0.9945571784628505\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 133 iteration0, G Loss: 0.9161839485168457, D Loss: 1.42500638961792, alpha: 0.9945246027137692\n",
      "epoch 133 iteration100, G Loss: 1.0340805053710938, D Loss: 1.2309529781341553, alpha: 0.9945246027137692\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 134 iteration0, G Loss: 1.6246551275253296, D Loss: 1.4055742025375366, alpha: 0.9944918330758603\n",
      "epoch 134 iteration100, G Loss: 1.0684698820114136, D Loss: 1.4197561740875244, alpha: 0.9944918330758603\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 135 iteration0, G Loss: 1.4400469064712524, D Loss: 0.9521632194519043, alpha: 0.9944588684080726\n",
      "epoch 135 iteration100, G Loss: 1.3252308368682861, D Loss: 1.3352477550506592, alpha: 0.9944588684080726\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 136 iteration0, G Loss: 1.2159851789474487, D Loss: 1.3985469341278076, alpha: 0.9944257075627952\n",
      "epoch 136 iteration100, G Loss: 1.274964690208435, D Loss: 1.2490458488464355, alpha: 0.9944257075627952\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 137 iteration0, G Loss: 1.4346920251846313, D Loss: 1.070275068283081, alpha: 0.994392349385822\n",
      "epoch 137 iteration100, G Loss: 1.4433343410491943, D Loss: 1.1112947463989258, alpha: 0.994392349385822\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 138 iteration0, G Loss: 1.3024134635925293, D Loss: 1.2914247512817383, alpha: 0.9943587927163153\n",
      "epoch 138 iteration100, G Loss: 2.0077004432678223, D Loss: 1.5572410821914673, alpha: 0.9943587927163153\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 139 iteration0, G Loss: 0.853666365146637, D Loss: 1.1671279668807983, alpha: 0.99432503638677\n",
      "epoch 139 iteration100, G Loss: 0.9965457916259766, D Loss: 1.4013779163360596, alpha: 0.99432503638677\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 140 iteration0, G Loss: 0.8541104197502136, D Loss: 1.6241731643676758, alpha: 0.9942910792229767\n",
      "epoch 140 iteration100, G Loss: 1.2205818891525269, D Loss: 1.3703961372375488, alpha: 0.9942910792229767\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 141 iteration0, G Loss: 0.7764196991920471, D Loss: 1.410089135169983, alpha: 0.9942569200439859\n",
      "epoch 141 iteration100, G Loss: 1.3147112131118774, D Loss: 1.183669090270996, alpha: 0.9942569200439859\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 142 iteration0, G Loss: 1.388945460319519, D Loss: 1.2607533931732178, alpha: 0.9942225576620709\n",
      "epoch 142 iteration100, G Loss: 1.356203317642212, D Loss: 1.2828785181045532, alpha: 0.9942225576620709\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 143 iteration0, G Loss: 0.9629521369934082, D Loss: 1.1119905710220337, alpha: 0.9941879908826907\n",
      "epoch 143 iteration100, G Loss: 0.6095144748687744, D Loss: 2.4648172855377197, alpha: 0.9941879908826907\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 144 iteration0, G Loss: 1.8511463403701782, D Loss: 1.23744535446167, alpha: 0.9941532185044534\n",
      "epoch 144 iteration100, G Loss: 1.4695767164230347, D Loss: 1.4192397594451904, alpha: 0.9941532185044534\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 145 iteration0, G Loss: 1.2530696392059326, D Loss: 1.2645390033721924, alpha: 0.9941182393190785\n",
      "epoch 145 iteration100, G Loss: 1.687155842781067, D Loss: 1.3754421472549438, alpha: 0.9941182393190785\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 146 iteration0, G Loss: 1.6564239263534546, D Loss: 1.335955023765564, alpha: 0.99408305211136\n",
      "epoch 146 iteration100, G Loss: 1.4053038358688354, D Loss: 1.2130857706069946, alpha: 0.99408305211136\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 147 iteration0, G Loss: 0.9932115077972412, D Loss: 1.4792208671569824, alpha: 0.9940476556591286\n",
      "epoch 147 iteration100, G Loss: 1.8579859733581543, D Loss: 1.307376742362976, alpha: 0.9940476556591286\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 148 iteration0, G Loss: 2.213970422744751, D Loss: 1.483346939086914, alpha: 0.9940120487332132\n",
      "epoch 148 iteration100, G Loss: 1.6282984018325806, D Loss: 0.9561702013015747, alpha: 0.9940120487332132\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 149 iteration0, G Loss: 0.8569661378860474, D Loss: 1.2212014198303223, alpha: 0.9939762300974047\n",
      "epoch 149 iteration100, G Loss: 0.908663272857666, D Loss: 1.0797243118286133, alpha: 0.9939762300974047\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 150 iteration0, G Loss: 0.9341819882392883, D Loss: 1.9124870300292969, alpha: 0.9939401985084159\n",
      "epoch 150 iteration100, G Loss: 1.9100935459136963, D Loss: 1.0745664834976196, alpha: 0.9939401985084159\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 151 iteration0, G Loss: 1.590873122215271, D Loss: 1.3129761219024658, alpha: 0.9939039527158445\n",
      "epoch 151 iteration100, G Loss: 1.6627146005630493, D Loss: 1.2195260524749756, alpha: 0.9939039527158445\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 152 iteration0, G Loss: 0.7445769309997559, D Loss: 2.0765483379364014, alpha: 0.9938674914621343\n",
      "epoch 152 iteration100, G Loss: 1.0926021337509155, D Loss: 1.1743214130401611, alpha: 0.9938674914621343\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 153 iteration0, G Loss: 1.6110721826553345, D Loss: 1.0775790214538574, alpha: 0.9938308134825361\n",
      "epoch 153 iteration100, G Loss: 1.5273975133895874, D Loss: 1.2941712141036987, alpha: 0.9938308134825361\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 154 iteration0, G Loss: 1.2551755905151367, D Loss: 0.9133037328720093, alpha: 0.9937939175050695\n",
      "epoch 154 iteration100, G Loss: 1.2561237812042236, D Loss: 1.1837666034698486, alpha: 0.9937939175050695\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 155 iteration0, G Loss: 1.5600416660308838, D Loss: 1.301741600036621, alpha: 0.9937568022504835\n",
      "epoch 155 iteration100, G Loss: 1.1338497400283813, D Loss: 1.2503023147583008, alpha: 0.9937568022504835\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 156 iteration0, G Loss: 1.0127394199371338, D Loss: 1.3556828498840332, alpha: 0.9937194664322172\n",
      "epoch 156 iteration100, G Loss: 1.530402660369873, D Loss: 1.230217695236206, alpha: 0.9937194664322172\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 157 iteration0, G Loss: 1.3172006607055664, D Loss: 1.2590248584747314, alpha: 0.9936819087563606\n",
      "epoch 157 iteration100, G Loss: 1.1830365657806396, D Loss: 1.3818254470825195, alpha: 0.9936819087563606\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 158 iteration0, G Loss: 1.5197982788085938, D Loss: 1.3185920715332031, alpha: 0.9936441279216154\n",
      "epoch 158 iteration100, G Loss: 1.3737647533416748, D Loss: 1.2436447143554688, alpha: 0.9936441279216154\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 159 iteration0, G Loss: 1.8212616443634033, D Loss: 1.1097517013549805, alpha: 0.9936061226192542\n",
      "epoch 159 iteration100, G Loss: 1.1582165956497192, D Loss: 1.1877365112304688, alpha: 0.9936061226192542\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 160 iteration0, G Loss: 1.2248505353927612, D Loss: 1.2088004350662231, alpha: 0.9935678915330813\n",
      "epoch 160 iteration100, G Loss: 1.6157655715942383, D Loss: 1.4050661325454712, alpha: 0.9935678915330813\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 161 iteration0, G Loss: 1.1306184530258179, D Loss: 1.2864998579025269, alpha: 0.9935294333393929\n",
      "epoch 161 iteration100, G Loss: 0.992128849029541, D Loss: 1.2356607913970947, alpha: 0.9935294333393929\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 162 iteration0, G Loss: 1.2308440208435059, D Loss: 1.2352876663208008, alpha: 0.9934907467069358\n",
      "epoch 162 iteration100, G Loss: 2.0900754928588867, D Loss: 1.199239730834961, alpha: 0.9934907467069358\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 163 iteration0, G Loss: 1.9869920015335083, D Loss: 1.0669926404953003, alpha: 0.9934518302968676\n",
      "epoch 163 iteration100, G Loss: 1.067868947982788, D Loss: 1.256643295288086, alpha: 0.9934518302968676\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 164 iteration0, G Loss: 1.3487465381622314, D Loss: 1.2493475675582886, alpha: 0.9934126827627158\n",
      "epoch 164 iteration100, G Loss: 1.0407814979553223, D Loss: 1.191542625427246, alpha: 0.9934126827627158\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 165 iteration0, G Loss: 1.5012906789779663, D Loss: 1.2776912450790405, alpha: 0.9933733027503371\n",
      "epoch 165 iteration100, G Loss: 1.425959587097168, D Loss: 1.0366004705429077, alpha: 0.9933733027503371\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 166 iteration0, G Loss: 1.08977472782135, D Loss: 1.209643840789795, alpha: 0.9933336888978759\n",
      "epoch 166 iteration100, G Loss: 1.7646652460098267, D Loss: 1.1554385423660278, alpha: 0.9933336888978759\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 167 iteration0, G Loss: 1.2146432399749756, D Loss: 1.1552797555923462, alpha: 0.9932938398357235\n",
      "epoch 167 iteration100, G Loss: 1.1134248971939087, D Loss: 1.0042188167572021, alpha: 0.9932938398357235\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 168 iteration0, G Loss: 0.8043373823165894, D Loss: 1.3152039051055908, alpha: 0.9932537541864765\n",
      "epoch 168 iteration100, G Loss: 1.519742488861084, D Loss: 1.2394219636917114, alpha: 0.9932537541864765\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 169 iteration0, G Loss: 1.404723882675171, D Loss: 1.1328588724136353, alpha: 0.9932134305648953\n",
      "epoch 169 iteration100, G Loss: 1.2078813314437866, D Loss: 1.8607330322265625, alpha: 0.9932134305648953\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 170 iteration0, G Loss: 1.7518508434295654, D Loss: 1.4024238586425781, alpha: 0.9931728675778618\n",
      "epoch 170 iteration100, G Loss: 1.2638283967971802, D Loss: 1.537712574005127, alpha: 0.9931728675778618\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 171 iteration0, G Loss: 1.4357211589813232, D Loss: 1.0075385570526123, alpha: 0.993132063824338\n",
      "epoch 171 iteration100, G Loss: 1.0237939357757568, D Loss: 1.3257346153259277, alpha: 0.993132063824338\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 172 iteration0, G Loss: 0.7543215751647949, D Loss: 1.0640959739685059, alpha: 0.9930910178953235\n",
      "epoch 172 iteration100, G Loss: 2.2446043491363525, D Loss: 1.5954679250717163, alpha: 0.9930910178953235\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 173 iteration0, G Loss: 1.0282992124557495, D Loss: 1.539250135421753, alpha: 0.9930497283738132\n",
      "epoch 173 iteration100, G Loss: 0.5789446234703064, D Loss: 1.0857499837875366, alpha: 0.9930497283738132\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 174 iteration0, G Loss: 1.5966928005218506, D Loss: 0.9256841540336609, alpha: 0.9930081938347545\n",
      "epoch 174 iteration100, G Loss: 1.4482531547546387, D Loss: 1.1986041069030762, alpha: 0.9930081938347545\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 175 iteration0, G Loss: 1.791526436805725, D Loss: 1.3466811180114746, alpha: 0.9929664128450049\n",
      "epoch 175 iteration100, G Loss: 1.73674738407135, D Loss: 1.3866490125656128, alpha: 0.9929664128450049\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 176 iteration0, G Loss: 1.8538106679916382, D Loss: 1.380448341369629, alpha: 0.9929243839632887\n",
      "epoch 176 iteration100, G Loss: 0.7655876874923706, D Loss: 1.4340903759002686, alpha: 0.9929243839632887\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 177 iteration0, G Loss: 1.9333698749542236, D Loss: 1.2346817255020142, alpha: 0.9928821057401542\n",
      "epoch 177 iteration100, G Loss: 3.04548716545105, D Loss: 1.738771915435791, alpha: 0.9928821057401542\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 178 iteration0, G Loss: 2.103890895843506, D Loss: 1.1941909790039062, alpha: 0.9928395767179302\n",
      "epoch 178 iteration100, G Loss: 1.2063627243041992, D Loss: 1.3037946224212646, alpha: 0.9928395767179302\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 179 iteration0, G Loss: 1.122464656829834, D Loss: 1.2515734434127808, alpha: 0.992796795430682\n",
      "epoch 179 iteration100, G Loss: 0.918687105178833, D Loss: 1.2807948589324951, alpha: 0.992796795430682\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 180 iteration0, G Loss: 1.2286206483840942, D Loss: 1.1316626071929932, alpha: 0.9927537604041685\n",
      "epoch 180 iteration100, G Loss: 1.4714910984039307, D Loss: 1.0655438899993896, alpha: 0.9927537604041685\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 181 iteration0, G Loss: 1.3038800954818726, D Loss: 0.9761139750480652, alpha: 0.9927104701557979\n",
      "epoch 181 iteration100, G Loss: 1.299980878829956, D Loss: 1.0295310020446777, alpha: 0.9927104701557979\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 182 iteration0, G Loss: 1.2959264516830444, D Loss: 1.0972925424575806, alpha: 0.9926669231945832\n",
      "epoch 182 iteration100, G Loss: 1.4727556705474854, D Loss: 1.094222068786621, alpha: 0.9926669231945832\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 183 iteration0, G Loss: 1.1811343431472778, D Loss: 1.1946278810501099, alpha: 0.9926231180210985\n",
      "epoch 183 iteration100, G Loss: 2.163806200027466, D Loss: 1.337829351425171, alpha: 0.9926231180210985\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 184 iteration0, G Loss: 2.0116381645202637, D Loss: 1.1933839321136475, alpha: 0.9925790531274342\n",
      "epoch 184 iteration100, G Loss: 1.746407151222229, D Loss: 2.196425437927246, alpha: 0.9925790531274342\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 185 iteration0, G Loss: 1.942520022392273, D Loss: 1.2268853187561035, alpha: 0.9925347269971523\n",
      "epoch 185 iteration100, G Loss: 1.6945064067840576, D Loss: 1.2933990955352783, alpha: 0.9925347269971523\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 186 iteration0, G Loss: 0.9900369644165039, D Loss: 1.5498055219650269, alpha: 0.9924901381052417\n",
      "epoch 186 iteration100, G Loss: 1.2925792932510376, D Loss: 1.2660033702850342, alpha: 0.9924901381052417\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 187 iteration0, G Loss: 1.0680683851242065, D Loss: 1.061588168144226, alpha: 0.9924452849180726\n",
      "epoch 187 iteration100, G Loss: 1.863377332687378, D Loss: 1.163913607597351, alpha: 0.9924452849180726\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 188 iteration0, G Loss: 0.8132296800613403, D Loss: 1.123150110244751, alpha: 0.9924001658933519\n",
      "epoch 188 iteration100, G Loss: 1.1132808923721313, D Loss: 1.1545917987823486, alpha: 0.9924001658933519\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 189 iteration0, G Loss: 0.9004244208335876, D Loss: 1.2284166812896729, alpha: 0.9923547794800773\n",
      "epoch 189 iteration100, G Loss: 2.8518574237823486, D Loss: 1.1514294147491455, alpha: 0.9923547794800773\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 190 iteration0, G Loss: 1.1383864879608154, D Loss: 1.27055025100708, alpha: 0.9923091241184917\n",
      "epoch 190 iteration100, G Loss: 1.2808201313018799, D Loss: 1.1280145645141602, alpha: 0.9923091241184917\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 191 iteration0, G Loss: 1.143111228942871, D Loss: 1.1973819732666016, alpha: 0.9922631982400372\n",
      "epoch 191 iteration100, G Loss: 0.9122638702392578, D Loss: 0.9549658298492432, alpha: 0.9922631982400372\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 192 iteration0, G Loss: 1.6400171518325806, D Loss: 1.3320351839065552, alpha: 0.9922170002673092\n",
      "epoch 192 iteration100, G Loss: 1.905819296836853, D Loss: 1.4449186325073242, alpha: 0.9922170002673092\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 193 iteration0, G Loss: 1.640495777130127, D Loss: 1.3217658996582031, alpha: 0.9921705286140102\n",
      "epoch 193 iteration100, G Loss: 1.0846115350723267, D Loss: 0.9742749929428101, alpha: 0.9921705286140102\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 194 iteration0, G Loss: 1.1622426509857178, D Loss: 1.3897322416305542, alpha: 0.9921237816849032\n",
      "epoch 194 iteration100, G Loss: 1.6214865446090698, D Loss: 1.3670729398727417, alpha: 0.9921237816849032\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 195 iteration0, G Loss: 1.059257984161377, D Loss: 1.2669641971588135, alpha: 0.992076757875765\n",
      "epoch 195 iteration100, G Loss: 0.6028786897659302, D Loss: 1.487999677658081, alpha: 0.992076757875765\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 196 iteration0, G Loss: 1.1028043031692505, D Loss: 1.1212612390518188, alpha: 0.9920294555733393\n",
      "epoch 196 iteration100, G Loss: 1.033748984336853, D Loss: 1.280737280845642, alpha: 0.9920294555733393\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 197 iteration0, G Loss: 1.5352274179458618, D Loss: 1.5233221054077148, alpha: 0.9919818731552897\n",
      "epoch 197 iteration100, G Loss: 2.11505126953125, D Loss: 1.4178802967071533, alpha: 0.9919818731552897\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 198 iteration0, G Loss: 2.097517967224121, D Loss: 1.2112884521484375, alpha: 0.9919340089901527\n",
      "epoch 198 iteration100, G Loss: 3.294426441192627, D Loss: 1.441526174545288, alpha: 0.9919340089901527\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 199 iteration0, G Loss: 1.2655690908432007, D Loss: 1.2876803874969482, alpha: 0.9918858614372899\n",
      "epoch 199 iteration100, G Loss: 1.0285565853118896, D Loss: 1.2706941366195679, alpha: 0.9918858614372899\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 200 iteration0, G Loss: 1.193613886833191, D Loss: 0.9812363982200623, alpha: 0.9918374288468401\n",
      "epoch 200 iteration100, G Loss: 0.9089905023574829, D Loss: 1.2173157930374146, alpha: 0.9918374288468401\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 201 iteration0, G Loss: 1.4377052783966064, D Loss: 1.1527810096740723, alpha: 0.9917887095596722\n",
      "epoch 201 iteration100, G Loss: 0.812044084072113, D Loss: 1.185592532157898, alpha: 0.9917887095596722\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 202 iteration0, G Loss: 0.9269099235534668, D Loss: 1.3553600311279297, alpha: 0.9917397019073367\n",
      "epoch 202 iteration100, G Loss: 2.3211944103240967, D Loss: 1.340820550918579, alpha: 0.9917397019073367\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 203 iteration0, G Loss: 1.482960820198059, D Loss: 1.1379165649414062, alpha: 0.9916904042120173\n",
      "epoch 203 iteration100, G Loss: 1.161865472793579, D Loss: 1.0924861431121826, alpha: 0.9916904042120173\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 204 iteration0, G Loss: 1.1454795598983765, D Loss: 1.600139856338501, alpha: 0.9916408147864824\n",
      "epoch 204 iteration100, G Loss: 1.2351188659667969, D Loss: 1.8366155624389648, alpha: 0.9916408147864824\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 205 iteration0, G Loss: 1.172339916229248, D Loss: 1.2709994316101074, alpha: 0.991590931934037\n",
      "epoch 205 iteration100, G Loss: 1.49981689453125, D Loss: 1.3356221914291382, alpha: 0.991590931934037\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 206 iteration0, G Loss: 1.3536546230316162, D Loss: 1.2123167514801025, alpha: 0.9915407539484734\n",
      "epoch 206 iteration100, G Loss: 1.244253158569336, D Loss: 1.3328566551208496, alpha: 0.9915407539484734\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 207 iteration0, G Loss: 0.9706780910491943, D Loss: 1.2685329914093018, alpha: 0.9914902791140221\n",
      "epoch 207 iteration100, G Loss: 1.37470543384552, D Loss: 1.1086199283599854, alpha: 0.9914902791140221\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 208 iteration0, G Loss: 0.9941080808639526, D Loss: 1.2146658897399902, alpha: 0.9914395057053028\n",
      "epoch 208 iteration100, G Loss: 1.5214266777038574, D Loss: 1.2200915813446045, alpha: 0.9914395057053028\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 209 iteration0, G Loss: 1.6500635147094727, D Loss: 1.169539451599121, alpha: 0.9913884319872747\n",
      "epoch 209 iteration100, G Loss: 0.7650877237319946, D Loss: 1.4044735431671143, alpha: 0.9913884319872747\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 210 iteration0, G Loss: 1.0075243711471558, D Loss: 1.4702816009521484, alpha: 0.9913370562151869\n",
      "epoch 210 iteration100, G Loss: 0.9587140679359436, D Loss: 1.1890050172805786, alpha: 0.9913370562151869\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 211 iteration0, G Loss: 0.9635573625564575, D Loss: 1.3610780239105225, alpha: 0.9912853766345285\n",
      "epoch 211 iteration100, G Loss: 2.3602519035339355, D Loss: 1.4622632265090942, alpha: 0.9912853766345285\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 212 iteration0, G Loss: 1.067266583442688, D Loss: 1.5121006965637207, alpha: 0.9912333914809791\n",
      "epoch 212 iteration100, G Loss: 1.157989501953125, D Loss: 1.2875950336456299, alpha: 0.9912333914809791\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 213 iteration0, G Loss: 1.8869212865829468, D Loss: 1.2559309005737305, alpha: 0.9911810989803573\n",
      "epoch 213 iteration100, G Loss: 1.4986746311187744, D Loss: 1.24344003200531, alpha: 0.9911810989803573\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 214 iteration0, G Loss: 0.8450883626937866, D Loss: 1.262043833732605, alpha: 0.9911284973485716\n",
      "epoch 214 iteration100, G Loss: 1.0255142450332642, D Loss: 1.6487319469451904, alpha: 0.9911284973485716\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 215 iteration0, G Loss: 1.8974848985671997, D Loss: 1.5408415794372559, alpha: 0.9910755847915687\n",
      "epoch 215 iteration100, G Loss: 0.6614208221435547, D Loss: 1.5033879280090332, alpha: 0.9910755847915687\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 216 iteration0, G Loss: 1.4129303693771362, D Loss: 1.3069350719451904, alpha: 0.9910223595052832\n",
      "epoch 216 iteration100, G Loss: 1.0409643650054932, D Loss: 1.4066441059112549, alpha: 0.9910223595052832\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 217 iteration0, G Loss: 1.0429432392120361, D Loss: 1.1186039447784424, alpha: 0.9909688196755864\n",
      "epoch 217 iteration100, G Loss: 1.3540689945220947, D Loss: 1.2300350666046143, alpha: 0.9909688196755864\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 218 iteration0, G Loss: 1.9474501609802246, D Loss: 1.3542311191558838, alpha: 0.9909149634782348\n",
      "epoch 218 iteration100, G Loss: 1.1177501678466797, D Loss: 1.2604169845581055, alpha: 0.9909149634782348\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 219 iteration0, G Loss: 1.3222333192825317, D Loss: 1.1434211730957031, alpha: 0.990860789078819\n",
      "epoch 219 iteration100, G Loss: 0.6310655474662781, D Loss: 1.341808795928955, alpha: 0.990860789078819\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 220 iteration0, G Loss: 0.9634420871734619, D Loss: 1.1720798015594482, alpha: 0.9908062946327119\n",
      "epoch 220 iteration100, G Loss: 1.4871159791946411, D Loss: 1.234397292137146, alpha: 0.9908062946327119\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 221 iteration0, G Loss: 2.174762010574341, D Loss: 1.44976806640625, alpha: 0.9907514782850164\n",
      "epoch 221 iteration100, G Loss: 1.6521329879760742, D Loss: 1.162388563156128, alpha: 0.9907514782850164\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 222 iteration0, G Loss: 1.0362329483032227, D Loss: 1.6508055925369263, alpha: 0.9906963381705141\n",
      "epoch 222 iteration100, G Loss: 2.2607851028442383, D Loss: 2.103029489517212, alpha: 0.9906963381705141\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 223 iteration0, G Loss: 1.7026528120040894, D Loss: 1.3275526762008667, alpha: 0.9906408724136121\n",
      "epoch 223 iteration100, G Loss: 1.8068780899047852, D Loss: 1.4795376062393188, alpha: 0.9906408724136121\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 224 iteration0, G Loss: 1.7213410139083862, D Loss: 1.1105048656463623, alpha: 0.9905850791282914\n",
      "epoch 224 iteration100, G Loss: 1.4823687076568604, D Loss: 1.219667673110962, alpha: 0.9905850791282914\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 225 iteration0, G Loss: 0.8461464643478394, D Loss: 1.267930030822754, alpha: 0.9905289564180539\n",
      "epoch 225 iteration100, G Loss: 1.074170708656311, D Loss: 1.2535154819488525, alpha: 0.9905289564180539\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 226 iteration0, G Loss: 1.8447723388671875, D Loss: 1.562657117843628, alpha: 0.990472502375869\n",
      "epoch 226 iteration100, G Loss: 1.3531080484390259, D Loss: 1.060080885887146, alpha: 0.990472502375869\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 227 iteration0, G Loss: 1.548513412475586, D Loss: 1.296118974685669, alpha: 0.9904157150841214\n",
      "epoch 227 iteration100, G Loss: 1.0744988918304443, D Loss: 1.193971037864685, alpha: 0.9904157150841214\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 228 iteration0, G Loss: 1.3397423028945923, D Loss: 1.3094819784164429, alpha: 0.9903585926145569\n",
      "epoch 228 iteration100, G Loss: 1.2427014112472534, D Loss: 1.3211841583251953, alpha: 0.9903585926145569\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 229 iteration0, G Loss: 0.6079546213150024, D Loss: 1.6675941944122314, alpha: 0.9903011330282299\n",
      "epoch 229 iteration100, G Loss: 1.3672550916671753, D Loss: 1.3321857452392578, alpha: 0.9903011330282299\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 230 iteration0, G Loss: 1.0012556314468384, D Loss: 1.1446324586868286, alpha: 0.9902433343754486\n",
      "epoch 230 iteration100, G Loss: 1.1264920234680176, D Loss: 1.2954132556915283, alpha: 0.9902433343754486\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 231 iteration0, G Loss: 1.2754148244857788, D Loss: 1.0738962888717651, alpha: 0.9901851946957222\n",
      "epoch 231 iteration100, G Loss: 1.3640908002853394, D Loss: 1.2647252082824707, alpha: 0.9901851946957222\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 232 iteration0, G Loss: 0.9976611137390137, D Loss: 1.3587298393249512, alpha: 0.990126712017706\n",
      "epoch 232 iteration100, G Loss: 0.7326564788818359, D Loss: 1.2519696950912476, alpha: 0.990126712017706\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 233 iteration0, G Loss: 0.8114065527915955, D Loss: 1.2446001768112183, alpha: 0.9900678843591474\n",
      "epoch 233 iteration100, G Loss: 1.4871705770492554, D Loss: 1.0836403369903564, alpha: 0.9900678843591474\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 234 iteration0, G Loss: 1.1085842847824097, D Loss: 1.734443187713623, alpha: 0.9900087097268315\n",
      "epoch 234 iteration100, G Loss: 1.8054300546646118, D Loss: 1.3957287073135376, alpha: 0.9900087097268315\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 235 iteration0, G Loss: 1.1566122770309448, D Loss: 1.2469841241836548, alpha: 0.9899491861165263\n",
      "epoch 235 iteration100, G Loss: 1.7964725494384766, D Loss: 1.3182462453842163, alpha: 0.9899491861165263\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 236 iteration0, G Loss: 0.9844945669174194, D Loss: 1.5123552083969116, alpha: 0.9898893115129276\n",
      "epoch 236 iteration100, G Loss: 1.500096321105957, D Loss: 0.9925307035446167, alpha: 0.9898893115129276\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 237 iteration0, G Loss: 1.3176419734954834, D Loss: 1.164685845375061, alpha: 0.9898290838896044\n",
      "epoch 237 iteration100, G Loss: 1.2907521724700928, D Loss: 1.4134178161621094, alpha: 0.9898290838896044\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 238 iteration0, G Loss: 1.3318145275115967, D Loss: 1.3408122062683105, alpha: 0.9897685012089434\n",
      "epoch 238 iteration100, G Loss: 1.2136030197143555, D Loss: 1.169813632965088, alpha: 0.9897685012089434\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 239 iteration0, G Loss: 1.1020616292953491, D Loss: 1.3060815334320068, alpha: 0.9897075614220929\n",
      "epoch 239 iteration100, G Loss: 0.5361629128456116, D Loss: 1.4222004413604736, alpha: 0.9897075614220929\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 240 iteration0, G Loss: 1.613765001296997, D Loss: 1.2564494609832764, alpha: 0.9896462624689083\n",
      "epoch 240 iteration100, G Loss: 1.1371264457702637, D Loss: 1.2389698028564453, alpha: 0.9896462624689083\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 241 iteration0, G Loss: 1.4713199138641357, D Loss: 1.2794790267944336, alpha: 0.9895846022778949\n",
      "epoch 241 iteration100, G Loss: 1.1128305196762085, D Loss: 1.5590564012527466, alpha: 0.9895846022778949\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 242 iteration0, G Loss: 0.8938066363334656, D Loss: 1.3888866901397705, alpha: 0.989522578766153\n",
      "epoch 242 iteration100, G Loss: 1.6541588306427002, D Loss: 1.244734525680542, alpha: 0.989522578766153\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 243 iteration0, G Loss: 0.6004644632339478, D Loss: 1.4270460605621338, alpha: 0.9894601898393207\n",
      "epoch 243 iteration100, G Loss: 0.8874020576477051, D Loss: 1.2072817087173462, alpha: 0.9894601898393207\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 244 iteration0, G Loss: 1.9697599411010742, D Loss: 1.5610144138336182, alpha: 0.9893974333915181\n",
      "epoch 244 iteration100, G Loss: 1.302858591079712, D Loss: 1.3114738464355469, alpha: 0.9893974333915181\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 245 iteration0, G Loss: 1.7090067863464355, D Loss: 1.4523545503616333, alpha: 0.98933430730529\n",
      "epoch 245 iteration100, G Loss: 1.209383249282837, D Loss: 1.2511909008026123, alpha: 0.98933430730529\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 246 iteration0, G Loss: 1.68150794506073, D Loss: 1.5883407592773438, alpha: 0.9892708094515494\n",
      "epoch 246 iteration100, G Loss: 0.8862825036048889, D Loss: 1.5581339597702026, alpha: 0.9892708094515494\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 247 iteration0, G Loss: 1.7395001649856567, D Loss: 1.0656640529632568, alpha: 0.9892069376895206\n",
      "epoch 247 iteration100, G Loss: 1.1712400913238525, D Loss: 1.360418438911438, alpha: 0.9892069376895206\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 248 iteration0, G Loss: 1.2778652906417847, D Loss: 1.32275390625, alpha: 0.9891426898666814\n",
      "epoch 248 iteration100, G Loss: 1.2782421112060547, D Loss: 1.4406366348266602, alpha: 0.9891426898666814\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 249 iteration0, G Loss: 1.441334843635559, D Loss: 1.3043454885482788, alpha: 0.9890780638187058\n",
      "epoch 249 iteration100, G Loss: 1.618969440460205, D Loss: 1.2489954233169556, alpha: 0.9890780638187058\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 250 iteration0, G Loss: 0.6197270750999451, D Loss: 1.7955372333526611, alpha: 0.9890130573694068\n",
      "epoch 250 iteration100, G Loss: 1.1531922817230225, D Loss: 1.3542588949203491, alpha: 0.9890130573694068\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 251 iteration0, G Loss: 0.8852628469467163, D Loss: 1.3364239931106567, alpha: 0.9889476683306779\n",
      "epoch 251 iteration100, G Loss: 1.6589359045028687, D Loss: 1.2069339752197266, alpha: 0.9889476683306779\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 252 iteration0, G Loss: 1.4891914129257202, D Loss: 1.2844021320343018, alpha: 0.9888818945024357\n",
      "epoch 252 iteration100, G Loss: 0.6139394044876099, D Loss: 1.8967794179916382, alpha: 0.9888818945024357\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 253 iteration0, G Loss: 1.2635809183120728, D Loss: 1.4184050559997559, alpha: 0.9888157336725607\n",
      "epoch 253 iteration100, G Loss: 1.4629592895507812, D Loss: 1.2315764427185059, alpha: 0.9888157336725607\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 254 iteration0, G Loss: 1.5448739528656006, D Loss: 1.3288685083389282, alpha: 0.9887491836168399\n",
      "epoch 254 iteration100, G Loss: 1.1267613172531128, D Loss: 1.3021209239959717, alpha: 0.9887491836168399\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 255 iteration0, G Loss: 2.4430692195892334, D Loss: 0.8999009728431702, alpha: 0.9886822420989076\n",
      "epoch 255 iteration100, G Loss: 1.7432498931884766, D Loss: 1.1079750061035156, alpha: 0.9886822420989076\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 256 iteration0, G Loss: 0.769781768321991, D Loss: 1.4569745063781738, alpha: 0.9886149068701868\n",
      "epoch 256 iteration100, G Loss: 1.1812505722045898, D Loss: 0.9579170346260071, alpha: 0.9886149068701868\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 257 iteration0, G Loss: 1.4174261093139648, D Loss: 1.350631833076477, alpha: 0.98854717566983\n",
      "epoch 257 iteration100, G Loss: 1.6427161693572998, D Loss: 1.1804494857788086, alpha: 0.98854717566983\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 258 iteration0, G Loss: 1.1246787309646606, D Loss: 1.1995009183883667, alpha: 0.9884790462246599\n",
      "epoch 258 iteration100, G Loss: 1.8366345167160034, D Loss: 1.251300573348999, alpha: 0.9884790462246599\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 259 iteration0, G Loss: 1.0358362197875977, D Loss: 1.1519179344177246, alpha: 0.9884105162491105\n",
      "epoch 259 iteration100, G Loss: 1.7946486473083496, D Loss: 1.0985794067382812, alpha: 0.9884105162491105\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 260 iteration0, G Loss: 0.8031700253486633, D Loss: 1.3137612342834473, alpha: 0.9883415834451669\n",
      "epoch 260 iteration100, G Loss: 1.2921251058578491, D Loss: 1.4313299655914307, alpha: 0.9883415834451669\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 261 iteration0, G Loss: 2.139277935028076, D Loss: 1.065436840057373, alpha: 0.9882722455023062\n",
      "epoch 261 iteration100, G Loss: 0.9557386636734009, D Loss: 1.1744924783706665, alpha: 0.9882722455023062\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 262 iteration0, G Loss: 1.566550850868225, D Loss: 1.4689878225326538, alpha: 0.9882025000974367\n",
      "epoch 262 iteration100, G Loss: 1.3472448587417603, D Loss: 1.1664049625396729, alpha: 0.9882025000974367\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 263 iteration0, G Loss: 0.8677992224693298, D Loss: 1.3395538330078125, alpha: 0.9881323448948387\n",
      "epoch 263 iteration100, G Loss: 1.0609325170516968, D Loss: 1.1285336017608643, alpha: 0.9881323448948387\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 264 iteration0, G Loss: 1.5560741424560547, D Loss: 1.2095502614974976, alpha: 0.9880617775461034\n",
      "epoch 264 iteration100, G Loss: 1.0534188747406006, D Loss: 1.3141229152679443, alpha: 0.9880617775461034\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 265 iteration0, G Loss: 0.6415752172470093, D Loss: 1.4425370693206787, alpha: 0.9879907956900726\n",
      "epoch 265 iteration100, G Loss: 1.966415286064148, D Loss: 1.4249505996704102, alpha: 0.9879907956900726\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 266 iteration0, G Loss: 1.1212204694747925, D Loss: 1.0112879276275635, alpha: 0.9879193969527783\n",
      "epoch 266 iteration100, G Loss: 1.234144687652588, D Loss: 1.2881944179534912, alpha: 0.9879193969527783\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 267 iteration0, G Loss: 1.3268018960952759, D Loss: 1.4048244953155518, alpha: 0.9878475789473815\n",
      "epoch 267 iteration100, G Loss: 1.6151258945465088, D Loss: 1.2841837406158447, alpha: 0.9878475789473815\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 268 iteration0, G Loss: 0.9699214100837708, D Loss: 1.2922296524047852, alpha: 0.9877753392741111\n",
      "epoch 268 iteration100, G Loss: 1.0440504550933838, D Loss: 1.2870908975601196, alpha: 0.9877753392741111\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 269 iteration0, G Loss: 1.15784752368927, D Loss: 1.211114764213562, alpha: 0.9877026755202027\n",
      "epoch 269 iteration100, G Loss: 1.3158173561096191, D Loss: 1.508588433265686, alpha: 0.9877026755202027\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 270 iteration0, G Loss: 1.3020144701004028, D Loss: 1.2635456323623657, alpha: 0.9876295852598376\n",
      "epoch 270 iteration100, G Loss: 0.8790497779846191, D Loss: 1.278512954711914, alpha: 0.9876295852598376\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 271 iteration0, G Loss: 1.2787361145019531, D Loss: 1.302206039428711, alpha: 0.9875560660540803\n",
      "epoch 271 iteration100, G Loss: 1.5258206129074097, D Loss: 2.586602210998535, alpha: 0.9875560660540803\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 272 iteration0, G Loss: 0.7352198958396912, D Loss: 1.2723318338394165, alpha: 0.9874821154508174\n",
      "epoch 272 iteration100, G Loss: 0.657767653465271, D Loss: 1.7765496969223022, alpha: 0.9874821154508174\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 273 iteration0, G Loss: 1.7379823923110962, D Loss: 1.2192620038986206, alpha: 0.9874077309846958\n",
      "epoch 273 iteration100, G Loss: 0.9901275038719177, D Loss: 1.2322053909301758, alpha: 0.9874077309846958\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 274 iteration0, G Loss: 0.8809946775436401, D Loss: 2.0880236625671387, alpha: 0.9873329101770595\n",
      "epoch 274 iteration100, G Loss: 1.0521034002304077, D Loss: 0.9303882718086243, alpha: 0.9873329101770595\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 275 iteration0, G Loss: 0.8690333366394043, D Loss: 1.2819976806640625, alpha: 0.9872576505358884\n",
      "epoch 275 iteration100, G Loss: 1.1747009754180908, D Loss: 1.3308966159820557, alpha: 0.9872576505358884\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 276 iteration0, G Loss: 1.9140383005142212, D Loss: 1.3706791400909424, alpha: 0.9871819495557353\n",
      "epoch 276 iteration100, G Loss: 0.9879846572875977, D Loss: 1.286298155784607, alpha: 0.9871819495557353\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 277 iteration0, G Loss: 1.5151820182800293, D Loss: 1.4741524457931519, alpha: 0.9871058047176633\n",
      "epoch 277 iteration100, G Loss: 0.8824005722999573, D Loss: 1.4488255977630615, alpha: 0.9871058047176633\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 278 iteration0, G Loss: 1.4137681722640991, D Loss: 1.3163223266601562, alpha: 0.9870292134891828\n",
      "epoch 278 iteration100, G Loss: 1.601007342338562, D Loss: 1.6736713647842407, alpha: 0.9870292134891828\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 279 iteration0, G Loss: 1.3314790725708008, D Loss: 1.230642318725586, alpha: 0.9869521733241887\n",
      "epoch 279 iteration100, G Loss: 1.0031644105911255, D Loss: 1.2984411716461182, alpha: 0.9869521733241887\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 280 iteration0, G Loss: 1.6288806200027466, D Loss: 1.363944172859192, alpha: 0.9868746816628972\n",
      "epoch 280 iteration100, G Loss: 1.8859227895736694, D Loss: 1.4466674327850342, alpha: 0.9868746816628972\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 281 iteration0, G Loss: 1.2848374843597412, D Loss: 1.2346946001052856, alpha: 0.9867967359317822\n",
      "epoch 281 iteration100, G Loss: 0.6606132388114929, D Loss: 1.2683210372924805, alpha: 0.9867967359317822\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 282 iteration0, G Loss: 0.7975071668624878, D Loss: 1.5804266929626465, alpha: 0.986718333543512\n",
      "epoch 282 iteration100, G Loss: 0.9724456667900085, D Loss: 1.3099961280822754, alpha: 0.986718333543512\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 283 iteration0, G Loss: 1.3372060060501099, D Loss: 1.0966168642044067, alpha: 0.9866394718968857\n",
      "epoch 283 iteration100, G Loss: 0.872628927230835, D Loss: 1.132210373878479, alpha: 0.9866394718968857\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 284 iteration0, G Loss: 1.7440322637557983, D Loss: 1.0727276802062988, alpha: 0.986560148376769\n",
      "epoch 284 iteration100, G Loss: 0.4052814841270447, D Loss: 1.3989522457122803, alpha: 0.986560148376769\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 285 iteration0, G Loss: 1.2919360399246216, D Loss: 0.9047075510025024, alpha: 0.9864803603540304\n",
      "epoch 285 iteration100, G Loss: 1.3301562070846558, D Loss: 1.14042329788208, alpha: 0.9864803603540304\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 286 iteration0, G Loss: 1.6440694332122803, D Loss: 1.293350100517273, alpha: 0.9864001051854769\n",
      "epoch 286 iteration100, G Loss: 3.0878632068634033, D Loss: 0.8969876766204834, alpha: 0.9864001051854769\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 287 iteration0, G Loss: 1.1736578941345215, D Loss: 1.1521869897842407, alpha: 0.9863193802137901\n",
      "epoch 287 iteration100, G Loss: 1.4364564418792725, D Loss: 1.5960991382598877, alpha: 0.9863193802137901\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 288 iteration0, G Loss: 1.6172208786010742, D Loss: 1.2481896877288818, alpha: 0.9862381827674608\n",
      "epoch 288 iteration100, G Loss: 0.7709828615188599, D Loss: 1.3488082885742188, alpha: 0.9862381827674608\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 289 iteration0, G Loss: 1.3193671703338623, D Loss: 1.400568962097168, alpha: 0.9861565101607251\n",
      "epoch 289 iteration100, G Loss: 1.607047438621521, D Loss: 1.0156598091125488, alpha: 0.9861565101607251\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 290 iteration0, G Loss: 1.3090429306030273, D Loss: 1.0370347499847412, alpha: 0.9860743596934997\n",
      "epoch 290 iteration100, G Loss: 0.8133254051208496, D Loss: 1.3548352718353271, alpha: 0.9860743596934997\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 291 iteration0, G Loss: 0.9418425559997559, D Loss: 1.1585160493850708, alpha: 0.9859917286513157\n",
      "epoch 291 iteration100, G Loss: 1.1682558059692383, D Loss: 1.4638744592666626, alpha: 0.9859917286513157\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 292 iteration0, G Loss: 0.9769264459609985, D Loss: 1.1995320320129395, alpha: 0.9859086143052553\n",
      "epoch 292 iteration100, G Loss: 1.3319122791290283, D Loss: 1.2673370838165283, alpha: 0.9859086143052553\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 293 iteration0, G Loss: 1.305713176727295, D Loss: 1.2457956075668335, alpha: 0.9858250139118848\n",
      "epoch 293 iteration100, G Loss: 1.4347331523895264, D Loss: 1.3258112668991089, alpha: 0.9858250139118848\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 294 iteration0, G Loss: 1.5542017221450806, D Loss: 1.1604655981063843, alpha: 0.9857409247131904\n",
      "epoch 294 iteration100, G Loss: 1.4601116180419922, D Loss: 0.9697068929672241, alpha: 0.9857409247131904\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 295 iteration0, G Loss: 1.6876709461212158, D Loss: 1.0435734987258911, alpha: 0.9856563439365119\n",
      "epoch 295 iteration100, G Loss: 1.4560679197311401, D Loss: 1.227165699005127, alpha: 0.9856563439365119\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 296 iteration0, G Loss: 0.7011497020721436, D Loss: 1.5121090412139893, alpha: 0.9855712687944772\n",
      "epoch 296 iteration100, G Loss: 0.8563131093978882, D Loss: 1.2908953428268433, alpha: 0.9855712687944772\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 297 iteration0, G Loss: 1.3149389028549194, D Loss: 1.2600806951522827, alpha: 0.9854856964849366\n",
      "epoch 297 iteration100, G Loss: 1.2199227809906006, D Loss: 1.3480470180511475, alpha: 0.9854856964849366\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 298 iteration0, G Loss: 0.9542988538742065, D Loss: 1.7588560581207275, alpha: 0.9853996241908963\n",
      "epoch 298 iteration100, G Loss: 1.5497736930847168, D Loss: 1.9143468141555786, alpha: 0.9853996241908963\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 299 iteration0, G Loss: 1.503243327140808, D Loss: 1.2154568433761597, alpha: 0.985313049080453\n",
      "epoch 299 iteration100, G Loss: 1.243664264678955, D Loss: 1.2545576095581055, alpha: 0.985313049080453\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 300 iteration0, G Loss: 1.1181983947753906, D Loss: 1.2368534803390503, alpha: 0.9852259683067269\n",
      "epoch 300 iteration100, G Loss: 1.2017093896865845, D Loss: 1.1443562507629395, alpha: 0.9852259683067269\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 301 iteration0, G Loss: 1.5785768032073975, D Loss: 1.5791223049163818, alpha: 0.9851383790077956\n",
      "epoch 301 iteration100, G Loss: 1.117153525352478, D Loss: 1.2853400707244873, alpha: 0.9851383790077956\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 302 iteration0, G Loss: 1.3097162246704102, D Loss: 1.185569405555725, alpha: 0.9850502783066273\n",
      "epoch 302 iteration100, G Loss: 1.4051344394683838, D Loss: 1.1770415306091309, alpha: 0.9850502783066273\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 303 iteration0, G Loss: 1.2150121927261353, D Loss: 1.2163419723510742, alpha: 0.9849616633110144\n",
      "epoch 303 iteration100, G Loss: 0.31949180364608765, D Loss: 1.4132020473480225, alpha: 0.9849616633110144\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 304 iteration0, G Loss: 1.054282784461975, D Loss: 1.2744266986846924, alpha: 0.9848725311135066\n",
      "epoch 304 iteration100, G Loss: 0.9774966835975647, D Loss: 1.3001389503479004, alpha: 0.9848725311135066\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 305 iteration0, G Loss: 1.4071435928344727, D Loss: 1.128582239151001, alpha: 0.9847828787913437\n",
      "epoch 305 iteration100, G Loss: 1.2710052728652954, D Loss: 1.049271821975708, alpha: 0.9847828787913437\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 306 iteration0, G Loss: 1.506516933441162, D Loss: 1.0224770307540894, alpha: 0.9846927034063887\n",
      "epoch 306 iteration100, G Loss: 1.0113497972488403, D Loss: 1.2996782064437866, alpha: 0.9846927034063887\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 307 iteration0, G Loss: 1.657383918762207, D Loss: 1.1467370986938477, alpha: 0.9846020020050607\n",
      "epoch 307 iteration100, G Loss: 2.5407631397247314, D Loss: 0.826866865158081, alpha: 0.9846020020050607\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 308 iteration0, G Loss: 2.6814844608306885, D Loss: 1.772768259048462, alpha: 0.984510771618267\n",
      "epoch 308 iteration100, G Loss: 1.1606647968292236, D Loss: 1.2481422424316406, alpha: 0.984510771618267\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 309 iteration0, G Loss: 0.8708277940750122, D Loss: 1.2062489986419678, alpha: 0.9844190092613365\n",
      "epoch 309 iteration100, G Loss: 1.0931386947631836, D Loss: 1.1764010190963745, alpha: 0.9844190092613365\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 310 iteration0, G Loss: 1.331241488456726, D Loss: 1.259278655052185, alpha: 0.9843267119339514\n",
      "epoch 310 iteration100, G Loss: 0.8511778116226196, D Loss: 1.1518561840057373, alpha: 0.9843267119339514\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 311 iteration0, G Loss: 1.092603087425232, D Loss: 1.3631823062896729, alpha: 0.9842338766200798\n",
      "epoch 311 iteration100, G Loss: 0.9093471169471741, D Loss: 1.1539126634597778, alpha: 0.9842338766200798\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 312 iteration0, G Loss: 1.608620524406433, D Loss: 1.1082758903503418, alpha: 0.9841405002879078\n",
      "epoch 312 iteration100, G Loss: 1.5961027145385742, D Loss: 1.324678659439087, alpha: 0.9841405002879078\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 313 iteration0, G Loss: 1.8535479307174683, D Loss: 1.5846161842346191, alpha: 0.9840465798897717\n",
      "epoch 313 iteration100, G Loss: 1.2479361295700073, D Loss: 1.3449753522872925, alpha: 0.9840465798897717\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 314 iteration0, G Loss: 2.302936553955078, D Loss: 1.141047716140747, alpha: 0.98395211236209\n",
      "epoch 314 iteration100, G Loss: 1.1389820575714111, D Loss: 1.2477245330810547, alpha: 0.98395211236209\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 315 iteration0, G Loss: 1.418973445892334, D Loss: 1.2264647483825684, alpha: 0.9838570946252948\n",
      "epoch 315 iteration100, G Loss: 1.2783639430999756, D Loss: 1.2874042987823486, alpha: 0.9838570946252948\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 316 iteration0, G Loss: 2.0349252223968506, D Loss: 0.9859635829925537, alpha: 0.9837615235837642\n",
      "epoch 316 iteration100, G Loss: 0.966076135635376, D Loss: 1.2578271627426147, alpha: 0.9837615235837642\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 317 iteration0, G Loss: 1.5643560886383057, D Loss: 1.1512200832366943, alpha: 0.9836653961257537\n",
      "epoch 317 iteration100, G Loss: 1.2519702911376953, D Loss: 1.0861825942993164, alpha: 0.9836653961257537\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 318 iteration0, G Loss: 1.1662039756774902, D Loss: 1.230560541152954, alpha: 0.9835687091233274\n",
      "epoch 318 iteration100, G Loss: 1.5893051624298096, D Loss: 0.9264736771583557, alpha: 0.9835687091233274\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 319 iteration0, G Loss: 1.3112245798110962, D Loss: 1.0294491052627563, alpha: 0.9834714594322902\n",
      "epoch 319 iteration100, G Loss: 1.6148021221160889, D Loss: 1.2712831497192383, alpha: 0.9834714594322902\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 320 iteration0, G Loss: 1.5440179109573364, D Loss: 1.037930965423584, alpha: 0.9833736438921183\n",
      "epoch 320 iteration100, G Loss: 2.1055445671081543, D Loss: 1.0005314350128174, alpha: 0.9833736438921183\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 321 iteration0, G Loss: 2.2270925045013428, D Loss: 1.3513814210891724, alpha: 0.9832752593258914\n",
      "epoch 321 iteration100, G Loss: 1.6971536874771118, D Loss: 1.4608726501464844, alpha: 0.9832752593258914\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 322 iteration0, G Loss: 1.215917944908142, D Loss: 1.148289442062378, alpha: 0.9831763025402231\n",
      "epoch 322 iteration100, G Loss: 1.284945011138916, D Loss: 1.1834661960601807, alpha: 0.9831763025402231\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 323 iteration0, G Loss: 1.3023784160614014, D Loss: 1.3506180047988892, alpha: 0.9830767703251925\n",
      "epoch 323 iteration100, G Loss: 1.7970490455627441, D Loss: 1.2338902950286865, alpha: 0.9830767703251925\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 324 iteration0, G Loss: 1.663085699081421, D Loss: 1.1312775611877441, alpha: 0.9829766594542746\n",
      "epoch 324 iteration100, G Loss: 1.3779865503311157, D Loss: 1.150075912475586, alpha: 0.9829766594542746\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 325 iteration0, G Loss: 1.609910011291504, D Loss: 1.0070393085479736, alpha: 0.9828759666842722\n",
      "epoch 325 iteration100, G Loss: 1.8479779958724976, D Loss: 1.6166436672210693, alpha: 0.9828759666842722\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 326 iteration0, G Loss: 2.3868231773376465, D Loss: 2.5452399253845215, alpha: 0.9827746887552462\n",
      "epoch 326 iteration100, G Loss: 0.9027421474456787, D Loss: 1.1535708904266357, alpha: 0.9827746887552462\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 327 iteration0, G Loss: 1.0492867231369019, D Loss: 1.4057255983352661, alpha: 0.9826728223904461\n",
      "epoch 327 iteration100, G Loss: 2.22308349609375, D Loss: 1.0820834636688232, alpha: 0.9826728223904461\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 328 iteration0, G Loss: 1.175857424736023, D Loss: 1.2186195850372314, alpha: 0.9825703642962413\n",
      "epoch 328 iteration100, G Loss: 1.5519808530807495, D Loss: 1.4301788806915283, alpha: 0.9825703642962413\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 329 iteration0, G Loss: 2.0884721279144287, D Loss: 1.3880060911178589, alpha: 0.9824673111620515\n",
      "epoch 329 iteration100, G Loss: 0.9727914929389954, D Loss: 1.187915563583374, alpha: 0.9824673111620515\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 330 iteration0, G Loss: 0.9167304039001465, D Loss: 1.1562501192092896, alpha: 0.9823636596602773\n",
      "epoch 330 iteration100, G Loss: 1.0972495079040527, D Loss: 1.158815860748291, alpha: 0.9823636596602773\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 331 iteration0, G Loss: 1.9805265665054321, D Loss: 0.9011250734329224, alpha: 0.9822594064462308\n",
      "epoch 331 iteration100, G Loss: 1.4667384624481201, D Loss: 1.2007685899734497, alpha: 0.9822594064462308\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 332 iteration0, G Loss: 0.7660006880760193, D Loss: 1.1545435190200806, alpha: 0.9821545481580658\n",
      "epoch 332 iteration100, G Loss: 1.4114412069320679, D Loss: 1.1874550580978394, alpha: 0.9821545481580658\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 333 iteration0, G Loss: 1.1703159809112549, D Loss: 1.1832902431488037, alpha: 0.9820490814167088\n",
      "epoch 333 iteration100, G Loss: 1.1846601963043213, D Loss: 1.1517186164855957, alpha: 0.9820490814167088\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 334 iteration0, G Loss: 0.8445055484771729, D Loss: 1.1153515577316284, alpha: 0.9819430028257886\n",
      "epoch 334 iteration100, G Loss: 1.3835610151290894, D Loss: 0.8750648498535156, alpha: 0.9819430028257886\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 335 iteration0, G Loss: 2.395533323287964, D Loss: 0.9628848433494568, alpha: 0.9818363089715675\n",
      "epoch 335 iteration100, G Loss: 0.9874773621559143, D Loss: 1.186513066291809, alpha: 0.9818363089715675\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 336 iteration0, G Loss: 1.2605396509170532, D Loss: 1.2966575622558594, alpha: 0.9817289964228708\n",
      "epoch 336 iteration100, G Loss: 0.9035601615905762, D Loss: 1.4300708770751953, alpha: 0.9817289964228708\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 337 iteration0, G Loss: 1.0459024906158447, D Loss: 1.3580007553100586, alpha: 0.9816210617310175\n",
      "epoch 337 iteration100, G Loss: 1.5315645933151245, D Loss: 1.281446933746338, alpha: 0.9816210617310175\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 338 iteration0, G Loss: 1.077560544013977, D Loss: 1.274735927581787, alpha: 0.9815125014297503\n",
      "epoch 338 iteration100, G Loss: 0.8787984848022461, D Loss: 1.5573335886001587, alpha: 0.9815125014297503\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 339 iteration0, G Loss: 0.6789124608039856, D Loss: 1.072762370109558, alpha: 0.981403312035166\n",
      "epoch 339 iteration100, G Loss: 1.3932000398635864, D Loss: 1.138752818107605, alpha: 0.981403312035166\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 340 iteration0, G Loss: 1.7289396524429321, D Loss: 1.4885029792785645, alpha: 0.9812934900456454\n",
      "epoch 340 iteration100, G Loss: 1.205834984779358, D Loss: 1.4142155647277832, alpha: 0.9812934900456454\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 341 iteration0, G Loss: 1.5963581800460815, D Loss: 1.144162893295288, alpha: 0.9811830319417836\n",
      "epoch 341 iteration100, G Loss: 1.6739095449447632, D Loss: 1.1491469144821167, alpha: 0.9811830319417836\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 342 iteration0, G Loss: 1.8092466592788696, D Loss: 1.829395055770874, alpha: 0.9810719341863199\n",
      "epoch 342 iteration100, G Loss: 0.9828884601593018, D Loss: 1.0916218757629395, alpha: 0.9810719341863199\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 343 iteration0, G Loss: 1.2933398485183716, D Loss: 1.191311240196228, alpha: 0.9809601932240681\n",
      "epoch 343 iteration100, G Loss: 1.1621006727218628, D Loss: 1.3212158679962158, alpha: 0.9809601932240681\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 344 iteration0, G Loss: 2.3955795764923096, D Loss: 1.4344372749328613, alpha: 0.9808478054818466\n",
      "epoch 344 iteration100, G Loss: 1.3099089860916138, D Loss: 1.1756012439727783, alpha: 0.9808478054818466\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 345 iteration0, G Loss: 1.0144304037094116, D Loss: 1.3289960622787476, alpha: 0.9807347673684084\n",
      "epoch 345 iteration100, G Loss: 1.3305519819259644, D Loss: 1.2895512580871582, alpha: 0.9807347673684084\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 346 iteration0, G Loss: 1.5116240978240967, D Loss: 1.5589971542358398, alpha: 0.9806210752743707\n",
      "epoch 346 iteration100, G Loss: 0.527802586555481, D Loss: 1.3434886932373047, alpha: 0.9806210752743707\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 347 iteration0, G Loss: 0.793310821056366, D Loss: 1.2438459396362305, alpha: 0.9805067255721459\n",
      "epoch 347 iteration100, G Loss: 1.3745672702789307, D Loss: 1.1100199222564697, alpha: 0.9805067255721459\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 348 iteration0, G Loss: 2.187650680541992, D Loss: 1.2240357398986816, alpha: 0.9803917146158709\n",
      "epoch 348 iteration100, G Loss: 1.0763795375823975, D Loss: 1.2968119382858276, alpha: 0.9803917146158709\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 349 iteration0, G Loss: 2.1202714443206787, D Loss: 1.3076680898666382, alpha: 0.9802760387413375\n",
      "epoch 349 iteration100, G Loss: 1.4041173458099365, D Loss: 1.4303661584854126, alpha: 0.9802760387413375\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 350 iteration0, G Loss: 1.4796792268753052, D Loss: 1.1433861255645752, alpha: 0.9801596942659225\n",
      "epoch 350 iteration100, G Loss: 2.0538508892059326, D Loss: 1.4307878017425537, alpha: 0.9801596942659225\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 351 iteration0, G Loss: 1.9426469802856445, D Loss: 1.2436916828155518, alpha: 0.9800426774885175\n",
      "epoch 351 iteration100, G Loss: 1.1498870849609375, D Loss: 1.1415069103240967, alpha: 0.9800426774885175\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 352 iteration0, G Loss: 1.0268971920013428, D Loss: 1.2347149848937988, alpha: 0.9799249846894594\n",
      "epoch 352 iteration100, G Loss: 1.6042540073394775, D Loss: 1.244182825088501, alpha: 0.9799249846894594\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 353 iteration0, G Loss: 1.358731746673584, D Loss: 1.1699767112731934, alpha: 0.9798066121304606\n",
      "epoch 353 iteration100, G Loss: 1.2740330696105957, D Loss: 1.0421570539474487, alpha: 0.9798066121304606\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 354 iteration0, G Loss: 0.8923170566558838, D Loss: 1.4337871074676514, alpha: 0.9796875560545386\n",
      "epoch 354 iteration100, G Loss: 0.822848916053772, D Loss: 1.667434811592102, alpha: 0.9796875560545386\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 355 iteration0, G Loss: 0.8485471606254578, D Loss: 1.3377838134765625, alpha: 0.979567812685947\n",
      "epoch 355 iteration100, G Loss: 2.97042179107666, D Loss: 1.0425257682800293, alpha: 0.979567812685947\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 356 iteration0, G Loss: 1.6855577230453491, D Loss: 1.3639419078826904, alpha: 0.9794473782301051\n",
      "epoch 356 iteration100, G Loss: 0.7931848764419556, D Loss: 0.9968643188476562, alpha: 0.9794473782301051\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 357 iteration0, G Loss: 1.5614748001098633, D Loss: 1.3345738649368286, alpha: 0.9793262488735286\n",
      "epoch 357 iteration100, G Loss: 1.1420222520828247, D Loss: 1.1757229566574097, alpha: 0.9793262488735286\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 358 iteration0, G Loss: 1.2116868495941162, D Loss: 1.0596933364868164, alpha: 0.97920442078376\n",
      "epoch 358 iteration100, G Loss: 0.8702231049537659, D Loss: 1.180717945098877, alpha: 0.97920442078376\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 359 iteration0, G Loss: 1.548607587814331, D Loss: 1.1988346576690674, alpha: 0.9790818901092987\n",
      "epoch 359 iteration100, G Loss: 1.8017675876617432, D Loss: 1.0389139652252197, alpha: 0.9790818901092987\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 360 iteration0, G Loss: 1.6343812942504883, D Loss: 1.1129322052001953, alpha: 0.9789586529795318\n",
      "epoch 360 iteration100, G Loss: 1.097903847694397, D Loss: 1.1099860668182373, alpha: 0.9789586529795318\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 361 iteration0, G Loss: 1.6881580352783203, D Loss: 1.316866159439087, alpha: 0.9788347055046641\n",
      "epoch 361 iteration100, G Loss: 1.3244447708129883, D Loss: 1.1368190050125122, alpha: 0.9788347055046641\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 362 iteration0, G Loss: 1.7318987846374512, D Loss: 1.2021474838256836, alpha: 0.9787100437756497\n",
      "epoch 362 iteration100, G Loss: 1.4047863483428955, D Loss: 1.2058547735214233, alpha: 0.9787100437756497\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 363 iteration0, G Loss: 1.2495492696762085, D Loss: 1.0805914402008057, alpha: 0.9785846638641217\n",
      "epoch 363 iteration100, G Loss: 0.8802561163902283, D Loss: 1.8081295490264893, alpha: 0.9785846638641217\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 364 iteration0, G Loss: 0.740756094455719, D Loss: 1.3556525707244873, alpha: 0.9784585618223235\n",
      "epoch 364 iteration100, G Loss: 1.0427019596099854, D Loss: 1.1667895317077637, alpha: 0.9784585618223235\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 365 iteration0, G Loss: 0.8883969783782959, D Loss: 1.165708303451538, alpha: 0.9783317336830395\n",
      "epoch 365 iteration100, G Loss: 1.9278924465179443, D Loss: 1.1757386922836304, alpha: 0.9783317336830395\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 366 iteration0, G Loss: 1.4860917329788208, D Loss: 1.6474041938781738, alpha: 0.9782041754595261\n",
      "epoch 366 iteration100, G Loss: 0.9500831961631775, D Loss: 1.02978515625, alpha: 0.9782041754595261\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 367 iteration0, G Loss: 0.8439002633094788, D Loss: 1.202858328819275, alpha: 0.9780758831454427\n",
      "epoch 367 iteration100, G Loss: 1.388468623161316, D Loss: 1.0442888736724854, alpha: 0.9780758831454427\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 368 iteration0, G Loss: 1.6710652112960815, D Loss: 1.2290325164794922, alpha: 0.977946852714783\n",
      "epoch 368 iteration100, G Loss: 1.5224993228912354, D Loss: 1.0723038911819458, alpha: 0.977946852714783\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 369 iteration0, G Loss: 1.844934105873108, D Loss: 1.3572931289672852, alpha: 0.977817080121806\n",
      "epoch 369 iteration100, G Loss: 1.5075877904891968, D Loss: 1.1827082633972168, alpha: 0.977817080121806\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 370 iteration0, G Loss: 0.6491626501083374, D Loss: 1.0269889831542969, alpha: 0.9776865613009678\n",
      "epoch 370 iteration100, G Loss: 1.3547780513763428, D Loss: 1.029211401939392, alpha: 0.9776865613009678\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 371 iteration0, G Loss: 1.5460413694381714, D Loss: 1.3979535102844238, alpha: 0.9775552921668526\n",
      "epoch 371 iteration100, G Loss: 1.4678524732589722, D Loss: 1.334211826324463, alpha: 0.9775552921668526\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 372 iteration0, G Loss: 0.8413150310516357, D Loss: 1.1858747005462646, alpha: 0.9774232686141044\n",
      "epoch 372 iteration100, G Loss: 1.177860975265503, D Loss: 1.2088781595230103, alpha: 0.9774232686141044\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 373 iteration0, G Loss: 1.5601717233657837, D Loss: 1.2512328624725342, alpha: 0.9772904865173597\n",
      "epoch 373 iteration100, G Loss: 0.7542783617973328, D Loss: 1.3455311059951782, alpha: 0.9772904865173597\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 374 iteration0, G Loss: 1.7973860502243042, D Loss: 1.23586106300354, alpha: 0.977156941731178\n",
      "epoch 374 iteration100, G Loss: 1.3381915092468262, D Loss: 1.2759546041488647, alpha: 0.977156941731178\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 375 iteration0, G Loss: 1.542299509048462, D Loss: 1.314598560333252, alpha: 0.9770226300899744\n",
      "epoch 375 iteration100, G Loss: 0.8481993079185486, D Loss: 1.4755444526672363, alpha: 0.9770226300899744\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 376 iteration0, G Loss: 1.0279351472854614, D Loss: 1.3380918502807617, alpha: 0.9768875474079524\n",
      "epoch 376 iteration100, G Loss: 0.7131544351577759, D Loss: 1.2707724571228027, alpha: 0.9768875474079524\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 377 iteration0, G Loss: 0.8685424327850342, D Loss: 1.4503320455551147, alpha: 0.9767516894790355\n",
      "epoch 377 iteration100, G Loss: 2.279663562774658, D Loss: 1.3004838228225708, alpha: 0.9767516894790355\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 378 iteration0, G Loss: 1.811507225036621, D Loss: 1.1452146768569946, alpha: 0.9766150520767999\n",
      "epoch 378 iteration100, G Loss: 1.3123853206634521, D Loss: 0.8530954122543335, alpha: 0.9766150520767999\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 379 iteration0, G Loss: 2.0221469402313232, D Loss: 1.2145487070083618, alpha: 0.9764776309544076\n",
      "epoch 379 iteration100, G Loss: 1.0580369234085083, D Loss: 1.2791774272918701, alpha: 0.9764776309544076\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 380 iteration0, G Loss: 0.8125321269035339, D Loss: 1.711449384689331, alpha: 0.9763394218445388\n",
      "epoch 380 iteration100, G Loss: 1.0133239030838013, D Loss: 1.3364770412445068, alpha: 0.9763394218445388\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 381 iteration0, G Loss: 1.2320759296417236, D Loss: 1.275639533996582, alpha: 0.976200420459325\n",
      "epoch 381 iteration100, G Loss: 1.7476476430892944, D Loss: 1.3545863628387451, alpha: 0.976200420459325\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 382 iteration0, G Loss: 1.67133629322052, D Loss: 1.21506667137146, alpha: 0.9760606224902827\n",
      "epoch 382 iteration100, G Loss: 1.4743001461029053, D Loss: 1.3963408470153809, alpha: 0.9760606224902827\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 383 iteration0, G Loss: 0.9320357441902161, D Loss: 1.2930917739868164, alpha: 0.9759200236082463\n",
      "epoch 383 iteration100, G Loss: 1.2819550037384033, D Loss: 1.1038248538970947, alpha: 0.9759200236082463\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 384 iteration0, G Loss: 1.7978965044021606, D Loss: 1.319313883781433, alpha: 0.9757786194633021\n",
      "epoch 384 iteration100, G Loss: 2.115374803543091, D Loss: 1.2980087995529175, alpha: 0.9757786194633021\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 385 iteration0, G Loss: 1.5007377862930298, D Loss: 1.1880214214324951, alpha: 0.9756364056847221\n",
      "epoch 385 iteration100, G Loss: 2.658343553543091, D Loss: 1.5321719646453857, alpha: 0.9756364056847221\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 386 iteration0, G Loss: 0.5109784603118896, D Loss: 1.6227692365646362, alpha: 0.9754933778808983\n",
      "epoch 386 iteration100, G Loss: 1.2520495653152466, D Loss: 1.0233725309371948, alpha: 0.9754933778808983\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 387 iteration0, G Loss: 1.9545409679412842, D Loss: 1.5306220054626465, alpha: 0.9753495316392763\n",
      "epoch 387 iteration100, G Loss: 0.9661998748779297, D Loss: 1.336381196975708, alpha: 0.9753495316392763\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 388 iteration0, G Loss: 1.8767013549804688, D Loss: 0.9072274565696716, alpha: 0.9752048625262908\n",
      "epoch 388 iteration100, G Loss: 1.3034336566925049, D Loss: 1.0366530418395996, alpha: 0.9752048625262908\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 389 iteration0, G Loss: 1.9349135160446167, D Loss: 1.9581876993179321, alpha: 0.9750593660872999\n",
      "epoch 389 iteration100, G Loss: 1.251634120941162, D Loss: 1.4408459663391113, alpha: 0.9750593660872999\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 390 iteration0, G Loss: 1.7045693397521973, D Loss: 1.2242496013641357, alpha: 0.9749130378465202\n",
      "epoch 390 iteration100, G Loss: 1.5796751976013184, D Loss: 1.5401184558868408, alpha: 0.9749130378465202\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 391 iteration0, G Loss: 1.5603126287460327, D Loss: 1.4773383140563965, alpha: 0.974765873306962\n",
      "epoch 391 iteration100, G Loss: 0.9904046058654785, D Loss: 1.2989596128463745, alpha: 0.974765873306962\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 392 iteration0, G Loss: 3.240900754928589, D Loss: 1.379205346107483, alpha: 0.974617867950365\n",
      "epoch 392 iteration100, G Loss: 1.1600162982940674, D Loss: 1.322128415107727, alpha: 0.974617867950365\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 393 iteration0, G Loss: 1.5701804161071777, D Loss: 1.3416321277618408, alpha: 0.9744690172371344\n",
      "epoch 393 iteration100, G Loss: 2.5403895378112793, D Loss: 1.4637222290039062, alpha: 0.9744690172371344\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 394 iteration0, G Loss: 0.9817003011703491, D Loss: 1.33060622215271, alpha: 0.9743193166062767\n",
      "epoch 394 iteration100, G Loss: 1.2710609436035156, D Loss: 1.2131133079528809, alpha: 0.9743193166062767\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 395 iteration0, G Loss: 1.0754685401916504, D Loss: 1.1530014276504517, alpha: 0.9741687614753359\n",
      "epoch 395 iteration100, G Loss: 1.9228084087371826, D Loss: 1.2163469791412354, alpha: 0.9741687614753359\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 396 iteration0, G Loss: 1.6483707427978516, D Loss: 1.359189748764038, alpha: 0.9740173472403311\n",
      "epoch 396 iteration100, G Loss: 0.8176198601722717, D Loss: 1.2868419885635376, alpha: 0.9740173472403311\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 397 iteration0, G Loss: 0.5586717128753662, D Loss: 1.5671956539154053, alpha: 0.9738650692756922\n",
      "epoch 397 iteration100, G Loss: 1.1071854829788208, D Loss: 1.1675477027893066, alpha: 0.9738650692756922\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 398 iteration0, G Loss: 1.0670719146728516, D Loss: 1.1446682214736938, alpha: 0.9737119229341985\n",
      "epoch 398 iteration100, G Loss: 2.1424262523651123, D Loss: 1.2530673742294312, alpha: 0.9737119229341985\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 399 iteration0, G Loss: 1.214051604270935, D Loss: 1.2761446237564087, alpha: 0.9735579035469157\n",
      "epoch 399 iteration100, G Loss: 1.1576387882232666, D Loss: 1.1937897205352783, alpha: 0.9735579035469157\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 400 iteration0, G Loss: 1.3149741888046265, D Loss: 1.016136646270752, alpha: 0.9734030064231342\n",
      "epoch 400 iteration100, G Loss: 1.5633295774459839, D Loss: 1.3855507373809814, alpha: 0.9734030064231342\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 401 iteration0, G Loss: 1.2503736019134521, D Loss: 1.1418930292129517, alpha: 0.9732472268503066\n",
      "epoch 401 iteration100, G Loss: 1.0963592529296875, D Loss: 0.9869853258132935, alpha: 0.9732472268503066\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 402 iteration0, G Loss: 0.9977116584777832, D Loss: 1.1040685176849365, alpha: 0.9730905600939879\n",
      "epoch 402 iteration100, G Loss: 1.3552114963531494, D Loss: 1.1184027194976807, alpha: 0.9730905600939879\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 403 iteration0, G Loss: 1.3722528219223022, D Loss: 2.0044021606445312, alpha: 0.972933001397773\n",
      "epoch 403 iteration100, G Loss: 1.132218360900879, D Loss: 0.9896483421325684, alpha: 0.972933001397773\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 404 iteration0, G Loss: 1.5491082668304443, D Loss: 1.0857484340667725, alpha: 0.9727745459832368\n",
      "epoch 404 iteration100, G Loss: 1.8050687313079834, D Loss: 1.3866254091262817, alpha: 0.9727745459832368\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 405 iteration0, G Loss: 1.2686959505081177, D Loss: 1.153629183769226, alpha: 0.9726151890498743\n",
      "epoch 405 iteration100, G Loss: 1.0323835611343384, D Loss: 1.567538857460022, alpha: 0.9726151890498743\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 406 iteration0, G Loss: 1.3280143737792969, D Loss: 1.3721779584884644, alpha: 0.9724549257750401\n",
      "epoch 406 iteration100, G Loss: 1.1655532121658325, D Loss: 1.2521700859069824, alpha: 0.9724549257750401\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 407 iteration0, G Loss: 2.6043553352355957, D Loss: 1.4203107357025146, alpha: 0.9722937513138894\n",
      "epoch 407 iteration100, G Loss: 1.3861697912216187, D Loss: 1.2014832496643066, alpha: 0.9722937513138894\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 408 iteration0, G Loss: 0.995151937007904, D Loss: 0.9017228484153748, alpha: 0.9721316607993185\n",
      "epoch 408 iteration100, G Loss: 0.6375359892845154, D Loss: 1.894850254058838, alpha: 0.9721316607993185\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 409 iteration0, G Loss: 0.5731893181800842, D Loss: 1.5702697038650513, alpha: 0.9719686493419065\n",
      "epoch 409 iteration100, G Loss: 1.646803617477417, D Loss: 0.9350039958953857, alpha: 0.9719686493419065\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 410 iteration0, G Loss: 0.8345021605491638, D Loss: 1.1890136003494263, alpha: 0.9718047120298574\n",
      "epoch 410 iteration100, G Loss: 0.7489055395126343, D Loss: 1.244652271270752, alpha: 0.9718047120298574\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 411 iteration0, G Loss: 1.9933468103408813, D Loss: 1.0753310918807983, alpha: 0.9716398439289416\n",
      "epoch 411 iteration100, G Loss: 0.8323336243629456, D Loss: 1.342095136642456, alpha: 0.9716398439289416\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 412 iteration0, G Loss: 1.7090688943862915, D Loss: 1.1043016910552979, alpha: 0.9714740400824391\n",
      "epoch 412 iteration100, G Loss: 1.2989884614944458, D Loss: 1.4272143840789795, alpha: 0.9714740400824391\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 413 iteration0, G Loss: 1.0288704633712769, D Loss: 1.0978423357009888, alpha: 0.9713072955110821\n",
      "epoch 413 iteration100, G Loss: 1.1255769729614258, D Loss: 1.6317318677902222, alpha: 0.9713072955110821\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 414 iteration0, G Loss: 1.1372034549713135, D Loss: 1.2690367698669434, alpha: 0.9711396052129992\n",
      "epoch 414 iteration100, G Loss: 1.8818116188049316, D Loss: 1.0989229679107666, alpha: 0.9711396052129992\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 415 iteration0, G Loss: 0.9957098364830017, D Loss: 1.4867496490478516, alpha: 0.9709709641636592\n",
      "epoch 415 iteration100, G Loss: 1.4187958240509033, D Loss: 1.3646187782287598, alpha: 0.9709709641636592\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 416 iteration0, G Loss: 1.0519518852233887, D Loss: 1.7485157251358032, alpha: 0.9708013673158152\n",
      "epoch 416 iteration100, G Loss: 1.262692928314209, D Loss: 1.038300633430481, alpha: 0.9708013673158152\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 417 iteration0, G Loss: 1.9503209590911865, D Loss: 1.0931856632232666, alpha: 0.9706308095994504\n",
      "epoch 417 iteration100, G Loss: 2.2359402179718018, D Loss: 1.1313893795013428, alpha: 0.9706308095994504\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 418 iteration0, G Loss: 1.5851740837097168, D Loss: 1.190955638885498, alpha: 0.9704592859217226\n",
      "epoch 418 iteration100, G Loss: 1.6327639818191528, D Loss: 1.1242917776107788, alpha: 0.9704592859217226\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 419 iteration0, G Loss: 0.8722495436668396, D Loss: 1.5224761962890625, alpha: 0.9702867911669113\n",
      "epoch 419 iteration100, G Loss: 1.0955954790115356, D Loss: 1.3375658988952637, alpha: 0.9702867911669113\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 420 iteration0, G Loss: 1.099928855895996, D Loss: 1.1726469993591309, alpha: 0.9701133201963638\n",
      "epoch 420 iteration100, G Loss: 1.4300343990325928, D Loss: 1.2942280769348145, alpha: 0.9701133201963638\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 421 iteration0, G Loss: 0.8951770663261414, D Loss: 1.1569629907608032, alpha: 0.9699388678484417\n",
      "epoch 421 iteration100, G Loss: 1.5376183986663818, D Loss: 1.066744327545166, alpha: 0.9699388678484417\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 422 iteration0, G Loss: 1.0831656455993652, D Loss: 0.9168537259101868, alpha: 0.9697634289384699\n",
      "epoch 422 iteration100, G Loss: 2.025102138519287, D Loss: 2.049724817276001, alpha: 0.9697634289384699\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 423 iteration0, G Loss: 0.7980780601501465, D Loss: 1.4125745296478271, alpha: 0.9695869982586831\n",
      "epoch 423 iteration100, G Loss: 1.5886207818984985, D Loss: 1.1982083320617676, alpha: 0.9695869982586831\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 424 iteration0, G Loss: 1.139540195465088, D Loss: 1.0945546627044678, alpha: 0.9694095705781761\n",
      "epoch 424 iteration100, G Loss: 1.1759095191955566, D Loss: 1.4373936653137207, alpha: 0.9694095705781761\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 425 iteration0, G Loss: 1.5885437726974487, D Loss: 1.004584550857544, alpha: 0.969231140642852\n",
      "epoch 425 iteration100, G Loss: 1.4505034685134888, D Loss: 1.2656797170639038, alpha: 0.969231140642852\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 426 iteration0, G Loss: 0.9998024702072144, D Loss: 1.4147980213165283, alpha: 0.9690517031753728\n",
      "epoch 426 iteration100, G Loss: 1.0229698419570923, D Loss: 1.2006518840789795, alpha: 0.9690517031753728\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 427 iteration0, G Loss: 0.93382328748703, D Loss: 1.552509069442749, alpha: 0.9688712528751097\n",
      "epoch 427 iteration100, G Loss: 1.2715733051300049, D Loss: 1.0775916576385498, alpha: 0.9688712528751097\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 428 iteration0, G Loss: 2.356387138366699, D Loss: 0.8226741552352905, alpha: 0.968689784418094\n",
      "epoch 428 iteration100, G Loss: 1.261651635169983, D Loss: 1.1286226511001587, alpha: 0.968689784418094\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 429 iteration0, G Loss: 0.9102622270584106, D Loss: 1.085103154182434, alpha: 0.9685072924569692\n",
      "epoch 429 iteration100, G Loss: 1.4167425632476807, D Loss: 1.4261432886123657, alpha: 0.9685072924569692\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 430 iteration0, G Loss: 2.0392496585845947, D Loss: 1.4238526821136475, alpha: 0.9683237716209436\n",
      "epoch 430 iteration100, G Loss: 1.2613966464996338, D Loss: 0.9886661171913147, alpha: 0.9683237716209436\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 431 iteration0, G Loss: 1.715762734413147, D Loss: 1.4182370901107788, alpha: 0.9681392165157425\n",
      "epoch 431 iteration100, G Loss: 1.621357798576355, D Loss: 0.9677945375442505, alpha: 0.9681392165157425\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 432 iteration0, G Loss: 1.6064547300338745, D Loss: 1.233458161354065, alpha: 0.9679536217235628\n",
      "epoch 432 iteration100, G Loss: 0.9421260952949524, D Loss: 1.4525299072265625, alpha: 0.9679536217235628\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 433 iteration0, G Loss: 1.4887365102767944, D Loss: 1.34108567237854, alpha: 0.9677669818030268\n",
      "epoch 433 iteration100, G Loss: 0.6480473279953003, D Loss: 1.3414660692214966, alpha: 0.9677669818030268\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 434 iteration0, G Loss: 1.2792079448699951, D Loss: 1.1061007976531982, alpha: 0.9675792912891376\n",
      "epoch 434 iteration100, G Loss: 1.185859203338623, D Loss: 1.1965993642807007, alpha: 0.9675792912891376\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 435 iteration0, G Loss: 1.322934627532959, D Loss: 1.1206902265548706, alpha: 0.9673905446932344\n",
      "epoch 435 iteration100, G Loss: 0.9989566802978516, D Loss: 1.256536841392517, alpha: 0.9673905446932344\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 436 iteration0, G Loss: 1.1221431493759155, D Loss: 1.437654733657837, alpha: 0.9672007365029498\n",
      "epoch 436 iteration100, G Loss: 0.9893169403076172, D Loss: 1.194537878036499, alpha: 0.9672007365029498\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 437 iteration0, G Loss: 1.497424840927124, D Loss: 1.1779701709747314, alpha: 0.9670098611821661\n",
      "epoch 437 iteration100, G Loss: 1.4960229396820068, D Loss: 1.3031198978424072, alpha: 0.9670098611821661\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 438 iteration0, G Loss: 0.9698996543884277, D Loss: 1.3355916738510132, alpha: 0.9668179131709738\n",
      "epoch 438 iteration100, G Loss: 1.9545191526412964, D Loss: 1.2528269290924072, alpha: 0.9668179131709738\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 439 iteration0, G Loss: 0.9509428143501282, D Loss: 1.3500702381134033, alpha: 0.9666248868856298\n",
      "epoch 439 iteration100, G Loss: 1.763526201248169, D Loss: 1.107656717300415, alpha: 0.9666248868856298\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 440 iteration0, G Loss: 1.9402769804000854, D Loss: 1.3505672216415405, alpha: 0.9664307767185175\n",
      "epoch 440 iteration100, G Loss: 1.0596415996551514, D Loss: 1.0655796527862549, alpha: 0.9664307767185175\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 441 iteration0, G Loss: 1.195335865020752, D Loss: 1.3325812816619873, alpha: 0.9662355770381064\n",
      "epoch 441 iteration100, G Loss: 1.252098560333252, D Loss: 1.1894510984420776, alpha: 0.9662355770381064\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 442 iteration0, G Loss: 1.389366626739502, D Loss: 1.1535472869873047, alpha: 0.9660392821889131\n",
      "epoch 442 iteration100, G Loss: 1.4850969314575195, D Loss: 1.3838403224945068, alpha: 0.9660392821889131\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 443 iteration0, G Loss: 1.4132657051086426, D Loss: 1.4551337957382202, alpha: 0.965841886491464\n",
      "epoch 443 iteration100, G Loss: 1.5937342643737793, D Loss: 1.3193362951278687, alpha: 0.965841886491464\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 444 iteration0, G Loss: 1.3352528810501099, D Loss: 0.9913173913955688, alpha: 0.9656433842422564\n",
      "epoch 444 iteration100, G Loss: 1.4719486236572266, D Loss: 1.401768445968628, alpha: 0.9656433842422564\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 445 iteration0, G Loss: 1.8099944591522217, D Loss: 1.2168753147125244, alpha: 0.9654437697137235\n",
      "epoch 445 iteration100, G Loss: 1.8524806499481201, D Loss: 1.1797878742218018, alpha: 0.9654437697137235\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 446 iteration0, G Loss: 0.7381435632705688, D Loss: 1.4435697793960571, alpha: 0.9652430371541975\n",
      "epoch 446 iteration100, G Loss: 1.03474760055542, D Loss: 1.6470946073532104, alpha: 0.9652430371541975\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 447 iteration0, G Loss: 1.827398419380188, D Loss: 1.2474610805511475, alpha: 0.9650411807878754\n",
      "epoch 447 iteration100, G Loss: 1.2610721588134766, D Loss: 1.3010125160217285, alpha: 0.9650411807878754\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 448 iteration0, G Loss: 1.20575749874115, D Loss: 1.2186386585235596, alpha: 0.9648381948147847\n",
      "epoch 448 iteration100, G Loss: 0.8250718712806702, D Loss: 1.6103153228759766, alpha: 0.9648381948147847\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 449 iteration0, G Loss: 1.8490458726882935, D Loss: 1.263290524482727, alpha: 0.9646340734107508\n",
      "epoch 449 iteration100, G Loss: 0.6044867634773254, D Loss: 1.8402249813079834, alpha: 0.9646340734107508\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 450 iteration0, G Loss: 2.217989206314087, D Loss: 1.2477028369903564, alpha: 0.9644288107273639\n",
      "epoch 450 iteration100, G Loss: 1.6986989974975586, D Loss: 1.0263535976409912, alpha: 0.9644288107273639\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 451 iteration0, G Loss: 1.8731915950775146, D Loss: 1.0892317295074463, alpha: 0.9642224008919484\n",
      "epoch 451 iteration100, G Loss: 1.2903506755828857, D Loss: 1.1599302291870117, alpha: 0.9642224008919484\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 452 iteration0, G Loss: 1.1660664081573486, D Loss: 1.5780789852142334, alpha: 0.9640148380075328\n",
      "epoch 452 iteration100, G Loss: 1.372926115989685, D Loss: 1.2620904445648193, alpha: 0.9640148380075328\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 453 iteration0, G Loss: 1.6482008695602417, D Loss: 1.2090016603469849, alpha: 0.9638061161528193\n",
      "epoch 453 iteration100, G Loss: 0.9371433258056641, D Loss: 1.036739468574524, alpha: 0.9638061161528193\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 454 iteration0, G Loss: 2.2939887046813965, D Loss: 1.1076017618179321, alpha: 0.9635962293821563\n",
      "epoch 454 iteration100, G Loss: 1.0372782945632935, D Loss: 1.1010740995407104, alpha: 0.9635962293821563\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 455 iteration0, G Loss: 0.7714906930923462, D Loss: 1.9133156538009644, alpha: 0.9633851717255101\n",
      "epoch 455 iteration100, G Loss: 1.631134271621704, D Loss: 1.2217938899993896, alpha: 0.9633851717255101\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 456 iteration0, G Loss: 1.2359662055969238, D Loss: 1.0876004695892334, alpha: 0.9631729371884394\n",
      "epoch 456 iteration100, G Loss: 2.30594801902771, D Loss: 1.6301449537277222, alpha: 0.9631729371884394\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 457 iteration0, G Loss: 2.159358263015747, D Loss: 1.134531021118164, alpha: 0.9629595197520688\n",
      "epoch 457 iteration100, G Loss: 1.3597638607025146, D Loss: 1.0306971073150635, alpha: 0.9629595197520688\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 458 iteration0, G Loss: 1.1878700256347656, D Loss: 1.03226900100708, alpha: 0.962744913373065\n",
      "epoch 458 iteration100, G Loss: 1.2371668815612793, D Loss: 1.5835505723953247, alpha: 0.962744913373065\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 459 iteration0, G Loss: 1.410352110862732, D Loss: 1.1425011157989502, alpha: 0.962529111983613\n",
      "epoch 459 iteration100, G Loss: 1.6919946670532227, D Loss: 1.336395502090454, alpha: 0.962529111983613\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 460 iteration0, G Loss: 1.6372792720794678, D Loss: 1.5176403522491455, alpha: 0.9623121094913941\n",
      "epoch 460 iteration100, G Loss: 1.2210030555725098, D Loss: 1.1947722434997559, alpha: 0.9623121094913941\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 461 iteration0, G Loss: 0.9580379724502563, D Loss: 1.1850934028625488, alpha: 0.9620938997795639\n",
      "epoch 461 iteration100, G Loss: 1.979697823524475, D Loss: 1.5378751754760742, alpha: 0.9620938997795639\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 462 iteration0, G Loss: 0.9141225814819336, D Loss: 1.4261417388916016, alpha: 0.9618744767067332\n",
      "epoch 462 iteration100, G Loss: 1.5138925313949585, D Loss: 1.2460459470748901, alpha: 0.9618744767067332\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 463 iteration0, G Loss: 1.3976072072982788, D Loss: 1.3043456077575684, alpha: 0.9616538341069474\n",
      "epoch 463 iteration100, G Loss: 1.636965036392212, D Loss: 1.1961281299591064, alpha: 0.9616538341069474\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 464 iteration0, G Loss: 1.2891206741333008, D Loss: 1.2745931148529053, alpha: 0.9614319657896698\n",
      "epoch 464 iteration100, G Loss: 1.4317035675048828, D Loss: 1.2103444337844849, alpha: 0.9614319657896698\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 465 iteration0, G Loss: 0.9574834108352661, D Loss: 1.2838544845581055, alpha: 0.9612088655397639\n",
      "epoch 465 iteration100, G Loss: 1.628212571144104, D Loss: 1.1577706336975098, alpha: 0.9612088655397639\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 466 iteration0, G Loss: 1.6073105335235596, D Loss: 1.0083236694335938, alpha: 0.9609845271174783\n",
      "epoch 466 iteration100, G Loss: 3.4922375679016113, D Loss: 1.5916975736618042, alpha: 0.9609845271174783\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 467 iteration0, G Loss: 0.7947908043861389, D Loss: 1.580670952796936, alpha: 0.9607589442584311\n",
      "epoch 467 iteration100, G Loss: 1.4521591663360596, D Loss: 1.4550422430038452, alpha: 0.9607589442584311\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 468 iteration0, G Loss: 1.5943511724472046, D Loss: 1.2750273942947388, alpha: 0.9605321106735979\n",
      "epoch 468 iteration100, G Loss: 1.4756125211715698, D Loss: 1.175570011138916, alpha: 0.9605321106735979\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 469 iteration0, G Loss: 2.6371347904205322, D Loss: 1.0656132698059082, alpha: 0.9603040200492985\n",
      "epoch 469 iteration100, G Loss: 2.520355463027954, D Loss: 1.3569107055664062, alpha: 0.9603040200492985\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 470 iteration0, G Loss: 1.2671581506729126, D Loss: 1.2794692516326904, alpha: 0.9600746660471862\n",
      "epoch 470 iteration100, G Loss: 1.6607171297073364, D Loss: 1.261224627494812, alpha: 0.9600746660471862\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 471 iteration0, G Loss: 1.6031904220581055, D Loss: 1.0983970165252686, alpha: 0.9598440423042385\n",
      "epoch 471 iteration100, G Loss: 1.3559215068817139, D Loss: 1.0291252136230469, alpha: 0.9598440423042385\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 472 iteration0, G Loss: 0.9747613668441772, D Loss: 1.1586122512817383, alpha: 0.959612142432748\n",
      "epoch 472 iteration100, G Loss: 2.8668832778930664, D Loss: 1.339074969291687, alpha: 0.959612142432748\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 473 iteration0, G Loss: 1.321991205215454, D Loss: 1.09187650680542, alpha: 0.9593789600203156\n",
      "epoch 473 iteration100, G Loss: 1.5871343612670898, D Loss: 1.0550737380981445, alpha: 0.9593789600203156\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 474 iteration0, G Loss: 0.993541955947876, D Loss: 1.2186102867126465, alpha: 0.9591444886298444\n",
      "epoch 474 iteration100, G Loss: 2.3229103088378906, D Loss: 1.2421653270721436, alpha: 0.9591444886298444\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 475 iteration0, G Loss: 0.8816101551055908, D Loss: 1.2692166566848755, alpha: 0.958908721799535\n",
      "epoch 475 iteration100, G Loss: 1.555496096611023, D Loss: 1.0469133853912354, alpha: 0.958908721799535\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 476 iteration0, G Loss: 1.1361336708068848, D Loss: 1.2802094221115112, alpha: 0.9586716530428824\n",
      "epoch 476 iteration100, G Loss: 1.4171618223190308, D Loss: 1.2003127336502075, alpha: 0.9586716530428824\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 477 iteration0, G Loss: 1.2535626888275146, D Loss: 1.083927869796753, alpha: 0.9584332758486738\n",
      "epoch 477 iteration100, G Loss: 0.9250844120979309, D Loss: 1.2027652263641357, alpha: 0.9584332758486738\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 478 iteration0, G Loss: 2.2869322299957275, D Loss: 1.22933828830719, alpha: 0.9581935836809882\n",
      "epoch 478 iteration100, G Loss: 1.039998173713684, D Loss: 1.221567153930664, alpha: 0.9581935836809882\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 479 iteration0, G Loss: 1.0880489349365234, D Loss: 1.1920400857925415, alpha: 0.957952569979197\n",
      "epoch 479 iteration100, G Loss: 1.632876992225647, D Loss: 1.0476093292236328, alpha: 0.957952569979197\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 480 iteration0, G Loss: 1.4145959615707397, D Loss: 0.9508059024810791, alpha: 0.9577102281579662\n",
      "epoch 480 iteration100, G Loss: 1.5263642072677612, D Loss: 1.200522780418396, alpha: 0.9577102281579662\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 481 iteration0, G Loss: 1.4702035188674927, D Loss: 1.377966046333313, alpha: 0.9574665516072602\n",
      "epoch 481 iteration100, G Loss: 1.6685971021652222, D Loss: 2.1768338680267334, alpha: 0.9574665516072602\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 482 iteration0, G Loss: 0.893609881401062, D Loss: 1.3490796089172363, alpha: 0.9572215336923464\n",
      "epoch 482 iteration100, G Loss: 1.9168336391448975, D Loss: 1.2502673864364624, alpha: 0.9572215336923464\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 483 iteration0, G Loss: 2.1113176345825195, D Loss: 1.2388982772827148, alpha: 0.9569751677538021\n",
      "epoch 483 iteration100, G Loss: 1.6015998125076294, D Loss: 1.0793060064315796, alpha: 0.9569751677538021\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 484 iteration0, G Loss: 1.5084244012832642, D Loss: 1.1836557388305664, alpha: 0.9567274471075219\n",
      "epoch 484 iteration100, G Loss: 2.424642562866211, D Loss: 1.1331452131271362, alpha: 0.9567274471075219\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 485 iteration0, G Loss: 1.130392074584961, D Loss: 1.0739765167236328, alpha: 0.9564783650447278\n",
      "epoch 485 iteration100, G Loss: 1.3031538724899292, D Loss: 0.7095943689346313, alpha: 0.9564783650447278\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 486 iteration0, G Loss: 0.8312115669250488, D Loss: 1.2197080850601196, alpha: 0.9562279148319796\n",
      "epoch 486 iteration100, G Loss: 1.2816951274871826, D Loss: 1.0453927516937256, alpha: 0.9562279148319796\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 487 iteration0, G Loss: 1.221676230430603, D Loss: 1.034914493560791, alpha: 0.9559760897111875\n",
      "epoch 487 iteration100, G Loss: 0.9928191304206848, D Loss: 0.8348819613456726, alpha: 0.9559760897111875\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 488 iteration0, G Loss: 2.1078426837921143, D Loss: 1.745164394378662, alpha: 0.9557228828996265\n",
      "epoch 488 iteration100, G Loss: 2.067258358001709, D Loss: 1.4035277366638184, alpha: 0.9557228828996265\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 489 iteration0, G Loss: 1.5701261758804321, D Loss: 1.401140809059143, alpha: 0.955468287589951\n",
      "epoch 489 iteration100, G Loss: 1.2194291353225708, D Loss: 0.9918212890625, alpha: 0.955468287589951\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 490 iteration0, G Loss: 1.5853232145309448, D Loss: 1.3213660717010498, alpha: 0.9552122969502133\n",
      "epoch 490 iteration100, G Loss: 1.459606409072876, D Loss: 1.2156898975372314, alpha: 0.9552122969502133\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 491 iteration0, G Loss: 1.039089560508728, D Loss: 0.9347582459449768, alpha: 0.9549549041238813\n",
      "epoch 491 iteration100, G Loss: 1.4631626605987549, D Loss: 2.5803074836730957, alpha: 0.9549549041238813\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 492 iteration0, G Loss: 0.837446928024292, D Loss: 1.1200642585754395, alpha: 0.9546961022298595\n",
      "epoch 492 iteration100, G Loss: 0.8583699464797974, D Loss: 1.2655246257781982, alpha: 0.9546961022298595\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 493 iteration0, G Loss: 2.0873801708221436, D Loss: 0.8683960437774658, alpha: 0.9544358843625106\n",
      "epoch 493 iteration100, G Loss: 1.7323914766311646, D Loss: 1.2459959983825684, alpha: 0.9544358843625106\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 494 iteration0, G Loss: 1.4079657793045044, D Loss: 1.2529165744781494, alpha: 0.95417424359168\n",
      "epoch 494 iteration100, G Loss: 1.7375240325927734, D Loss: 1.2809596061706543, alpha: 0.95417424359168\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 495 iteration0, G Loss: 1.0279738903045654, D Loss: 1.0376884937286377, alpha: 0.9539111729627203\n",
      "epoch 495 iteration100, G Loss: 1.4354023933410645, D Loss: 1.283111333847046, alpha: 0.9539111729627203\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 496 iteration0, G Loss: 0.8116105198860168, D Loss: 1.2784335613250732, alpha: 0.9536466654965192\n",
      "epoch 496 iteration100, G Loss: 1.4210439920425415, D Loss: 1.386268973350525, alpha: 0.9536466654965192\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 497 iteration0, G Loss: 0.9080331921577454, D Loss: 1.322995662689209, alpha: 0.9533807141895276\n",
      "epoch 497 iteration100, G Loss: 1.2209672927856445, D Loss: 0.9104636907577515, alpha: 0.9533807141895276\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 498 iteration0, G Loss: 1.1120504140853882, D Loss: 1.175611972808838, alpha: 0.9531133120137913\n",
      "epoch 498 iteration100, G Loss: 1.7749968767166138, D Loss: 1.1257553100585938, alpha: 0.9531133120137913\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 499 iteration0, G Loss: 0.8354560732841492, D Loss: 1.3800894021987915, alpha: 0.9528444519169822\n",
      "epoch 499 iteration100, G Loss: 2.2425413131713867, D Loss: 1.1330516338348389, alpha: 0.9528444519169822\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 500 iteration0, G Loss: 1.4408562183380127, D Loss: 0.8591414093971252, alpha: 0.9525741268224333\n",
      "epoch 500 iteration100, G Loss: 1.6080068349838257, D Loss: 1.1018403768539429, alpha: 0.9525741268224333\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 501 iteration0, G Loss: 1.802727222442627, D Loss: 1.102051019668579, alpha: 0.9523023296291744\n",
      "epoch 501 iteration100, G Loss: 1.9765833616256714, D Loss: 1.1456315517425537, alpha: 0.9523023296291744\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 502 iteration0, G Loss: 0.8271976113319397, D Loss: 1.3781261444091797, alpha: 0.9520290532119702\n",
      "epoch 502 iteration100, G Loss: 1.7521874904632568, D Loss: 1.1162978410720825, alpha: 0.9520290532119702\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 503 iteration0, G Loss: 1.3118020296096802, D Loss: 1.309122085571289, alpha: 0.9517542904213597\n",
      "epoch 503 iteration100, G Loss: 1.7803987264633179, D Loss: 1.3227245807647705, alpha: 0.9517542904213597\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 504 iteration0, G Loss: 1.7047851085662842, D Loss: 1.3285086154937744, alpha: 0.9514780340836981\n",
      "epoch 504 iteration100, G Loss: 1.1470004320144653, D Loss: 1.2794861793518066, alpha: 0.9514780340836981\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 505 iteration0, G Loss: 1.7324140071868896, D Loss: 1.212087631225586, alpha: 0.9512002770012\n",
      "epoch 505 iteration100, G Loss: 1.3832157850265503, D Loss: 0.9703015089035034, alpha: 0.9512002770012\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 506 iteration0, G Loss: 1.4636338949203491, D Loss: 1.098341464996338, alpha: 0.9509210119519851\n",
      "epoch 506 iteration100, G Loss: 1.738213300704956, D Loss: 1.2347923517227173, alpha: 0.9509210119519851\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 507 iteration0, G Loss: 1.347785472869873, D Loss: 1.4073567390441895, alpha: 0.9506402316901252\n",
      "epoch 507 iteration100, G Loss: 1.68971848487854, D Loss: 1.3134520053863525, alpha: 0.9506402316901252\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 508 iteration0, G Loss: 0.8061713576316833, D Loss: 1.426878809928894, alpha: 0.9503579289456944\n",
      "epoch 508 iteration100, G Loss: 1.722258448600769, D Loss: 1.226853847503662, alpha: 0.9503579289456944\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 509 iteration0, G Loss: 1.2940211296081543, D Loss: 1.355846881866455, alpha: 0.9500740964248193\n",
      "epoch 509 iteration100, G Loss: 0.8857154250144958, D Loss: 1.1387677192687988, alpha: 0.9500740964248193\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 510 iteration0, G Loss: 1.0374292135238647, D Loss: 1.5049493312835693, alpha: 0.9497887268097335\n",
      "epoch 510 iteration100, G Loss: 1.4887360334396362, D Loss: 0.912784218788147, alpha: 0.9497887268097335\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 511 iteration0, G Loss: 1.7431715726852417, D Loss: 1.388986349105835, alpha: 0.9495018127588318\n",
      "epoch 511 iteration100, G Loss: 1.7436869144439697, D Loss: 0.9512941241264343, alpha: 0.9495018127588318\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 512 iteration0, G Loss: 1.1236536502838135, D Loss: 1.316030740737915, alpha: 0.9492133469067291\n",
      "epoch 512 iteration100, G Loss: 1.1300288438796997, D Loss: 1.454770565032959, alpha: 0.9492133469067291\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 513 iteration0, G Loss: 1.711329460144043, D Loss: 1.2242501974105835, alpha: 0.9489233218643187\n",
      "epoch 513 iteration100, G Loss: 1.028732419013977, D Loss: 1.1675548553466797, alpha: 0.9489233218643187\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 514 iteration0, G Loss: 2.145933151245117, D Loss: 1.5181339979171753, alpha: 0.9486317302188345\n",
      "epoch 514 iteration100, G Loss: 0.8337219953536987, D Loss: 1.5323710441589355, alpha: 0.9486317302188345\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 515 iteration0, G Loss: 1.2211865186691284, D Loss: 1.1513913869857788, alpha: 0.9483385645339152\n",
      "epoch 515 iteration100, G Loss: 0.847258448600769, D Loss: 1.161095380783081, alpha: 0.9483385645339152\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 516 iteration0, G Loss: 1.651511549949646, D Loss: 1.1375133991241455, alpha: 0.9480438173496692\n",
      "epoch 516 iteration100, G Loss: 2.6921281814575195, D Loss: 1.9414221048355103, alpha: 0.9480438173496692\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 517 iteration0, G Loss: 1.0673952102661133, D Loss: 0.9705151319503784, alpha: 0.9477474811827441\n",
      "epoch 517 iteration100, G Loss: 1.4665920734405518, D Loss: 2.1298787593841553, alpha: 0.9477474811827441\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 518 iteration0, G Loss: 1.5619757175445557, D Loss: 1.7352330684661865, alpha: 0.9474495485263958\n",
      "epoch 518 iteration100, G Loss: 2.3107945919036865, D Loss: 1.1992007493972778, alpha: 0.9474495485263958\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 519 iteration0, G Loss: 1.746293544769287, D Loss: 0.9801998138427734, alpha: 0.947150011850562\n",
      "epoch 519 iteration100, G Loss: 0.9387986660003662, D Loss: 1.4651598930358887, alpha: 0.947150011850562\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 520 iteration0, G Loss: 1.5671522617340088, D Loss: 1.1458991765975952, alpha: 0.9468488636019363\n",
      "epoch 520 iteration100, G Loss: 1.8860137462615967, D Loss: 0.8774288892745972, alpha: 0.9468488636019363\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 521 iteration0, G Loss: 1.2945126295089722, D Loss: 1.108435869216919, alpha: 0.9465460962040458\n",
      "epoch 521 iteration100, G Loss: 1.452143907546997, D Loss: 1.214124083518982, alpha: 0.9465460962040458\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 522 iteration0, G Loss: 1.0974311828613281, D Loss: 1.4096746444702148, alpha: 0.9462417020573307\n",
      "epoch 522 iteration100, G Loss: 1.5542488098144531, D Loss: 1.3209078311920166, alpha: 0.9462417020573307\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 523 iteration0, G Loss: 2.122652769088745, D Loss: 1.2088292837142944, alpha: 0.945935673539225\n",
      "epoch 523 iteration100, G Loss: 1.4102246761322021, D Loss: 1.2976421117782593, alpha: 0.945935673539225\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 524 iteration0, G Loss: 1.2419955730438232, D Loss: 1.0706478357315063, alpha: 0.9456280030042419\n",
      "epoch 524 iteration100, G Loss: 1.7051773071289062, D Loss: 0.9503039121627808, alpha: 0.9456280030042419\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 525 iteration0, G Loss: 1.538745641708374, D Loss: 1.1633822917938232, alpha: 0.9453186827840593\n",
      "epoch 525 iteration100, G Loss: 1.4261839389801025, D Loss: 1.3190228939056396, alpha: 0.9453186827840593\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 526 iteration0, G Loss: 1.040045976638794, D Loss: 1.0820770263671875, alpha: 0.9450077051876089\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
