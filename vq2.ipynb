{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e150ff-a647-4d14-b4d5-ab6d46fe22b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f8-256\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f8.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f8.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf952f1-4fc4-46fb-84e1-6236a8fd564e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"celeba_256\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"2.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/celeba/celeba-lmdb/\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52101fc-7b67-40be-b60e-8cffe52815af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"lsun\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"1.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/lsun/\",\n",
    "\"--master_port\",\"6088\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"60.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414e48d4-68ae-49af-a4ba-86470b9cf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10'] or True:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior)\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103c482e-774c-4f91-a209-aa2e8e6bf643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/std/2021/21k0005/improved-ddgan/wandb/run-20240917_230623-l80ss9ox</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kotomiya07/TEST/runs/l80ss9ox' target=\"_blank\">vq-f4-256</a></strong> to <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">https://wandb.ai/kotomiya07/TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kotomiya07/TEST/runs/l80ss9ox' target=\"_blank\">https://wandb.ai/kotomiya07/TEST/runs/l80ss9ox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kotomiya07/TEST/runs/l80ss9ox?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe8af8c7580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"TEST\",\n",
    "            name=args.exp,\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"channels_dae\": args.num_channels_dae,\n",
    "                \"ch_nult\": args.ch_mult,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"res_blocks\": args.num_res_blocks,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"embedding_type\": args.embedding_type,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"z_emb_dim\": args.z_emb_dim,\n",
    "                \"attn_resolutions\": args.attn_resolutions,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17debf4-678d-48e4-94c3-015065223e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "=> loaded checkpoint (epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 iteration0, G Loss: 1.1906884908676147, D Loss: 1.396654486656189, alpha: 0.9974674681467623\n",
      "epoch 1 iteration100, G Loss: 0.704937219619751, D Loss: 1.3863120079040527, alpha: 0.9974674681467623\n",
      "epoch 1 iteration200, G Loss: 0.7060672044754028, D Loss: 1.386368989944458, alpha: 0.9974674681467623\n",
      "epoch 1 iteration300, G Loss: 0.7088481783866882, D Loss: 1.3862414360046387, alpha: 0.9974674681467623\n",
      "epoch 1 iteration400, G Loss: 0.7028843760490417, D Loss: 1.386204481124878, alpha: 0.9974674681467623\n",
      "epoch 1 iteration500, G Loss: 0.6994001269340515, D Loss: 1.3862820863723755, alpha: 0.9974674681467623\n",
      "epoch 1 iteration600, G Loss: 0.7081409692764282, D Loss: 1.3860225677490234, alpha: 0.9974674681467623\n",
      "epoch 1 iteration700, G Loss: 1.597832202911377, D Loss: 1.7965446710586548, alpha: 0.9974674681467623\n",
      "epoch 1 iteration800, G Loss: 0.701684296131134, D Loss: 1.3860929012298584, alpha: 0.9974674681467623\n",
      "epoch 1 iteration900, G Loss: 0.7026692032814026, D Loss: 1.3864991664886475, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1000, G Loss: 0.6929638981819153, D Loss: 1.3864073753356934, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1100, G Loss: 0.7005866169929504, D Loss: 1.386251449584961, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1200, G Loss: 0.7072696089744568, D Loss: 1.386183261871338, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1300, G Loss: 0.6695861220359802, D Loss: 1.3865489959716797, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1400, G Loss: 0.7171077132225037, D Loss: 1.386095404624939, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1500, G Loss: 0.6846300363540649, D Loss: 1.3864078521728516, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1600, G Loss: 0.7389342188835144, D Loss: 1.388404130935669, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1700, G Loss: 0.7073379755020142, D Loss: 1.3859834671020508, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1800, G Loss: 0.7200145125389099, D Loss: 1.3873937129974365, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1900, G Loss: 0.6109075546264648, D Loss: 1.3884857892990112, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2000, G Loss: 0.7017436027526855, D Loss: 1.385883092880249, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2100, G Loss: 0.7683566212654114, D Loss: 1.3857511281967163, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2200, G Loss: 0.7046748399734497, D Loss: 1.3862522840499878, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2300, G Loss: 0.6937839984893799, D Loss: 1.3865866661071777, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2400, G Loss: 0.5757134556770325, D Loss: 1.3957433700561523, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2500, G Loss: 0.706429123878479, D Loss: 1.3866970539093018, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2600, G Loss: 0.6663786172866821, D Loss: 1.3902826309204102, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2700, G Loss: 0.7139277458190918, D Loss: 1.3859574794769287, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2800, G Loss: 0.7074839472770691, D Loss: 1.3862382173538208, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2900, G Loss: 0.7863652110099792, D Loss: 1.3889503479003906, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3000, G Loss: 0.7053182721138, D Loss: 1.3856747150421143, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3100, G Loss: 0.7090469598770142, D Loss: 1.386121153831482, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3200, G Loss: 0.6948316693305969, D Loss: 1.3887414932250977, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3300, G Loss: 0.3982779383659363, D Loss: 1.4167976379394531, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3400, G Loss: 0.6653205752372742, D Loss: 1.4303669929504395, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3500, G Loss: 0.6347365379333496, D Loss: 1.3875758647918701, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3600, G Loss: 0.6937941908836365, D Loss: 1.385507583618164, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3700, G Loss: 0.7042725682258606, D Loss: 1.3861768245697021, alpha: 0.9974674681467623\n",
      "Saving content.\n",
      "epoch 2 iteration0, G Loss: 0.7288994193077087, D Loss: 1.3863866329193115, alpha: 0.997406111708621\n",
      "epoch 2 iteration100, G Loss: 0.6719657778739929, D Loss: 1.3868498802185059, alpha: 0.997406111708621\n",
      "epoch 2 iteration200, G Loss: 0.7127041816711426, D Loss: 1.3854506015777588, alpha: 0.997406111708621\n",
      "epoch 2 iteration300, G Loss: 0.7028262615203857, D Loss: 1.3867415189743042, alpha: 0.997406111708621\n",
      "epoch 2 iteration400, G Loss: 0.7061275839805603, D Loss: 1.3864439725875854, alpha: 0.997406111708621\n",
      "epoch 2 iteration500, G Loss: 0.7997363209724426, D Loss: 1.388607144355774, alpha: 0.997406111708621\n",
      "epoch 2 iteration600, G Loss: 0.5400274991989136, D Loss: 1.473559856414795, alpha: 0.997406111708621\n",
      "epoch 2 iteration700, G Loss: 0.6897477507591248, D Loss: 1.4034581184387207, alpha: 0.997406111708621\n",
      "epoch 2 iteration800, G Loss: 0.681108832359314, D Loss: 1.3865253925323486, alpha: 0.997406111708621\n",
      "epoch 2 iteration900, G Loss: 0.7277839183807373, D Loss: 1.386625051498413, alpha: 0.997406111708621\n",
      "epoch 2 iteration1000, G Loss: 0.7238662838935852, D Loss: 1.3865801095962524, alpha: 0.997406111708621\n",
      "epoch 2 iteration1100, G Loss: 0.6704652309417725, D Loss: 1.5868133306503296, alpha: 0.997406111708621\n",
      "epoch 2 iteration1200, G Loss: 0.8109114170074463, D Loss: 1.3880999088287354, alpha: 0.997406111708621\n",
      "epoch 2 iteration1300, G Loss: 0.7003293633460999, D Loss: 1.3870174884796143, alpha: 0.997406111708621\n",
      "epoch 2 iteration1400, G Loss: 0.709347665309906, D Loss: 1.3865649700164795, alpha: 0.997406111708621\n",
      "epoch 2 iteration1500, G Loss: 0.7174423336982727, D Loss: 1.3858534097671509, alpha: 0.997406111708621\n",
      "epoch 2 iteration1600, G Loss: 0.7196836471557617, D Loss: 1.3860790729522705, alpha: 0.997406111708621\n",
      "epoch 2 iteration1700, G Loss: 0.6780956387519836, D Loss: 1.386544942855835, alpha: 0.997406111708621\n",
      "epoch 2 iteration1800, G Loss: 0.6902485489845276, D Loss: 1.3864378929138184, alpha: 0.997406111708621\n",
      "epoch 2 iteration1900, G Loss: 0.6998613476753235, D Loss: 1.38596773147583, alpha: 0.997406111708621\n",
      "epoch 2 iteration2000, G Loss: 0.6752265691757202, D Loss: 1.3850303888320923, alpha: 0.997406111708621\n",
      "epoch 2 iteration2100, G Loss: 0.8696454167366028, D Loss: 1.3951270580291748, alpha: 0.997406111708621\n",
      "epoch 2 iteration2200, G Loss: 0.7078135013580322, D Loss: 1.3859055042266846, alpha: 0.997406111708621\n",
      "epoch 2 iteration2300, G Loss: 0.7052112817764282, D Loss: 1.3851675987243652, alpha: 0.997406111708621\n",
      "epoch 2 iteration2400, G Loss: 0.7255666255950928, D Loss: 1.386671543121338, alpha: 0.997406111708621\n",
      "epoch 2 iteration2500, G Loss: 0.7074810266494751, D Loss: 1.3858139514923096, alpha: 0.997406111708621\n",
      "epoch 2 iteration2600, G Loss: 0.6919801831245422, D Loss: 1.3843481540679932, alpha: 0.997406111708621\n",
      "epoch 2 iteration2700, G Loss: 0.7043112516403198, D Loss: 1.3862998485565186, alpha: 0.997406111708621\n",
      "epoch 2 iteration2800, G Loss: 0.7068913578987122, D Loss: 1.3878092765808105, alpha: 0.997406111708621\n",
      "epoch 2 iteration2900, G Loss: 0.7217629551887512, D Loss: 1.3865740299224854, alpha: 0.997406111708621\n",
      "epoch 2 iteration3000, G Loss: 0.7065844535827637, D Loss: 1.383082628250122, alpha: 0.997406111708621\n",
      "epoch 2 iteration3100, G Loss: 0.6987379789352417, D Loss: 1.3849483728408813, alpha: 0.997406111708621\n",
      "epoch 2 iteration3200, G Loss: 0.6949871182441711, D Loss: 1.3850377798080444, alpha: 0.997406111708621\n",
      "epoch 2 iteration3300, G Loss: 0.7070338726043701, D Loss: 1.3849412202835083, alpha: 0.997406111708621\n",
      "epoch 2 iteration3400, G Loss: 0.7223453521728516, D Loss: 1.3846783638000488, alpha: 0.997406111708621\n",
      "epoch 2 iteration3500, G Loss: 0.701170802116394, D Loss: 1.3858234882354736, alpha: 0.997406111708621\n",
      "epoch 2 iteration3600, G Loss: 0.7278695702552795, D Loss: 1.385927438735962, alpha: 0.997406111708621\n",
      "epoch 2 iteration3700, G Loss: 0.7055909037590027, D Loss: 1.3853884935379028, alpha: 0.997406111708621\n",
      "Saving content.\n",
      "epoch 3 iteration0, G Loss: 0.7454013228416443, D Loss: 1.3871426582336426, alpha: 0.9973432727281952\n",
      "epoch 3 iteration100, G Loss: 0.7039435505867004, D Loss: 1.386504054069519, alpha: 0.9973432727281952\n",
      "epoch 3 iteration200, G Loss: 0.645559549331665, D Loss: 1.387143850326538, alpha: 0.9973432727281952\n",
      "epoch 3 iteration300, G Loss: 0.700372576713562, D Loss: 1.3869438171386719, alpha: 0.9973432727281952\n",
      "epoch 3 iteration400, G Loss: 0.7084789276123047, D Loss: 1.38652765750885, alpha: 0.9973432727281952\n",
      "epoch 3 iteration500, G Loss: 0.5157685279846191, D Loss: 1.391117811203003, alpha: 0.9973432727281952\n",
      "epoch 3 iteration600, G Loss: 0.7889717817306519, D Loss: 1.3881889581680298, alpha: 0.9973432727281952\n",
      "epoch 3 iteration700, G Loss: 0.6628555655479431, D Loss: 1.387213945388794, alpha: 0.9973432727281952\n",
      "epoch 3 iteration800, G Loss: 0.7320125699043274, D Loss: 1.3853542804718018, alpha: 0.9973432727281952\n",
      "epoch 3 iteration900, G Loss: 0.7012309432029724, D Loss: 1.3868556022644043, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1000, G Loss: 0.6962184906005859, D Loss: 1.3867015838623047, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1100, G Loss: 0.7091907858848572, D Loss: 1.386655330657959, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1200, G Loss: 0.7389764189720154, D Loss: 1.452581763267517, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1300, G Loss: 0.7218264937400818, D Loss: 1.3857007026672363, alpha: 0.9973432727281952\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e13c6-9f5d-4b5d-8d40-fe862e1f0919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
