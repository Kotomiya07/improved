{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7afbb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f8-256\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f8.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f8.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068aa0b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"celeba_256\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"2.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/celeba/celeba-lmdb/\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72183620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"lsun\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f8-256\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"1.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/lsun/\",\n",
    "\"--master_port\",\"6088\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"60.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f8.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f8.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d32835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10'] or True:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior)\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fb70d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/std/2021/21k0005/improved-ddgan/wandb/run-20240918_134717-ghxo9rnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kotomiya07/TEST/runs/ghxo9rnt' target=\"_blank\">vq-f8-256</a></strong> to <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">https://wandb.ai/kotomiya07/TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kotomiya07/TEST/runs/ghxo9rnt' target=\"_blank\">https://wandb.ai/kotomiya07/TEST/runs/ghxo9rnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kotomiya07/TEST/runs/ghxo9rnt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0270f9ba90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"TEST\",\n",
    "            name=args.exp,\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"channels_dae\": args.num_channels_dae,\n",
    "                \"ch_nult\": args.ch_mult,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"res_blocks\": args.num_res_blocks,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"embedding_type\": args.embedding_type,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"z_emb_dim\": args.z_emb_dim,\n",
    "                \"attn_resolutions\": args.attn_resolutions,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32498549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration0, G Loss: 3.185203790664673, D Loss: 1.3873904943466187, alpha: 0.9975273768433652\n",
      "epoch 0 iteration100, G Loss: 0.7883992195129395, D Loss: 1.3881330490112305, alpha: 0.9975273768433652\n",
      "epoch 0 iteration200, G Loss: 0.7020480036735535, D Loss: 1.3872854709625244, alpha: 0.9975273768433652\n",
      "epoch 0 iteration300, G Loss: 0.7587946653366089, D Loss: 1.4047415256500244, alpha: 0.9975273768433652\n",
      "epoch 0 iteration400, G Loss: 0.7169008851051331, D Loss: 1.387438416481018, alpha: 0.9975273768433652\n",
      "epoch 0 iteration500, G Loss: 0.7146915197372437, D Loss: 1.386303186416626, alpha: 0.9975273768433652\n",
      "epoch 0 iteration600, G Loss: 0.6772194504737854, D Loss: 1.3929647207260132, alpha: 0.9975273768433652\n",
      "epoch 0 iteration700, G Loss: 0.693682074546814, D Loss: 1.4016062021255493, alpha: 0.9975273768433652\n",
      "epoch 0 iteration800, G Loss: 0.7209129333496094, D Loss: 1.3857510089874268, alpha: 0.9975273768433652\n",
      "epoch 0 iteration900, G Loss: 0.7144039869308472, D Loss: 1.3889442682266235, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1000, G Loss: 0.7071077227592468, D Loss: 1.386608362197876, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1100, G Loss: 0.7428786754608154, D Loss: 1.3866852521896362, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1200, G Loss: 0.6490824222564697, D Loss: 1.3952666521072388, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1300, G Loss: 0.6475988030433655, D Loss: 1.4006611108779907, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1400, G Loss: 0.7190750241279602, D Loss: 1.3875186443328857, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1500, G Loss: 0.6509254574775696, D Loss: 1.3877382278442383, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1600, G Loss: 0.7058733701705933, D Loss: 1.3874808549880981, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1700, G Loss: 0.5971091389656067, D Loss: 1.3880138397216797, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1800, G Loss: 0.712715744972229, D Loss: 1.386262059211731, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1900, G Loss: 0.7340002059936523, D Loss: 1.3871567249298096, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2000, G Loss: 0.7062861919403076, D Loss: 1.3860721588134766, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2100, G Loss: 0.7060566544532776, D Loss: 1.3861724138259888, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2200, G Loss: 0.673761785030365, D Loss: 1.387502908706665, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2300, G Loss: 0.7067661881446838, D Loss: 1.386094331741333, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2400, G Loss: 0.7070552110671997, D Loss: 1.3854148387908936, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2500, G Loss: 0.7043565511703491, D Loss: 1.3862930536270142, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2600, G Loss: 0.7052357196807861, D Loss: 1.3862831592559814, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2700, G Loss: 0.7418100833892822, D Loss: 1.4024640321731567, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2800, G Loss: 0.6698503494262695, D Loss: 1.3894150257110596, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2900, G Loss: 0.6977275609970093, D Loss: 1.3860894441604614, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3000, G Loss: 0.6991503238677979, D Loss: 1.3859381675720215, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3100, G Loss: 0.7024574279785156, D Loss: 1.386056900024414, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3200, G Loss: 0.7103514075279236, D Loss: 1.3858311176300049, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3300, G Loss: 0.6427513957023621, D Loss: 1.3867626190185547, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3400, G Loss: 0.6974334120750427, D Loss: 1.387023687362671, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3500, G Loss: 0.7002435326576233, D Loss: 1.3862848281860352, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3600, G Loss: 0.7061522603034973, D Loss: 1.386504888534546, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3700, G Loss: 0.7063308954238892, D Loss: 1.3864656686782837, alpha: 0.9975273768433652\n",
      "Saving content.\n",
      "epoch 1 iteration0, G Loss: 0.7042051553726196, D Loss: 1.386418342590332, alpha: 0.9974674681467623\n",
      "epoch 1 iteration100, G Loss: 0.6967397332191467, D Loss: 1.3881924152374268, alpha: 0.9974674681467623\n",
      "epoch 1 iteration200, G Loss: 0.7043854594230652, D Loss: 1.3863157033920288, alpha: 0.9974674681467623\n",
      "epoch 1 iteration300, G Loss: 0.7249237298965454, D Loss: 1.3869786262512207, alpha: 0.9974674681467623\n",
      "epoch 1 iteration400, G Loss: 0.7133597135543823, D Loss: 1.3862814903259277, alpha: 0.9974674681467623\n",
      "epoch 1 iteration500, G Loss: 0.7061545848846436, D Loss: 1.3865940570831299, alpha: 0.9974674681467623\n",
      "epoch 1 iteration600, G Loss: 0.7022172808647156, D Loss: 1.3871822357177734, alpha: 0.9974674681467623\n",
      "epoch 1 iteration700, G Loss: 0.7124233841896057, D Loss: 1.3862040042877197, alpha: 0.9974674681467623\n",
      "epoch 1 iteration800, G Loss: 0.7135782241821289, D Loss: 1.3862806558609009, alpha: 0.9974674681467623\n",
      "epoch 1 iteration900, G Loss: 0.6964535713195801, D Loss: 1.38631010055542, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1000, G Loss: 0.7059587240219116, D Loss: 1.3864574432373047, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1100, G Loss: 0.705690860748291, D Loss: 1.3862457275390625, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1200, G Loss: 0.6934570670127869, D Loss: 1.3862555027008057, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1300, G Loss: 0.7020646333694458, D Loss: 1.3864233493804932, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1400, G Loss: 0.7051233649253845, D Loss: 1.3868743181228638, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1500, G Loss: 0.7118592262268066, D Loss: 1.386307716369629, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1600, G Loss: 0.7165535688400269, D Loss: 1.388171672821045, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1700, G Loss: 0.707343578338623, D Loss: 1.3862054347991943, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1800, G Loss: 0.7038343548774719, D Loss: 1.3861709833145142, alpha: 0.9974674681467623\n",
      "epoch 1 iteration1900, G Loss: 0.7036511301994324, D Loss: 1.3863056898117065, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2000, G Loss: 0.70122891664505, D Loss: 1.3865058422088623, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2100, G Loss: 0.7006441354751587, D Loss: 1.3865456581115723, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2200, G Loss: 0.702401340007782, D Loss: 1.3863630294799805, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2300, G Loss: 0.7047266364097595, D Loss: 1.3863027095794678, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2400, G Loss: 0.7349061369895935, D Loss: 1.385948657989502, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2500, G Loss: 0.672466516494751, D Loss: 1.3863508701324463, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2600, G Loss: 0.7178467512130737, D Loss: 1.3863548040390015, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2700, G Loss: 0.7052208781242371, D Loss: 1.386616587638855, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2800, G Loss: 0.6929880380630493, D Loss: 1.3890910148620605, alpha: 0.9974674681467623\n",
      "epoch 1 iteration2900, G Loss: 0.6965967416763306, D Loss: 1.386188268661499, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3000, G Loss: 0.7138988971710205, D Loss: 1.387214183807373, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3100, G Loss: 0.6784026622772217, D Loss: 1.3869776725769043, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3200, G Loss: 0.7055773735046387, D Loss: 1.3862779140472412, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3300, G Loss: 0.7055490612983704, D Loss: 1.3863837718963623, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3400, G Loss: 0.7003469467163086, D Loss: 1.3861274719238281, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3500, G Loss: 0.7076542973518372, D Loss: 1.3863977193832397, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3600, G Loss: 0.7253660559654236, D Loss: 1.3864070177078247, alpha: 0.9974674681467623\n",
      "epoch 1 iteration3700, G Loss: 0.7259640097618103, D Loss: 1.3868110179901123, alpha: 0.9974674681467623\n",
      "Saving content.\n",
      "epoch 2 iteration0, G Loss: 0.7175021767616272, D Loss: 1.3864929676055908, alpha: 0.997406111708621\n",
      "epoch 2 iteration100, G Loss: 0.6997067928314209, D Loss: 1.3863483667373657, alpha: 0.997406111708621\n",
      "epoch 2 iteration200, G Loss: 0.7189398407936096, D Loss: 1.3864777088165283, alpha: 0.997406111708621\n",
      "epoch 2 iteration300, G Loss: 0.7089613080024719, D Loss: 1.3864130973815918, alpha: 0.997406111708621\n",
      "epoch 2 iteration400, G Loss: 0.7186949253082275, D Loss: 1.386683702468872, alpha: 0.997406111708621\n",
      "epoch 2 iteration500, G Loss: 0.6962264180183411, D Loss: 1.386575698852539, alpha: 0.997406111708621\n",
      "epoch 2 iteration600, G Loss: 0.7282392978668213, D Loss: 1.387476921081543, alpha: 0.997406111708621\n",
      "epoch 2 iteration700, G Loss: 0.7020490169525146, D Loss: 1.3863611221313477, alpha: 0.997406111708621\n",
      "epoch 2 iteration800, G Loss: 0.7025123238563538, D Loss: 1.3861658573150635, alpha: 0.997406111708621\n",
      "epoch 2 iteration900, G Loss: 0.7012608051300049, D Loss: 1.3862249851226807, alpha: 0.997406111708621\n",
      "epoch 2 iteration1000, G Loss: 0.7173107266426086, D Loss: 1.3865671157836914, alpha: 0.997406111708621\n",
      "epoch 2 iteration1100, G Loss: 0.703943133354187, D Loss: 1.3870153427124023, alpha: 0.997406111708621\n",
      "epoch 2 iteration1200, G Loss: 0.7092920541763306, D Loss: 1.3879553079605103, alpha: 0.997406111708621\n",
      "epoch 2 iteration1300, G Loss: 0.7179978489875793, D Loss: 1.3864357471466064, alpha: 0.997406111708621\n",
      "epoch 2 iteration1400, G Loss: 0.7038435339927673, D Loss: 1.3863176107406616, alpha: 0.997406111708621\n",
      "epoch 2 iteration1500, G Loss: 0.6884047985076904, D Loss: 1.3856419324874878, alpha: 0.997406111708621\n",
      "epoch 2 iteration1600, G Loss: 0.7006083130836487, D Loss: 1.3865087032318115, alpha: 0.997406111708621\n",
      "epoch 2 iteration1700, G Loss: 0.6843534111976624, D Loss: 1.3860937356948853, alpha: 0.997406111708621\n",
      "epoch 2 iteration1800, G Loss: 0.712418258190155, D Loss: 1.3882102966308594, alpha: 0.997406111708621\n",
      "epoch 2 iteration1900, G Loss: 0.7064822316169739, D Loss: 1.3863263130187988, alpha: 0.997406111708621\n",
      "epoch 2 iteration2000, G Loss: 0.7145890593528748, D Loss: 1.3864015340805054, alpha: 0.997406111708621\n",
      "epoch 2 iteration2100, G Loss: 0.7077717781066895, D Loss: 1.386348843574524, alpha: 0.997406111708621\n",
      "epoch 2 iteration2200, G Loss: 0.6997994780540466, D Loss: 1.3866040706634521, alpha: 0.997406111708621\n",
      "epoch 2 iteration2300, G Loss: 0.7024259567260742, D Loss: 1.3863402605056763, alpha: 0.997406111708621\n",
      "epoch 2 iteration2400, G Loss: 0.7158132195472717, D Loss: 1.3861008882522583, alpha: 0.997406111708621\n",
      "epoch 2 iteration2500, G Loss: 0.7138659358024597, D Loss: 1.387243390083313, alpha: 0.997406111708621\n",
      "epoch 2 iteration2600, G Loss: 0.7060189247131348, D Loss: 1.3863143920898438, alpha: 0.997406111708621\n",
      "epoch 2 iteration2700, G Loss: 0.6997250914573669, D Loss: 1.387526035308838, alpha: 0.997406111708621\n",
      "epoch 2 iteration2800, G Loss: 0.6973465085029602, D Loss: 1.3863695859909058, alpha: 0.997406111708621\n",
      "epoch 2 iteration2900, G Loss: 0.6825023889541626, D Loss: 1.3867952823638916, alpha: 0.997406111708621\n",
      "epoch 2 iteration3000, G Loss: 0.7020396590232849, D Loss: 1.3856050968170166, alpha: 0.997406111708621\n",
      "epoch 2 iteration3100, G Loss: 0.6821753978729248, D Loss: 1.3860275745391846, alpha: 0.997406111708621\n",
      "epoch 2 iteration3200, G Loss: 0.6699609160423279, D Loss: 1.3897573947906494, alpha: 0.997406111708621\n",
      "epoch 2 iteration3300, G Loss: 0.6905373930931091, D Loss: 1.3864858150482178, alpha: 0.997406111708621\n",
      "epoch 2 iteration3400, G Loss: 0.6990227103233337, D Loss: 1.3867413997650146, alpha: 0.997406111708621\n",
      "epoch 2 iteration3500, G Loss: 0.708821177482605, D Loss: 1.386277198791504, alpha: 0.997406111708621\n",
      "epoch 2 iteration3600, G Loss: 0.7050009965896606, D Loss: 1.3866908550262451, alpha: 0.997406111708621\n",
      "epoch 2 iteration3700, G Loss: 0.7032898664474487, D Loss: 1.386344075202942, alpha: 0.997406111708621\n",
      "Saving content.\n",
      "epoch 3 iteration0, G Loss: 0.7421661615371704, D Loss: 1.3867027759552002, alpha: 0.9973432727281952\n",
      "epoch 3 iteration100, G Loss: 0.703711211681366, D Loss: 1.3865478038787842, alpha: 0.9973432727281952\n",
      "epoch 3 iteration200, G Loss: 0.7064245343208313, D Loss: 1.38639235496521, alpha: 0.9973432727281952\n",
      "epoch 3 iteration300, G Loss: 0.7039251923561096, D Loss: 1.3863742351531982, alpha: 0.9973432727281952\n",
      "epoch 3 iteration400, G Loss: 0.704563319683075, D Loss: 1.386488437652588, alpha: 0.9973432727281952\n",
      "epoch 3 iteration500, G Loss: 0.7005274891853333, D Loss: 1.3865723609924316, alpha: 0.9973432727281952\n",
      "epoch 3 iteration600, G Loss: 0.7036410570144653, D Loss: 1.386540174484253, alpha: 0.9973432727281952\n",
      "epoch 3 iteration700, G Loss: 0.6970049142837524, D Loss: 1.3864154815673828, alpha: 0.9973432727281952\n",
      "epoch 3 iteration800, G Loss: 0.6918938159942627, D Loss: 1.3865420818328857, alpha: 0.9973432727281952\n",
      "epoch 3 iteration900, G Loss: 0.7066704630851746, D Loss: 1.3864842653274536, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1000, G Loss: 0.702229380607605, D Loss: 1.38643217086792, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1100, G Loss: 0.7009739875793457, D Loss: 1.3864014148712158, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1200, G Loss: 0.6922019720077515, D Loss: 1.3917150497436523, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1300, G Loss: 0.7010412216186523, D Loss: 1.386418104171753, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1400, G Loss: 0.7024027705192566, D Loss: 1.386306643486023, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1500, G Loss: 0.7063599824905396, D Loss: 1.386634349822998, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1600, G Loss: 0.706076979637146, D Loss: 1.386602759361267, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1700, G Loss: 0.7069529891014099, D Loss: 1.3861565589904785, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1800, G Loss: 0.6842123866081238, D Loss: 1.3862249851226807, alpha: 0.9973432727281952\n",
      "epoch 3 iteration1900, G Loss: 0.7155439853668213, D Loss: 1.386523962020874, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2000, G Loss: 0.7639287114143372, D Loss: 1.3871417045593262, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2100, G Loss: 0.705993115901947, D Loss: 1.3863270282745361, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2200, G Loss: 0.7353696823120117, D Loss: 1.3862452507019043, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2300, G Loss: 0.7307490110397339, D Loss: 1.387526035308838, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2400, G Loss: 0.7232128381729126, D Loss: 1.3869452476501465, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2500, G Loss: 0.7612831592559814, D Loss: 1.3874179124832153, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2600, G Loss: 0.6745485663414001, D Loss: 1.3873080015182495, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2700, G Loss: 0.7066643238067627, D Loss: 1.387055516242981, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2800, G Loss: 0.7056658267974854, D Loss: 1.3865900039672852, alpha: 0.9973432727281952\n",
      "epoch 3 iteration2900, G Loss: 0.7105563282966614, D Loss: 1.3864336013793945, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3000, G Loss: 0.7110435366630554, D Loss: 1.3860881328582764, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3100, G Loss: 0.686729907989502, D Loss: 1.3863308429718018, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3200, G Loss: 0.7036532759666443, D Loss: 1.3864364624023438, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3300, G Loss: 0.6985276341438293, D Loss: 1.386307954788208, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3400, G Loss: 0.7034853100776672, D Loss: 1.3864445686340332, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3500, G Loss: 0.7055197358131409, D Loss: 1.3865368366241455, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3600, G Loss: 0.6958253383636475, D Loss: 1.386225700378418, alpha: 0.9973432727281952\n",
      "epoch 3 iteration3700, G Loss: 0.7103610634803772, D Loss: 1.3862431049346924, alpha: 0.9973432727281952\n",
      "Saving content.\n",
      "epoch 4 iteration0, G Loss: 0.6994068026542664, D Loss: 1.3864271640777588, alpha: 0.9972789155772751\n",
      "epoch 4 iteration100, G Loss: 0.7099364995956421, D Loss: 1.3861454725265503, alpha: 0.9972789155772751\n",
      "epoch 4 iteration200, G Loss: 0.7006394863128662, D Loss: 1.3861435651779175, alpha: 0.9972789155772751\n",
      "epoch 4 iteration300, G Loss: 0.704929769039154, D Loss: 1.3859988451004028, alpha: 0.9972789155772751\n",
      "epoch 4 iteration400, G Loss: 0.6953719258308411, D Loss: 1.3861721754074097, alpha: 0.9972789155772751\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906a89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iddgan",
   "language": "python",
   "name": "iddgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
