{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e150ff-a647-4d14-b4d5-ab6d46fe22b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f8-256\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f8.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f8.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf952f1-4fc4-46fb-84e1-6236a8fd564e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"celeba_256\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"2.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/celeba/celeba-lmdb/\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52101fc-7b67-40be-b60e-8cffe52815af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"lsun\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256-likeCelebA\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"2.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/lsun/\",\n",
    "\"--master_port\",\"6088\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be97eb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f115c636b90>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f113b796080, raw_cell=\"from get_args import get_args\n",
      "args = [\n",
      "\"--dataset\"..\" store_history=True silent=False shell_futures=True cell_id=1507affa-ddc2-4256-b775-d9e74f7b2f6c>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:449\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:735\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:363\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:225\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    223\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m    224\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:157\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    152\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:132\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    130\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f115c636b90>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f113b795150, execution_count=5 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f113b796080, raw_cell=\"from get_args import get_args\n",
      "args = [\n",
      "\"--dataset\"..\" store_history=True silent=False shell_futures=True cell_id=1507affa-ddc2-4256-b775-d9e74f7b2f6c> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:444\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:727\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:359\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:225\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    223\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m    224\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:157\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    152\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iddgan/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:132\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    130\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"lsun\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"4\",\n",
    "\"--num_res_blocks\",\"3\",\n",
    "\"--batch_size\",\"8\",\n",
    "\"--num_epoch\",\"1000\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"50\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"1.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/lsun/\",\n",
    "\"--master_port\",\"6088\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"60.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414e48d4-68ae-49af-a4ba-86470b9cf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10'] or True:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior)\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103c482e-774c-4f91-a209-aa2e8e6bf643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/std/2021/21k0005/improved-ddgan/wandb/run-20241101_235806-jjvr8ixy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kotomiya07/LSUN/runs/jjvr8ixy' target=\"_blank\">vq-f4</a></strong> to <a href='https://wandb.ai/kotomiya07/LSUN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kotomiya07/LSUN' target=\"_blank\">https://wandb.ai/kotomiya07/LSUN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kotomiya07/LSUN/runs/jjvr8ixy' target=\"_blank\">https://wandb.ai/kotomiya07/LSUN/runs/jjvr8ixy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kotomiya07/LSUN/runs/jjvr8ixy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f105f5b79a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"LSUN\",\n",
    "            name=args.exp,\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"channels_dae\": args.num_channels_dae,\n",
    "                \"ch_nult\": args.ch_mult,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"res_blocks\": args.num_res_blocks,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"embedding_type\": args.embedding_type,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"z_emb_dim\": args.z_emb_dim,\n",
    "                \"attn_resolutions\": args.attn_resolutions,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17debf4-678d-48e4-94c3-015065223e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/std/2021/21k0005/anaconda3/envs/iddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/users/std/2021/21k0005/anaconda3/envs/iddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration0, G Loss: 0.8468887209892273, D Loss: 1.3890571594238281, alpha: 0.9975273768433652\n",
      "epoch 0 iteration100, G Loss: 1.9246770143508911, D Loss: 1.7250813245773315, alpha: 0.9975273768433652\n",
      "epoch 0 iteration200, G Loss: 0.37030062079429626, D Loss: 1.5971399545669556, alpha: 0.9975273768433652\n",
      "epoch 0 iteration300, G Loss: 0.7331171035766602, D Loss: 1.4015755653381348, alpha: 0.9975273768433652\n",
      "epoch 0 iteration400, G Loss: 0.7983617782592773, D Loss: 1.4057066440582275, alpha: 0.9975273768433652\n",
      "epoch 0 iteration500, G Loss: 0.6991939544677734, D Loss: 1.3886913061141968, alpha: 0.9975273768433652\n",
      "epoch 0 iteration600, G Loss: 0.7056222558021545, D Loss: 1.3873003721237183, alpha: 0.9975273768433652\n",
      "epoch 0 iteration700, G Loss: 0.6350253224372864, D Loss: 1.431035041809082, alpha: 0.9975273768433652\n",
      "epoch 0 iteration800, G Loss: 0.5941745042800903, D Loss: 1.4165966510772705, alpha: 0.9975273768433652\n",
      "epoch 0 iteration900, G Loss: 0.832555890083313, D Loss: 1.7100694179534912, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1000, G Loss: 0.7106829285621643, D Loss: 1.3864989280700684, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1100, G Loss: 0.7625415921211243, D Loss: 1.386671543121338, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1200, G Loss: 0.9718523621559143, D Loss: 1.4120726585388184, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1300, G Loss: 0.7026684284210205, D Loss: 1.3861310482025146, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1400, G Loss: 0.7164912223815918, D Loss: 1.386370062828064, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1500, G Loss: 0.5836321711540222, D Loss: 1.3952922821044922, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1600, G Loss: 0.698292076587677, D Loss: 1.3864140510559082, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1700, G Loss: 0.7047454714775085, D Loss: 1.3857563734054565, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1800, G Loss: 0.8111492395401001, D Loss: 1.4081743955612183, alpha: 0.9975273768433652\n",
      "epoch 0 iteration1900, G Loss: 0.5659648776054382, D Loss: 1.406479835510254, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2000, G Loss: 0.8527551889419556, D Loss: 1.3910186290740967, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2100, G Loss: 0.7203443050384521, D Loss: 1.3864136934280396, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2200, G Loss: 0.8355186581611633, D Loss: 1.4092512130737305, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2300, G Loss: 0.7207205295562744, D Loss: 1.386191964149475, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2400, G Loss: 0.5480902791023254, D Loss: 1.3933777809143066, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2500, G Loss: 0.7018918991088867, D Loss: 1.3868408203125, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2600, G Loss: 0.7155443429946899, D Loss: 1.3900163173675537, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2700, G Loss: 0.7085014581680298, D Loss: 1.3867080211639404, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2800, G Loss: 0.6530057191848755, D Loss: 1.390064001083374, alpha: 0.9975273768433652\n",
      "epoch 0 iteration2900, G Loss: 0.7011067867279053, D Loss: 1.3880172967910767, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3000, G Loss: 0.7154375910758972, D Loss: 1.3867743015289307, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3100, G Loss: 0.6285852789878845, D Loss: 1.391203761100769, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3200, G Loss: 0.7069916129112244, D Loss: 1.3867218494415283, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3300, G Loss: 0.7131995558738708, D Loss: 1.3904023170471191, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3400, G Loss: 0.7306793332099915, D Loss: 1.3882153034210205, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3500, G Loss: 0.7161816954612732, D Loss: 1.386846899986267, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3600, G Loss: 0.7003817558288574, D Loss: 1.3874704837799072, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3700, G Loss: 0.9046729803085327, D Loss: 1.3889679908752441, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3800, G Loss: 0.7066559195518494, D Loss: 1.3871086835861206, alpha: 0.9975273768433652\n",
      "epoch 0 iteration3900, G Loss: 0.7904606461524963, D Loss: 1.3872076272964478, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4000, G Loss: 0.7036911249160767, D Loss: 1.3885047435760498, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4100, G Loss: 0.7090157270431519, D Loss: 1.3871092796325684, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4200, G Loss: 0.6949136257171631, D Loss: 1.7632415294647217, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4300, G Loss: 0.7066881656646729, D Loss: 1.386148452758789, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4400, G Loss: 0.7053501605987549, D Loss: 1.3863682746887207, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4500, G Loss: 0.6327142715454102, D Loss: 1.3916096687316895, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4600, G Loss: 0.6993628144264221, D Loss: 1.3864667415618896, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4700, G Loss: 0.7481885552406311, D Loss: 1.3865350484848022, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4800, G Loss: 0.7071452736854553, D Loss: 1.3877642154693604, alpha: 0.9975273768433652\n",
      "epoch 0 iteration4900, G Loss: 0.7128523588180542, D Loss: 1.386146903038025, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5000, G Loss: 0.7098007798194885, D Loss: 1.3868558406829834, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5100, G Loss: 0.7129054665565491, D Loss: 1.3871761560440063, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5200, G Loss: 1.0842938423156738, D Loss: 1.3925617933273315, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5300, G Loss: 0.7082905769348145, D Loss: 1.3870556354522705, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5400, G Loss: 0.7549765110015869, D Loss: 1.3872283697128296, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5500, G Loss: 0.7091989517211914, D Loss: 1.386307954788208, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5600, G Loss: 0.7037944197654724, D Loss: 1.3866970539093018, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5700, G Loss: 0.6989833116531372, D Loss: 1.3857849836349487, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5800, G Loss: 0.7189622521400452, D Loss: 1.3877449035644531, alpha: 0.9975273768433652\n",
      "epoch 0 iteration5900, G Loss: 0.6266724467277527, D Loss: 1.3891582489013672, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6000, G Loss: 0.7035909295082092, D Loss: 1.3864914178848267, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6100, G Loss: 0.7068443298339844, D Loss: 1.3865506649017334, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6200, G Loss: 0.7392309904098511, D Loss: 1.390605092048645, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6300, G Loss: 0.7185967564582825, D Loss: 1.3867937326431274, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6400, G Loss: 0.6780716180801392, D Loss: 1.3872876167297363, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6500, G Loss: 0.7077540755271912, D Loss: 1.3862851858139038, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6600, G Loss: 0.6870523691177368, D Loss: 1.389845848083496, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6700, G Loss: 0.6996948719024658, D Loss: 1.3873062133789062, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6800, G Loss: 0.7033646106719971, D Loss: 1.3867690563201904, alpha: 0.9975273768433652\n",
      "epoch 0 iteration6900, G Loss: 0.8670098781585693, D Loss: 1.525962471961975, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7000, G Loss: 0.7078633308410645, D Loss: 1.3863717317581177, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7100, G Loss: 0.6866700053215027, D Loss: 1.3864850997924805, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7200, G Loss: 0.7098564505577087, D Loss: 1.3864107131958008, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7300, G Loss: 0.6234257817268372, D Loss: 1.4649713039398193, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7400, G Loss: 0.7269936800003052, D Loss: 1.38596510887146, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7500, G Loss: 0.6109346151351929, D Loss: 1.4019118547439575, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7600, G Loss: 0.7054303884506226, D Loss: 1.3861724138259888, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7700, G Loss: 0.7209659218788147, D Loss: 1.3871207237243652, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7800, G Loss: 0.6901567578315735, D Loss: 1.3978850841522217, alpha: 0.9975273768433652\n",
      "epoch 0 iteration7900, G Loss: 0.38815683126449585, D Loss: 1.5270925760269165, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8000, G Loss: 0.6719773411750793, D Loss: 1.3862664699554443, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8100, G Loss: 0.7085751891136169, D Loss: 1.3858599662780762, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8200, G Loss: 0.7374979257583618, D Loss: 1.3868663311004639, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8300, G Loss: 0.7022703886032104, D Loss: 1.3865256309509277, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8400, G Loss: 0.6978246569633484, D Loss: 1.3860191106796265, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8500, G Loss: 0.6939514875411987, D Loss: 1.38914155960083, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8600, G Loss: 0.7115518450737, D Loss: 1.3865411281585693, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8700, G Loss: 0.7066450119018555, D Loss: 1.3855377435684204, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8800, G Loss: 0.7456421852111816, D Loss: 1.388199806213379, alpha: 0.9975273768433652\n",
      "epoch 0 iteration8900, G Loss: 0.6256153583526611, D Loss: 1.3907172679901123, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9000, G Loss: 0.46054330468177795, D Loss: 1.491150140762329, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9100, G Loss: 0.7051435112953186, D Loss: 1.3866322040557861, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9200, G Loss: 0.7054716944694519, D Loss: 1.3865368366241455, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9300, G Loss: 0.7062770128250122, D Loss: 1.386368751525879, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9400, G Loss: 0.7653543949127197, D Loss: 1.3881027698516846, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9500, G Loss: 0.7055291533470154, D Loss: 1.3855127096176147, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9600, G Loss: 0.7085509300231934, D Loss: 1.387042760848999, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9700, G Loss: 0.6696321964263916, D Loss: 1.3865649700164795, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9800, G Loss: 0.6999062299728394, D Loss: 1.3882017135620117, alpha: 0.9975273768433652\n",
      "epoch 0 iteration9900, G Loss: 0.7230591177940369, D Loss: 1.3864976167678833, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10000, G Loss: 0.7042454481124878, D Loss: 1.3876752853393555, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10100, G Loss: 0.7196884155273438, D Loss: 1.3865249156951904, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10200, G Loss: 0.7057018280029297, D Loss: 1.3862016201019287, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10300, G Loss: 0.6996011137962341, D Loss: 1.385890245437622, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10400, G Loss: 0.6991375684738159, D Loss: 1.3843824863433838, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10500, G Loss: 0.7123160362243652, D Loss: 1.3883781433105469, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10600, G Loss: 0.7272108197212219, D Loss: 1.3862667083740234, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10700, G Loss: 0.6972365379333496, D Loss: 1.386834979057312, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10800, G Loss: 0.7106330990791321, D Loss: 1.3873646259307861, alpha: 0.9975273768433652\n",
      "epoch 0 iteration10900, G Loss: 0.6917009353637695, D Loss: 1.3887977600097656, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11000, G Loss: 0.7823160290718079, D Loss: 1.3891184329986572, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11100, G Loss: 0.7016202807426453, D Loss: 1.3870885372161865, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11200, G Loss: 0.743533194065094, D Loss: 1.391204595565796, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11300, G Loss: 0.7825428247451782, D Loss: 1.3886785507202148, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11400, G Loss: 0.7035039067268372, D Loss: 1.3863508701324463, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11500, G Loss: 0.5993762016296387, D Loss: 1.3906075954437256, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11600, G Loss: 0.7463098764419556, D Loss: 1.3894777297973633, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11700, G Loss: 0.9221968650817871, D Loss: 1.3918437957763672, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11800, G Loss: 0.7055725455284119, D Loss: 1.386551022529602, alpha: 0.9975273768433652\n",
      "epoch 0 iteration11900, G Loss: 0.7083236575126648, D Loss: 1.3868231773376465, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12000, G Loss: 0.6936506628990173, D Loss: 1.3865121603012085, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12100, G Loss: 0.8124685883522034, D Loss: 1.39296293258667, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12200, G Loss: 0.7106762528419495, D Loss: 1.386641502380371, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12300, G Loss: 0.7115415930747986, D Loss: 1.3866323232650757, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12400, G Loss: 0.9294017553329468, D Loss: 1.3963590860366821, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12500, G Loss: 0.7276135683059692, D Loss: 1.3870329856872559, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12600, G Loss: 0.7101092338562012, D Loss: 1.3865079879760742, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12700, G Loss: 0.7017350196838379, D Loss: 1.3860241174697876, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12800, G Loss: 0.7327229976654053, D Loss: 1.3865176439285278, alpha: 0.9975273768433652\n",
      "epoch 0 iteration12900, G Loss: 0.7009575366973877, D Loss: 1.386488437652588, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13000, G Loss: 0.6375361680984497, D Loss: 1.3950555324554443, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13100, G Loss: 0.7075597047805786, D Loss: 1.386228322982788, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13200, G Loss: 0.6927332878112793, D Loss: 1.3875290155410767, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13300, G Loss: 0.5527434945106506, D Loss: 1.4479973316192627, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13400, G Loss: 0.6718850135803223, D Loss: 1.3965826034545898, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13500, G Loss: 0.7159638404846191, D Loss: 1.389503836631775, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13600, G Loss: 0.7080206871032715, D Loss: 1.3868365287780762, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13700, G Loss: 0.7038761377334595, D Loss: 1.3881127834320068, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13800, G Loss: 0.7075420618057251, D Loss: 1.386110782623291, alpha: 0.9975273768433652\n",
      "epoch 0 iteration13900, G Loss: 0.7599760293960571, D Loss: 1.3887906074523926, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14000, G Loss: 0.7487767338752747, D Loss: 1.3866533041000366, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14100, G Loss: 0.7105907797813416, D Loss: 1.385907769203186, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14200, G Loss: 0.7070570588111877, D Loss: 1.386513590812683, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14300, G Loss: 0.6723729372024536, D Loss: 1.3860728740692139, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14400, G Loss: 0.8644834756851196, D Loss: 1.3857905864715576, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14500, G Loss: 0.7049413919448853, D Loss: 1.3865265846252441, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14600, G Loss: 0.8838269114494324, D Loss: 1.3916791677474976, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14700, G Loss: 0.6799156665802002, D Loss: 1.3887255191802979, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14800, G Loss: 0.7003001570701599, D Loss: 1.3866922855377197, alpha: 0.9975273768433652\n",
      "epoch 0 iteration14900, G Loss: 0.6860100626945496, D Loss: 1.386712908744812, alpha: 0.9975273768433652\n",
      "Saving content.\n",
      "epoch 1 iteration0, G Loss: 0.7123156785964966, D Loss: 1.387179970741272, alpha: 0.997497601319515\n",
      "epoch 1 iteration100, G Loss: 0.7066801190376282, D Loss: 1.385801076889038, alpha: 0.997497601319515\n",
      "epoch 1 iteration200, G Loss: 0.781332790851593, D Loss: 1.4790267944335938, alpha: 0.997497601319515\n",
      "epoch 1 iteration300, G Loss: 0.7096717953681946, D Loss: 1.3872898817062378, alpha: 0.997497601319515\n",
      "epoch 1 iteration400, G Loss: 0.7098146677017212, D Loss: 1.386986494064331, alpha: 0.997497601319515\n",
      "epoch 1 iteration500, G Loss: 0.7091925144195557, D Loss: 1.3857059478759766, alpha: 0.997497601319515\n",
      "epoch 1 iteration600, G Loss: 0.7740735411643982, D Loss: 1.4050570726394653, alpha: 0.997497601319515\n",
      "epoch 1 iteration700, G Loss: 0.7079293131828308, D Loss: 1.3858182430267334, alpha: 0.997497601319515\n",
      "epoch 1 iteration800, G Loss: 0.7150444984436035, D Loss: 1.3861405849456787, alpha: 0.997497601319515\n",
      "epoch 1 iteration900, G Loss: 0.6218488216400146, D Loss: 1.389306902885437, alpha: 0.997497601319515\n",
      "epoch 1 iteration1000, G Loss: 0.7057942748069763, D Loss: 1.385972499847412, alpha: 0.997497601319515\n",
      "epoch 1 iteration1100, G Loss: 0.6940529942512512, D Loss: 1.3862740993499756, alpha: 0.997497601319515\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e13c6-9f5d-4b5d-8d40-fe862e1f0919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iddgan",
   "language": "python",
   "name": "iddgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
