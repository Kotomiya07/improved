{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1075cd01-3c38-4baa-922f-a90e07d71fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 [1, 2, 2] [32]\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 4, 16, 16) = 1024 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Inference time: 564.31+/-62.22ms\n"
     ]
    }
   ],
   "source": [
    "#@title measure time\n",
    "!python3 test_iddgan.py --dataset cifar10 --exp kl-f2-2 --epoch_id 1700 --num_channels 4 \\\n",
    "--num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --nz 50 --z_emb_dim 256 \\\n",
    "--n_mlp 4 --ch_mult 1 2 2 --image_size 32 --current_resolution 16 --attn_resolutions 32 \\\n",
    "--scale_factor 105.0 --AutoEncoder_config autoencoder/config/kl-f2.yaml --AutoEncoder_ckpt autoencoder/weight/kl-f2.ckpt \\\n",
    "--batch_size 100 --measure_time --compute_fid --real_img_dir pytorch_fid/cifar10_train_stat.npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4ef209-41a0-4c6c-b2fd-278f237cf504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 [1, 2, 2] [32]\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 4, 16, 16) = 1024 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "['./wddgan_generated_samples/cifar10/kl-f2-2/1700', 'pytorch_fid/cifar10_train_stat.npy']\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|█████████████████████████████████████████| 500/500 [01:23<00:00,  5.96it/s]\n",
      "FID = 4.3555737228003295\n",
      "dataset: cifar10, exp: kl-f2-2, epoch: 1700, FID: 4.3555737228003295\n"
     ]
    }
   ],
   "source": [
    "#@title cumpute FID\n",
    "!python3 test_iddgan.py --dataset cifar10 --exp kl-f2-2 --epoch_id 1700 --num_channels 4 \\\n",
    "--num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --nz 50 --z_emb_dim 256 \\\n",
    "--n_mlp 4 --ch_mult 1 2 2 --image_size 32 --current_resolution 16 --attn_resolutions 32 \\\n",
    "--scale_factor 105.0 --AutoEncoder_config autoencoder/config/kl-f2.yaml --AutoEncoder_ckpt autoencoder/weight/kl-f2.ckpt \\\n",
    "--batch_size 250 --compute_fid --real_img_dir pytorch_fid/cifar10_train_stat.npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e4365b-b180-43a1-b993-9f5182d4d46f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 [1, 2, 2, 2] [16]\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "['./wddgan_generated_samples/afhq_cat/vq-f4-256/400', 'data/afhq/train/cat']\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|█████████████████████████████████████████| 500/500 [01:45<00:00,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 52/52 [00:40<00:00,  1.30it/s]\n",
      "FID = 22.459739544399042\n"
     ]
    }
   ],
   "source": [
    "!python3 test_iddgan_celeba.py --dataset afhq_cat --exp vq-f4-256 --epoch_id 400 --num_channels 3 \\\n",
    "--num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --nz 100 --z_emb_dim 256 \\\n",
    "--n_mlp 4 --ch_mult 1 2 2 2 --image_size 256 --current_resolution 64 --attn_resolutions 16 \\\n",
    "--scale_factor 6.0 --AutoEncoder_config autoencoder/config/vq-f4.yaml --AutoEncoder_ckpt autoencoder/weight/vq-f4.ckpt \\\n",
    "--batch_size 50 --measure_time --compute_fid --real_img_dir data/afhq/train/cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae3603-aac0-4fee-a077-1bf72df80804",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: CIFAR10, exp: kl-f2, epoch:475, FID: 9.471584868250147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee2753-8c6b-4afc-b957-44ddcff0be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: cifar10, exp: kl-f2-2, epoch: 1600, FID: 4.540297830066777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb00930-d1ee-4f3c-81e4-32b90911e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: cifar10, exp: kl-f2-2, epoch: 1700, FID: 4.3555737228003295, Inference time: 564.31+/-62.22ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af657dec-2aca-4a8b-b08c-f8ca63f5fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFHQ-CAT vq-f4-256 epoch:400 FID = 22.459739544399042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18af61-33f3-40ec-a8ac-71c9be7b6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFHQ-CAT vq-f4-256 epoch:500 FID = 21.64908864472619"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
