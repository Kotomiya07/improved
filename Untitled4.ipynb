{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e150ff-a647-4d14-b4d5-ab6d46fe22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"64\",\n",
    "\"--exp\",\"kl-f4-64\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"4\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"2000\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"50\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.9999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.25e-4\",\n",
    "\"--lr_g\",\"1.6e-4\",\n",
    "\"--lazy_reg\",\"15\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6088\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"16\",\n",
    "\"--attn_resolutions\", \"32\",\n",
    "\"--num_disc_layers\", \"3\",\n",
    "\"--scale_factor\", \"105.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/kl-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/kl-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414e48d4-68ae-49af-a4ba-86470b9cf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10', 'afhq_cat']:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.sample().detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior.sample())\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17debf4-678d-48e4-94c3-015065223e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration0, G Loss: 2.3332111835479736, D Loss: 1.3875195980072021, alpha: 0.9975273768433652\n",
      "epoch 0 iteration100, G Loss: 0.6881375312805176, D Loss: 1.4088276624679565, alpha: 0.9975273768433652\n",
      "Saving content.\n",
      "epoch 1 iteration0, G Loss: 0.865545928478241, D Loss: 1.3909094333648682, alpha: 0.9975125335223958\n",
      "epoch 1 iteration100, G Loss: 0.7046482563018799, D Loss: 1.389861822128296, alpha: 0.9975125335223958\n",
      "Saving content.\n",
      "epoch 2 iteration0, G Loss: 0.7155339121818542, D Loss: 1.3928086757659912, alpha: 0.997497601319515\n",
      "epoch 2 iteration100, G Loss: 0.7057290077209473, D Loss: 1.3896028995513916, alpha: 0.997497601319515\n",
      "Saving content.\n",
      "epoch 3 iteration0, G Loss: 0.7115098834037781, D Loss: 1.3876268863677979, alpha: 0.9974825797051893\n",
      "epoch 3 iteration100, G Loss: 0.7265442609786987, D Loss: 1.386878252029419, alpha: 0.9974825797051893\n",
      "Saving content.\n",
      "epoch 4 iteration0, G Loss: 0.6553261280059814, D Loss: 1.3869068622589111, alpha: 0.9974674681467623\n",
      "epoch 4 iteration100, G Loss: 0.7453159689903259, D Loss: 1.3861836194992065, alpha: 0.9974674681467623\n",
      "Saving content.\n",
      "epoch 5 iteration0, G Loss: 0.7325004935264587, D Loss: 1.3940458297729492, alpha: 0.9974522661084374\n",
      "epoch 5 iteration100, G Loss: 0.6614384055137634, D Loss: 1.388619303703308, alpha: 0.9974522661084374\n",
      "Saving content.\n",
      "epoch 6 iteration0, G Loss: 0.6877944469451904, D Loss: 1.3851497173309326, alpha: 0.9974369730512593\n",
      "epoch 6 iteration100, G Loss: 0.580572783946991, D Loss: 1.3888944387435913, alpha: 0.9974369730512593\n",
      "Saving content.\n",
      "epoch 7 iteration0, G Loss: 0.7314046621322632, D Loss: 1.388602375984192, alpha: 0.9974215884330963\n",
      "epoch 7 iteration100, G Loss: 0.8190510272979736, D Loss: 1.3873000144958496, alpha: 0.9974215884330963\n",
      "Saving content.\n",
      "epoch 8 iteration0, G Loss: 0.7269418239593506, D Loss: 1.3867188692092896, alpha: 0.997406111708621\n",
      "epoch 8 iteration100, G Loss: 0.7296995520591736, D Loss: 1.3881218433380127, alpha: 0.997406111708621\n",
      "Saving content.\n",
      "epoch 9 iteration0, G Loss: 0.8036767840385437, D Loss: 1.3941326141357422, alpha: 0.997390542329293\n",
      "epoch 9 iteration100, G Loss: 0.7922875881195068, D Loss: 1.3896691799163818, alpha: 0.997390542329293\n",
      "Saving content.\n",
      "epoch 10 iteration0, G Loss: 0.5868846774101257, D Loss: 1.4174036979675293, alpha: 0.9973748797433398\n",
      "epoch 10 iteration100, G Loss: 0.7399120926856995, D Loss: 1.3806945085525513, alpha: 0.9973748797433398\n",
      "Saving content.\n",
      "epoch 11 iteration0, G Loss: 0.7161495685577393, D Loss: 1.3878856897354126, alpha: 0.9973591233957381\n",
      "epoch 11 iteration100, G Loss: 0.7305490374565125, D Loss: 1.3868836164474487, alpha: 0.9973591233957381\n",
      "Saving content.\n",
      "epoch 12 iteration0, G Loss: 0.7084153294563293, D Loss: 1.3873460292816162, alpha: 0.9973432727281952\n",
      "epoch 12 iteration100, G Loss: 0.6435222625732422, D Loss: 1.3839111328125, alpha: 0.9973432727281952\n",
      "Saving content.\n",
      "epoch 13 iteration0, G Loss: 0.7371506094932556, D Loss: 1.3865735530853271, alpha: 0.9973273271791304\n",
      "epoch 13 iteration100, G Loss: 0.8368872404098511, D Loss: 1.3914821147918701, alpha: 0.9973273271791304\n",
      "Saving content.\n",
      "epoch 14 iteration0, G Loss: 0.7286078333854675, D Loss: 1.3877809047698975, alpha: 0.9973112861836558\n",
      "epoch 14 iteration100, G Loss: 0.7580060958862305, D Loss: 1.3831411600112915, alpha: 0.9973112861836558\n",
      "Saving content.\n",
      "epoch 15 iteration0, G Loss: 0.7227853536605835, D Loss: 1.3901114463806152, alpha: 0.9972951491735572\n",
      "epoch 15 iteration100, G Loss: 0.6759948134422302, D Loss: 1.4641144275665283, alpha: 0.9972951491735572\n",
      "Saving content.\n",
      "epoch 16 iteration0, G Loss: 0.7134568095207214, D Loss: 1.3910740613937378, alpha: 0.9972789155772751\n",
      "epoch 16 iteration100, G Loss: 0.7470678687095642, D Loss: 1.3869266510009766, alpha: 0.9972789155772751\n",
      "Saving content.\n",
      "epoch 17 iteration0, G Loss: 0.6394176483154297, D Loss: 1.3880388736724854, alpha: 0.9972625848198857\n",
      "epoch 17 iteration100, G Loss: 0.7491527795791626, D Loss: 1.3937170505523682, alpha: 0.9972625848198857\n",
      "Saving content.\n",
      "epoch 18 iteration0, G Loss: 0.6973457932472229, D Loss: 1.3912242650985718, alpha: 0.997246156323081\n",
      "epoch 18 iteration100, G Loss: 0.7318655848503113, D Loss: 1.3798013925552368, alpha: 0.997246156323081\n",
      "Saving content.\n",
      "epoch 19 iteration0, G Loss: 0.7177475094795227, D Loss: 1.384331464767456, alpha: 0.9972296295051497\n",
      "epoch 19 iteration100, G Loss: 0.718926727771759, D Loss: 1.389547348022461, alpha: 0.9972296295051497\n",
      "Saving content.\n",
      "epoch 20 iteration0, G Loss: 0.770764946937561, D Loss: 1.3852407932281494, alpha: 0.9972130037809577\n",
      "epoch 20 iteration100, G Loss: 0.7266530394554138, D Loss: 1.3918007612228394, alpha: 0.9972130037809577\n",
      "Saving content.\n",
      "epoch 21 iteration0, G Loss: 0.7385839819908142, D Loss: 1.3880589008331299, alpha: 0.9971962785619283\n",
      "epoch 21 iteration100, G Loss: 0.7372348308563232, D Loss: 1.3849077224731445, alpha: 0.9971962785619283\n",
      "Saving content.\n",
      "epoch 22 iteration0, G Loss: 0.5857833623886108, D Loss: 1.4204161167144775, alpha: 0.9971794532560223\n",
      "epoch 22 iteration100, G Loss: 0.5006086230278015, D Loss: 1.4266095161437988, alpha: 0.9971794532560223\n",
      "Saving content.\n",
      "epoch 23 iteration0, G Loss: 0.7236216068267822, D Loss: 1.3859426975250244, alpha: 0.9971625272677183\n",
      "epoch 23 iteration100, G Loss: 0.706677258014679, D Loss: 1.3849458694458008, alpha: 0.9971625272677183\n",
      "Saving content.\n",
      "epoch 24 iteration0, G Loss: 0.7500833868980408, D Loss: 1.385428547859192, alpha: 0.9971454999979927\n",
      "epoch 24 iteration100, G Loss: 0.8008061647415161, D Loss: 1.3852455615997314, alpha: 0.9971454999979927\n",
      "Saving content.\n",
      "epoch 25 iteration0, G Loss: 0.7551994919776917, D Loss: 1.3915464878082275, alpha: 0.9971283708442996\n",
      "epoch 25 iteration100, G Loss: 0.7104729413986206, D Loss: 1.3864585161209106, alpha: 0.9971283708442996\n",
      "Saving content.\n",
      "epoch 26 iteration0, G Loss: 0.7404097318649292, D Loss: 1.389319658279419, alpha: 0.9971111392005504\n",
      "epoch 26 iteration100, G Loss: 0.6591176390647888, D Loss: 1.3886213302612305, alpha: 0.9971111392005504\n",
      "Saving content.\n",
      "epoch 27 iteration0, G Loss: 0.7254546880722046, D Loss: 1.3897324800491333, alpha: 0.9970938044570938\n",
      "epoch 27 iteration100, G Loss: 0.7276486158370972, D Loss: 1.3887970447540283, alpha: 0.9970938044570938\n",
      "Saving content.\n",
      "epoch 28 iteration0, G Loss: 0.7431560754776001, D Loss: 1.3894703388214111, alpha: 0.9970763660006952\n",
      "epoch 28 iteration100, G Loss: 0.7153012156486511, D Loss: 1.387927770614624, alpha: 0.9970763660006952\n",
      "Saving content.\n",
      "epoch 29 iteration0, G Loss: 0.7313553690910339, D Loss: 1.3866232633590698, alpha: 0.9970588232145158\n",
      "epoch 29 iteration100, G Loss: 0.7284747362136841, D Loss: 1.3890137672424316, alpha: 0.9970588232145158\n",
      "Saving content.\n",
      "epoch 30 iteration0, G Loss: 0.7303158044815063, D Loss: 1.384684443473816, alpha: 0.9970411754780928\n",
      "epoch 30 iteration100, G Loss: 0.7524781823158264, D Loss: 1.3904380798339844, alpha: 0.9970411754780928\n",
      "Saving content.\n",
      "epoch 31 iteration0, G Loss: 0.7429707646369934, D Loss: 1.3901795148849487, alpha: 0.9970234221673178\n",
      "epoch 31 iteration100, G Loss: 0.7570396661758423, D Loss: 1.3925795555114746, alpha: 0.9970234221673178\n",
      "Saving content.\n",
      "epoch 32 iteration0, G Loss: 0.7465570569038391, D Loss: 1.387190341949463, alpha: 0.9970055626544164\n",
      "epoch 32 iteration100, G Loss: 0.7311907410621643, D Loss: 1.3935930728912354, alpha: 0.9970055626544164\n",
      "Saving content.\n",
      "epoch 33 iteration0, G Loss: 0.7119895815849304, D Loss: 1.3894104957580566, alpha: 0.9969875963079272\n",
      "epoch 33 iteration100, G Loss: 0.6942006945610046, D Loss: 1.386640191078186, alpha: 0.9969875963079272\n",
      "Saving content.\n",
      "epoch 34 iteration0, G Loss: 0.7106589078903198, D Loss: 1.3806285858154297, alpha: 0.9969695224926802\n",
      "epoch 34 iteration100, G Loss: 0.774744987487793, D Loss: 1.3860359191894531, alpha: 0.9969695224926802\n",
      "Saving content.\n",
      "epoch 35 iteration0, G Loss: 0.732252299785614, D Loss: 1.385730504989624, alpha: 0.9969513405697763\n",
      "epoch 35 iteration100, G Loss: 0.7279707193374634, D Loss: 1.3793952465057373, alpha: 0.9969513405697763\n",
      "Saving content.\n",
      "epoch 36 iteration0, G Loss: 0.7775861620903015, D Loss: 1.390511155128479, alpha: 0.9969330498965654\n",
      "epoch 36 iteration100, G Loss: 0.711040735244751, D Loss: 1.388164758682251, alpha: 0.9969330498965654\n",
      "Saving content.\n",
      "epoch 37 iteration0, G Loss: 0.7153511643409729, D Loss: 1.384541630744934, alpha: 0.9969146498266254\n",
      "epoch 37 iteration100, G Loss: 0.7232469916343689, D Loss: 1.3829014301300049, alpha: 0.9969146498266254\n",
      "Saving content.\n",
      "epoch 38 iteration0, G Loss: 0.7280425429344177, D Loss: 1.392937421798706, alpha: 0.9968961397097401\n",
      "epoch 38 iteration100, G Loss: 0.7306951284408569, D Loss: 1.3860480785369873, alpha: 0.9968961397097401\n",
      "Saving content.\n",
      "epoch 39 iteration0, G Loss: 0.7220361828804016, D Loss: 1.3914105892181396, alpha: 0.9968775188918781\n",
      "epoch 39 iteration100, G Loss: 0.7332691550254822, D Loss: 1.389843225479126, alpha: 0.9968775188918781\n",
      "Saving content.\n",
      "epoch 40 iteration0, G Loss: 0.6969062685966492, D Loss: 1.3925700187683105, alpha: 0.9968587867151706\n",
      "epoch 40 iteration100, G Loss: 0.7405083179473877, D Loss: 1.392482876777649, alpha: 0.9968587867151706\n",
      "Saving content.\n",
      "epoch 41 iteration0, G Loss: 0.7210291028022766, D Loss: 1.3866633176803589, alpha: 0.9968399425178895\n",
      "epoch 41 iteration100, G Loss: 0.7327150106430054, D Loss: 1.3851096630096436, alpha: 0.9968399425178895\n",
      "Saving content.\n",
      "epoch 42 iteration0, G Loss: 0.7336533665657043, D Loss: 1.3927029371261597, alpha: 0.9968209856344259\n",
      "epoch 42 iteration100, G Loss: 0.7239548563957214, D Loss: 1.3846733570098877, alpha: 0.9968209856344259\n",
      "Saving content.\n",
      "epoch 43 iteration0, G Loss: 0.7876075506210327, D Loss: 1.382015347480774, alpha: 0.9968019153952671\n",
      "epoch 43 iteration100, G Loss: 0.7119824290275574, D Loss: 1.384792447090149, alpha: 0.9968019153952671\n",
      "Saving content.\n",
      "epoch 44 iteration0, G Loss: 0.8853819966316223, D Loss: 1.3787511587142944, alpha: 0.9967827311269749\n",
      "epoch 44 iteration100, G Loss: 0.7243761420249939, D Loss: 1.3822026252746582, alpha: 0.9967827311269749\n",
      "Saving content.\n",
      "epoch 45 iteration0, G Loss: 0.7028394341468811, D Loss: 1.3870071172714233, alpha: 0.9967634321521631\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e13c6-9f5d-4b5d-8d40-fe862e1f0919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
