{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c2e1ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"afhq_cat\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f8-256\",\n",
    "\"--num_channels\",\"4\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"0.02\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/afhq\",\n",
    "\"--master_port\",\"6087\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"32\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f8.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f8.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39abba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\n",
    "\"--dataset\",\"celeba_256\",\n",
    "\"--image_size\",\"256\",\n",
    "\"--exp\",\"vq-f4-256\",\n",
    "\"--num_channels\",\"3\",\n",
    "\"--num_channels_dae\",\"128\",\n",
    "\"--num_timesteps\",\"2\",\n",
    "\"--num_res_blocks\",\"2\",\n",
    "\"--batch_size\",\"32\",\n",
    "\"--num_epoch\",\"500\",\n",
    "\"--ngf\",\"64\",\n",
    "\"--nz\",\"100\",\n",
    "\"--z_emb_dim\",\"256\",\n",
    "\"--n_mlp\",\"4\",\n",
    "\"--embedding_type\",\"positional\",\n",
    "\"--use_ema\",\n",
    "\"--ema_decay\",\"0.999\",\n",
    "\"--r1_gamma\",\"2.\",\n",
    "\"--lr_d\",\"1.0e-4\",\n",
    "\"--lr_g\",\"2.0e-4\",\n",
    "\"--lazy_reg\",\"10\",\n",
    "\"--ch_mult\", \"1\", \"2\", \"2\", \"2\",\n",
    "\"--save_content\",\n",
    "\"--datadir\",\"data/celeba/celeba-lmdb/\",\n",
    "\"--master_port\",\"6090\",\n",
    "\"--num_process_per_node\",\"1\",\n",
    "\"--save_content_every\",\"1\",\n",
    "\"--current_resolution\", \"64\",\n",
    "\"--attn_resolutions\", \"16\",\n",
    "\"--num_disc_layers\", \"4\",\n",
    "\"--scale_factor\", \"6.0\",\n",
    "\"--no_lr_decay\", \n",
    "\"--AutoEncoder_config\", \"autoencoder/config/vq-f4.yaml\", \n",
    "\"--AutoEncoder_ckpt\", \"autoencoder/weight/vq-f4.ckpt\", \n",
    "\"--rec_loss\",\n",
    "\"--sigmoid_learning\",\n",
    "]\n",
    "\n",
    "args = get_args(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435563f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10'] or True:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior)\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9deaf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### ###  ####    ######  ######  #####    ###### #####      #     #### ### ### #### ### ###  ##### \n",
    " ##  #  ##  ##    ##  ##  ##  ##  ## ##   # ## #  ## ##     ##     ##   ##  #   ##   ##  #  ##  ## \n",
    " ### #  ##  ##    ##  ##  ##  ##  ## ##     ##    ## ##    ###     ##   ### #   ##   ### #  ##     \n",
    " # # #  ##  ##    ##  ##  ##  ##  ####      ##    ####     # ##    ##   # # #   ##   # # #  ## ### \n",
    " # ###  ##  ##    ##  ##  ##  ##  ##        ##    ## #    #####    ##   # ###   ##   # ###  ##  ## \n",
    " #  ##  ##  ##    ##  ##  ##  ##  ##        ##    ## ##   #  ###   ##   #  ##   ##   #  ##  ##  ## \n",
    "###  #   ####    ######  ######  ####      ####  ### ### ### #### #### ###  #  #### ###  #   ####  \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    #train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "    #                                                                num_replicas=args.world_size,\n",
    "    #                                                                rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              #sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    #broadcast_params(netG.parameters())\n",
    "    #broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    #netG = nn.parallel.DistributedDataParallel(\n",
    "    #    netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    #netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10'] or True:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        #train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior)\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c18b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/std/2021/21k0005/improved-ddgan/wandb/run-20241007_165032-0ml4hm61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kotomiya07/TEST/runs/0ml4hm61' target=\"_blank\">vq-f4-256</a></strong> to <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">https://wandb.ai/kotomiya07/TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kotomiya07/TEST/runs/0ml4hm61' target=\"_blank\">https://wandb.ai/kotomiya07/TEST/runs/0ml4hm61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kotomiya07/TEST/runs/0ml4hm61?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb311896e00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"TEST\",\n",
    "            name=args.exp,\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"channels_dae\": args.num_channels_dae,\n",
    "                \"ch_nult\": args.ch_mult,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"res_blocks\": args.num_res_blocks,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"embedding_type\": args.embedding_type,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"z_emb_dim\": args.z_emb_dim,\n",
    "                \"attn_resolutions\": args.attn_resolutions,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1d3bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/std/2021/21k0005/anaconda3/envs/iddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/users/std/2021/21k0005/anaconda3/envs/iddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "=> loaded checkpoint (epoch 463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 463 iteration0, G Loss: 1.4097973108291626, D Loss: 1.1842002868652344, alpha: 0.005987951266786751\n",
      "epoch 463 iteration100, G Loss: 0.7727916240692139, D Loss: 1.235471487045288, alpha: 0.005987951266786751\n",
      "epoch 463 iteration200, G Loss: 0.835367739200592, D Loss: 1.1779515743255615, alpha: 0.005987951266786751\n",
      "epoch 463 iteration300, G Loss: 0.9745060205459595, D Loss: 1.180989384651184, alpha: 0.005987951266786751\n",
      "epoch 463 iteration400, G Loss: 1.2838258743286133, D Loss: 1.2548378705978394, alpha: 0.005987951266786751\n",
      "epoch 463 iteration500, G Loss: 0.7595456838607788, D Loss: 1.2916030883789062, alpha: 0.005987951266786751\n",
      "epoch 463 iteration600, G Loss: 0.947861909866333, D Loss: 1.211135745048523, alpha: 0.005987951266786751\n",
      "epoch 463 iteration700, G Loss: 1.3064278364181519, D Loss: 1.261845588684082, alpha: 0.005987951266786751\n",
      "epoch 463 iteration800, G Loss: 1.1051867008209229, D Loss: 1.0761889219284058, alpha: 0.005987951266786751\n",
      "Saving content.\n",
      "epoch 464 iteration0, G Loss: 1.0593039989471436, D Loss: 1.119879126548767, alpha: 0.005846781495546738\n",
      "epoch 464 iteration100, G Loss: 0.6211174130439758, D Loss: 1.2574095726013184, alpha: 0.005846781495546738\n",
      "epoch 464 iteration200, G Loss: 0.8530636429786682, D Loss: 1.1889861822128296, alpha: 0.005846781495546738\n",
      "epoch 464 iteration300, G Loss: 0.6901138424873352, D Loss: 1.1436796188354492, alpha: 0.005846781495546738\n",
      "epoch 464 iteration400, G Loss: 0.9285516738891602, D Loss: 1.2268468141555786, alpha: 0.005846781495546738\n",
      "epoch 464 iteration500, G Loss: 1.1310278177261353, D Loss: 0.9791022539138794, alpha: 0.005846781495546738\n",
      "epoch 464 iteration600, G Loss: 1.0827375650405884, D Loss: 1.2737362384796143, alpha: 0.005846781495546738\n",
      "epoch 464 iteration700, G Loss: 0.9581696391105652, D Loss: 1.2491703033447266, alpha: 0.005846781495546738\n",
      "epoch 464 iteration800, G Loss: 0.8198946714401245, D Loss: 1.2100303173065186, alpha: 0.005846781495546738\n",
      "Saving content.\n",
      "epoch 465 iteration0, G Loss: 1.262075424194336, D Loss: 1.0738962888717651, alpha: 0.005708920777023452\n",
      "epoch 465 iteration100, G Loss: 1.274449110031128, D Loss: 1.1888951063156128, alpha: 0.005708920777023452\n",
      "epoch 465 iteration200, G Loss: 1.0575112104415894, D Loss: 1.1543878316879272, alpha: 0.005708920777023452\n",
      "epoch 465 iteration300, G Loss: 1.0172398090362549, D Loss: 1.1459357738494873, alpha: 0.005708920777023452\n",
      "epoch 465 iteration400, G Loss: 0.6606994867324829, D Loss: 1.2674652338027954, alpha: 0.005708920777023452\n",
      "epoch 465 iteration500, G Loss: 0.8098092079162598, D Loss: 1.1981501579284668, alpha: 0.005708920777023452\n",
      "epoch 465 iteration600, G Loss: 0.8217735886573792, D Loss: 1.206047534942627, alpha: 0.005708920777023452\n",
      "epoch 465 iteration700, G Loss: 1.085315227508545, D Loss: 1.0670206546783447, alpha: 0.005708920777023452\n",
      "epoch 465 iteration800, G Loss: 0.9269901514053345, D Loss: 1.131676197052002, alpha: 0.005708920777023452\n",
      "Saving content.\n",
      "epoch 466 iteration0, G Loss: 1.1215609312057495, D Loss: 1.0936706066131592, alpha: 0.005574292437204642\n",
      "epoch 466 iteration100, G Loss: 0.8247889280319214, D Loss: 1.186370849609375, alpha: 0.005574292437204642\n",
      "epoch 466 iteration200, G Loss: 0.8925132751464844, D Loss: 1.0376601219177246, alpha: 0.005574292437204642\n",
      "epoch 466 iteration300, G Loss: 0.9714645147323608, D Loss: 1.0862048864364624, alpha: 0.005574292437204642\n",
      "epoch 466 iteration400, G Loss: 0.994999885559082, D Loss: 1.1358706951141357, alpha: 0.005574292437204642\n",
      "epoch 466 iteration500, G Loss: 0.927932620048523, D Loss: 1.1558010578155518, alpha: 0.005574292437204642\n",
      "epoch 466 iteration600, G Loss: 1.154552936553955, D Loss: 1.2347431182861328, alpha: 0.005574292437204642\n",
      "epoch 466 iteration700, G Loss: 0.7399078011512756, D Loss: 1.3870271444320679, alpha: 0.005574292437204642\n",
      "epoch 466 iteration800, G Loss: 0.9877551198005676, D Loss: 1.27577543258667, alpha: 0.005574292437204642\n",
      "Saving content.\n",
      "epoch 467 iteration0, G Loss: 0.8810710310935974, D Loss: 1.1618001461029053, alpha: 0.005442821537149478\n",
      "epoch 467 iteration100, G Loss: 0.7551447749137878, D Loss: 1.2065056562423706, alpha: 0.005442821537149478\n",
      "epoch 467 iteration200, G Loss: 0.9510429501533508, D Loss: 1.1026701927185059, alpha: 0.005442821537149478\n",
      "epoch 467 iteration300, G Loss: 0.7769046425819397, D Loss: 1.011457085609436, alpha: 0.005442821537149478\n",
      "epoch 467 iteration400, G Loss: 0.7457557916641235, D Loss: 1.1320794820785522, alpha: 0.005442821537149478\n",
      "epoch 467 iteration500, G Loss: 0.5602684020996094, D Loss: 1.1025679111480713, alpha: 0.005442821537149478\n",
      "epoch 467 iteration600, G Loss: 1.0297486782073975, D Loss: 1.1837189197540283, alpha: 0.005442821537149478\n",
      "epoch 467 iteration700, G Loss: 0.9366917014122009, D Loss: 1.2151767015457153, alpha: 0.005442821537149478\n",
      "epoch 467 iteration800, G Loss: 0.971356987953186, D Loss: 1.1490039825439453, alpha: 0.005442821537149478\n",
      "Saving content.\n",
      "epoch 468 iteration0, G Loss: 1.0706346035003662, D Loss: 1.2034052610397339, alpha: 0.005314434835667403\n",
      "epoch 468 iteration100, G Loss: 1.1414607763290405, D Loss: 1.2079417705535889, alpha: 0.005314434835667403\n",
      "epoch 468 iteration200, G Loss: 1.053244709968567, D Loss: 1.1730554103851318, alpha: 0.005314434835667403\n",
      "epoch 468 iteration300, G Loss: 0.7791473269462585, D Loss: 1.1005877256393433, alpha: 0.005314434835667403\n",
      "epoch 468 iteration400, G Loss: 1.3915773630142212, D Loss: 1.1977217197418213, alpha: 0.005314434835667403\n",
      "epoch 468 iteration500, G Loss: 0.7000586986541748, D Loss: 1.1177119016647339, alpha: 0.005314434835667403\n",
      "epoch 468 iteration600, G Loss: 1.2244439125061035, D Loss: 1.2364825010299683, alpha: 0.005314434835667403\n",
      "epoch 468 iteration700, G Loss: 0.9146134853363037, D Loss: 1.0352911949157715, alpha: 0.005314434835667403\n",
      "epoch 468 iteration800, G Loss: 1.2321438789367676, D Loss: 1.23969566822052, alpha: 0.005314434835667403\n",
      "Saving content.\n",
      "epoch 469 iteration0, G Loss: 1.106127142906189, D Loss: 1.1372747421264648, alpha: 0.005189060752710528\n",
      "epoch 469 iteration100, G Loss: 1.1478896141052246, D Loss: 1.135474443435669, alpha: 0.005189060752710528\n",
      "epoch 469 iteration200, G Loss: 0.8670642375946045, D Loss: 1.080875039100647, alpha: 0.005189060752710528\n",
      "epoch 469 iteration300, G Loss: 1.1827126741409302, D Loss: 1.2127575874328613, alpha: 0.005189060752710528\n",
      "epoch 469 iteration400, G Loss: 0.8796502351760864, D Loss: 1.1635082960128784, alpha: 0.005189060752710528\n",
      "epoch 469 iteration500, G Loss: 0.9760621786117554, D Loss: 1.2024729251861572, alpha: 0.005189060752710528\n",
      "epoch 469 iteration600, G Loss: 0.897115170955658, D Loss: 1.0683913230895996, alpha: 0.005189060752710528\n",
      "epoch 469 iteration700, G Loss: 1.4110767841339111, D Loss: 1.071568489074707, alpha: 0.005189060752710528\n",
      "epoch 469 iteration800, G Loss: 1.137041449546814, D Loss: 1.034257411956787, alpha: 0.005189060752710528\n",
      "Saving content.\n",
      "epoch 470 iteration0, G Loss: 0.8023020625114441, D Loss: 1.1569774150848389, alpha: 0.005066629333466244\n",
      "epoch 470 iteration100, G Loss: 0.7977839708328247, D Loss: 1.201804518699646, alpha: 0.005066629333466244\n",
      "epoch 470 iteration200, G Loss: 1.0334726572036743, D Loss: 1.1748037338256836, alpha: 0.005066629333466244\n",
      "epoch 470 iteration300, G Loss: 0.7968175411224365, D Loss: 1.1788126230239868, alpha: 0.005066629333466244\n",
      "epoch 470 iteration400, G Loss: 0.7078563570976257, D Loss: 1.1420913934707642, alpha: 0.005066629333466244\n",
      "epoch 470 iteration500, G Loss: 1.44563889503479, D Loss: 1.4794234037399292, alpha: 0.005066629333466244\n",
      "epoch 470 iteration600, G Loss: 1.0679179430007935, D Loss: 1.2269723415374756, alpha: 0.005066629333466244\n",
      "epoch 470 iteration700, G Loss: 1.003800392150879, D Loss: 1.248672366142273, alpha: 0.005066629333466244\n",
      "epoch 470 iteration800, G Loss: 0.8190739750862122, D Loss: 1.22393798828125, alpha: 0.005066629333466244\n",
      "Saving content.\n",
      "epoch 471 iteration0, G Loss: 0.8095570206642151, D Loss: 1.0657744407653809, alpha: 0.004947072213142167\n",
      "epoch 471 iteration100, G Loss: 0.9056047201156616, D Loss: 1.1813608407974243, alpha: 0.004947072213142167\n",
      "epoch 471 iteration200, G Loss: 0.8667837381362915, D Loss: 1.1054062843322754, alpha: 0.004947072213142167\n",
      "epoch 471 iteration300, G Loss: 1.0482921600341797, D Loss: 1.185107707977295, alpha: 0.004947072213142167\n",
      "epoch 471 iteration400, G Loss: 1.4351874589920044, D Loss: 1.1794967651367188, alpha: 0.004947072213142167\n",
      "epoch 471 iteration500, G Loss: 1.175158143043518, D Loss: 1.3902039527893066, alpha: 0.004947072213142167\n",
      "epoch 471 iteration600, G Loss: 1.1880319118499756, D Loss: 1.0989353656768799, alpha: 0.004947072213142167\n",
      "epoch 471 iteration700, G Loss: 0.8582873344421387, D Loss: 1.2240626811981201, alpha: 0.004947072213142167\n",
      "epoch 471 iteration800, G Loss: 0.9886583089828491, D Loss: 1.2035913467407227, alpha: 0.004947072213142167\n",
      "Saving content.\n",
      "epoch 472 iteration0, G Loss: 0.8180764317512512, D Loss: 1.1610726118087769, alpha: 0.004830322582434765\n",
      "epoch 472 iteration100, G Loss: 0.975890576839447, D Loss: 1.0941262245178223, alpha: 0.004830322582434765\n",
      "epoch 472 iteration200, G Loss: 1.029401183128357, D Loss: 1.2298972606658936, alpha: 0.004830322582434765\n",
      "epoch 472 iteration300, G Loss: 1.1732741594314575, D Loss: 1.1154335737228394, alpha: 0.004830322582434765\n",
      "epoch 472 iteration400, G Loss: 0.9758339524269104, D Loss: 1.113738775253296, alpha: 0.004830322582434765\n",
      "epoch 472 iteration500, G Loss: 0.830104410648346, D Loss: 1.19264817237854, alpha: 0.004830322582434765\n",
      "epoch 472 iteration600, G Loss: 0.7559197545051575, D Loss: 1.1346787214279175, alpha: 0.004830322582434765\n",
      "epoch 472 iteration700, G Loss: 1.244455337524414, D Loss: 1.1785626411437988, alpha: 0.004830322582434765\n",
      "epoch 472 iteration800, G Loss: 0.8320468068122864, D Loss: 1.229809045791626, alpha: 0.004830322582434765\n",
      "Saving content.\n",
      "epoch 473 iteration0, G Loss: 0.8980607390403748, D Loss: 1.209782600402832, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration100, G Loss: 1.0242174863815308, D Loss: 1.182856559753418, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration200, G Loss: 0.8778379559516907, D Loss: 1.1625800132751465, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration300, G Loss: 0.9375901222229004, D Loss: 1.1931910514831543, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration400, G Loss: 0.8396552205085754, D Loss: 1.1488486528396606, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration500, G Loss: 0.514482855796814, D Loss: 1.0487594604492188, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration600, G Loss: 0.7615821957588196, D Loss: 1.1003236770629883, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration700, G Loss: 0.9504516124725342, D Loss: 1.123626708984375, alpha: 0.0047163151536718795\n",
      "epoch 473 iteration800, G Loss: 0.880120038986206, D Loss: 1.0972568988800049, alpha: 0.0047163151536718795\n",
      "Saving content.\n",
      "epoch 474 iteration0, G Loss: 0.5730764865875244, D Loss: 1.35202956199646, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration100, G Loss: 0.9655992388725281, D Loss: 1.136427640914917, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration200, G Loss: 0.7357550263404846, D Loss: 1.0534095764160156, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration300, G Loss: 0.8846069574356079, D Loss: 1.2305307388305664, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration400, G Loss: 0.9769122004508972, D Loss: 1.292971134185791, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration500, G Loss: 1.0891696214675903, D Loss: 1.234149694442749, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration600, G Loss: 0.9806576371192932, D Loss: 1.1614243984222412, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration700, G Loss: 1.07476007938385, D Loss: 1.1393392086029053, alpha: 0.0046049861276189485\n",
      "epoch 474 iteration800, G Loss: 1.0129059553146362, D Loss: 1.0744009017944336, alpha: 0.0046049861276189485\n",
      "Saving content.\n",
      "epoch 475 iteration0, G Loss: 1.1540261507034302, D Loss: 1.1946868896484375, alpha: 0.004496273160941144\n",
      "epoch 475 iteration100, G Loss: 0.847131073474884, D Loss: 1.1356275081634521, alpha: 0.004496273160941144\n",
      "epoch 475 iteration200, G Loss: 1.1855937242507935, D Loss: 1.1122664213180542, alpha: 0.004496273160941144\n",
      "epoch 475 iteration300, G Loss: 0.619565486907959, D Loss: 1.3268394470214844, alpha: 0.004496273160941144\n",
      "epoch 475 iteration400, G Loss: 1.0452078580856323, D Loss: 1.2525415420532227, alpha: 0.004496273160941144\n",
      "epoch 475 iteration500, G Loss: 0.6280162334442139, D Loss: 1.1198128461837769, alpha: 0.004496273160941144\n",
      "epoch 475 iteration600, G Loss: 0.7986259460449219, D Loss: 1.0862491130828857, alpha: 0.004496273160941144\n",
      "epoch 475 iteration700, G Loss: 1.037392020225525, D Loss: 1.1503783464431763, alpha: 0.004496273160941144\n",
      "epoch 475 iteration800, G Loss: 0.7839449644088745, D Loss: 1.1859214305877686, alpha: 0.004496273160941144\n",
      "Saving content.\n",
      "epoch 476 iteration0, G Loss: 1.0457333326339722, D Loss: 1.1294565200805664, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration100, G Loss: 0.864670991897583, D Loss: 1.2116895914077759, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration200, G Loss: 0.9901496767997742, D Loss: 1.0950264930725098, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration300, G Loss: 1.0373154878616333, D Loss: 1.1712429523468018, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration400, G Loss: 0.803641140460968, D Loss: 1.3465020656585693, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration500, G Loss: 0.7616032958030701, D Loss: 1.0520306825637817, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration600, G Loss: 1.016124963760376, D Loss: 1.029927372932434, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration700, G Loss: 0.9779662489891052, D Loss: 1.1948504447937012, alpha: 0.0043901153343109955\n",
      "epoch 476 iteration800, G Loss: 0.8148521184921265, D Loss: 1.1564035415649414, alpha: 0.0043901153343109955\n",
      "Saving content.\n",
      "epoch 477 iteration0, G Loss: 0.8250758647918701, D Loss: 1.207654356956482, alpha: 0.004286453121149725\n",
      "epoch 477 iteration100, G Loss: 0.9191675782203674, D Loss: 1.0273375511169434, alpha: 0.004286453121149725\n",
      "epoch 477 iteration200, G Loss: 0.6875941753387451, D Loss: 1.1305255889892578, alpha: 0.004286453121149725\n",
      "epoch 477 iteration300, G Loss: 1.161802053451538, D Loss: 1.2549697160720825, alpha: 0.004286453121149725\n",
      "epoch 477 iteration400, G Loss: 0.9222116470336914, D Loss: 1.2956405878067017, alpha: 0.004286453121149725\n",
      "epoch 477 iteration500, G Loss: 0.7280892729759216, D Loss: 1.097564458847046, alpha: 0.004286453121149725\n",
      "epoch 477 iteration600, G Loss: 0.7520861625671387, D Loss: 1.1520545482635498, alpha: 0.004286453121149725\n",
      "epoch 477 iteration700, G Loss: 0.8514065146446228, D Loss: 1.175036907196045, alpha: 0.004286453121149725\n",
      "epoch 477 iteration800, G Loss: 0.8157898783683777, D Loss: 1.0855064392089844, alpha: 0.004286453121149725\n",
      "Saving content.\n",
      "epoch 478 iteration0, G Loss: 0.8974182605743408, D Loss: 1.1007883548736572, alpha: 0.004185228356997639\n",
      "epoch 478 iteration100, G Loss: 0.7598524689674377, D Loss: 1.1186468601226807, alpha: 0.004185228356997639\n",
      "epoch 478 iteration200, G Loss: 0.813261866569519, D Loss: 1.1790964603424072, alpha: 0.004185228356997639\n",
      "epoch 478 iteration300, G Loss: 1.0313551425933838, D Loss: 1.124262809753418, alpha: 0.004185228356997639\n",
      "epoch 478 iteration400, G Loss: 0.8751416206359863, D Loss: 1.0956807136535645, alpha: 0.004185228356997639\n",
      "epoch 478 iteration500, G Loss: 0.7727435827255249, D Loss: 0.9958759546279907, alpha: 0.004185228356997639\n",
      "epoch 478 iteration600, G Loss: 0.9812001585960388, D Loss: 1.1121562719345093, alpha: 0.004185228356997639\n",
      "epoch 478 iteration700, G Loss: 0.9886177182197571, D Loss: 1.1620755195617676, alpha: 0.004185228356997639\n",
      "epoch 478 iteration800, G Loss: 1.0505410432815552, D Loss: 1.1974258422851562, alpha: 0.004185228356997639\n",
      "Saving content.\n",
      "epoch 479 iteration0, G Loss: 0.8005074858665466, D Loss: 1.116053819656372, alpha: 0.004086384209499028\n",
      "epoch 479 iteration100, G Loss: 0.7691619396209717, D Loss: 1.0472475290298462, alpha: 0.004086384209499028\n",
      "epoch 479 iteration200, G Loss: 0.6285093426704407, D Loss: 1.2062616348266602, alpha: 0.004086384209499028\n",
      "epoch 479 iteration300, G Loss: 1.0343972444534302, D Loss: 1.096896767616272, alpha: 0.004086384209499028\n",
      "epoch 479 iteration400, G Loss: 0.8178744912147522, D Loss: 1.2682934999465942, alpha: 0.004086384209499028\n",
      "epoch 479 iteration500, G Loss: 0.893018901348114, D Loss: 1.2105913162231445, alpha: 0.004086384209499028\n",
      "epoch 479 iteration600, G Loss: 0.7172775864601135, D Loss: 1.1061880588531494, alpha: 0.004086384209499028\n",
      "epoch 479 iteration700, G Loss: 0.8065363168716431, D Loss: 1.2230652570724487, alpha: 0.004086384209499028\n",
      "epoch 479 iteration800, G Loss: 0.8740460872650146, D Loss: 1.1864205598831177, alpha: 0.004086384209499028\n",
      "Saving content.\n",
      "epoch 480 iteration0, G Loss: 0.5956246256828308, D Loss: 1.2715867757797241, alpha: 0.003989865148994021\n",
      "epoch 480 iteration100, G Loss: 1.0035964250564575, D Loss: 1.1767112016677856, alpha: 0.003989865148994021\n",
      "epoch 480 iteration200, G Loss: 0.9092313051223755, D Loss: 1.2020937204360962, alpha: 0.003989865148994021\n",
      "epoch 480 iteration300, G Loss: 0.7977414727210999, D Loss: 1.0483880043029785, alpha: 0.003989865148994021\n",
      "epoch 480 iteration400, G Loss: 0.8533030152320862, D Loss: 1.1578253507614136, alpha: 0.003989865148994021\n",
      "epoch 480 iteration500, G Loss: 1.1373122930526733, D Loss: 1.0801647901535034, alpha: 0.003989865148994021\n",
      "epoch 480 iteration600, G Loss: 1.077731966972351, D Loss: 1.0535738468170166, alpha: 0.003989865148994021\n",
      "epoch 480 iteration700, G Loss: 0.9465367197990417, D Loss: 1.2369388341903687, alpha: 0.003989865148994021\n",
      "epoch 480 iteration800, G Loss: 1.0471912622451782, D Loss: 1.0883357524871826, alpha: 0.003989865148994021\n",
      "Saving content.\n",
      "epoch 481 iteration0, G Loss: 0.6790949106216431, D Loss: 1.203426718711853, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration100, G Loss: 0.7620573043823242, D Loss: 1.0687811374664307, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration200, G Loss: 0.9047155976295471, D Loss: 1.1387784481048584, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration300, G Loss: 0.9064885377883911, D Loss: 1.180942416191101, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration400, G Loss: 0.9201392531394958, D Loss: 1.2137188911437988, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration500, G Loss: 0.9040712714195251, D Loss: 1.1287288665771484, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration600, G Loss: 0.6967340111732483, D Loss: 1.1906625032424927, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration700, G Loss: 0.7241470217704773, D Loss: 1.0464967489242554, alpha: 0.0038956169197096324\n",
      "epoch 481 iteration800, G Loss: 0.9643047451972961, D Loss: 1.0502961874008179, alpha: 0.0038956169197096324\n",
      "Saving content.\n",
      "epoch 482 iteration0, G Loss: 0.9278563857078552, D Loss: 1.1950510740280151, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration100, G Loss: 0.9118281006813049, D Loss: 1.1400799751281738, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration200, G Loss: 0.9680920243263245, D Loss: 1.1537432670593262, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration300, G Loss: 1.1500818729400635, D Loss: 1.0631136894226074, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration400, G Loss: 0.7946714758872986, D Loss: 1.2155286073684692, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration500, G Loss: 0.8970887064933777, D Loss: 1.128403663635254, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration600, G Loss: 0.7497817873954773, D Loss: 1.181634783744812, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration700, G Loss: 1.9919307231903076, D Loss: 1.265751600265503, alpha: 0.0038035865115351175\n",
      "epoch 482 iteration800, G Loss: 0.9745953679084778, D Loss: 1.109330415725708, alpha: 0.0038035865115351175\n",
      "Saving content.\n",
      "epoch 483 iteration0, G Loss: 0.9174579977989197, D Loss: 1.131138801574707, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration100, G Loss: 0.623072624206543, D Loss: 1.1991513967514038, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration200, G Loss: 0.8111684918403625, D Loss: 1.1348308324813843, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration300, G Loss: 1.1069047451019287, D Loss: 1.1166342496871948, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration400, G Loss: 0.9715629816055298, D Loss: 1.1290091276168823, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration500, G Loss: 0.7172456383705139, D Loss: 1.2406805753707886, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration600, G Loss: 0.9266290664672852, D Loss: 1.0880556106567383, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration700, G Loss: 0.875368058681488, D Loss: 1.2142040729522705, alpha: 0.0037137221323788605\n",
      "epoch 483 iteration800, G Loss: 0.9220640063285828, D Loss: 1.2020370960235596, alpha: 0.0037137221323788605\n",
      "Saving content.\n",
      "epoch 484 iteration0, G Loss: 0.9561532735824585, D Loss: 1.1693929433822632, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration100, G Loss: 0.7182460427284241, D Loss: 1.049196720123291, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration200, G Loss: 0.7817893028259277, D Loss: 1.194028377532959, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration300, G Loss: 0.7124252319335938, D Loss: 1.1701240539550781, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration400, G Loss: 0.9666503071784973, D Loss: 1.0471049547195435, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration500, G Loss: 0.9594957828521729, D Loss: 1.1770564317703247, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration600, G Loss: 0.7716347575187683, D Loss: 1.1084493398666382, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration700, G Loss: 1.123284935951233, D Loss: 1.1057343482971191, alpha: 0.0036259731810909246\n",
      "epoch 484 iteration800, G Loss: 1.092488408088684, D Loss: 1.3085954189300537, alpha: 0.0036259731810909246\n",
      "Saving content.\n",
      "epoch 485 iteration0, G Loss: 1.1656575202941895, D Loss: 1.2638322114944458, alpha: 0.003540290220946485\n",
      "epoch 485 iteration100, G Loss: 1.0435047149658203, D Loss: 1.122793436050415, alpha: 0.003540290220946485\n",
      "epoch 485 iteration200, G Loss: 1.077286720275879, D Loss: 1.0492318868637085, alpha: 0.003540290220946485\n",
      "epoch 485 iteration300, G Loss: 0.8087571263313293, D Loss: 1.2479543685913086, alpha: 0.003540290220946485\n",
      "epoch 485 iteration400, G Loss: 1.3077222108840942, D Loss: 1.1439690589904785, alpha: 0.003540290220946485\n",
      "epoch 485 iteration500, G Loss: 1.2434052228927612, D Loss: 0.9984872341156006, alpha: 0.003540290220946485\n",
      "epoch 485 iteration600, G Loss: 0.9004998207092285, D Loss: 1.1538007259368896, alpha: 0.003540290220946485\n",
      "epoch 485 iteration700, G Loss: 0.9050922989845276, D Loss: 1.0885430574417114, alpha: 0.003540290220946485\n",
      "epoch 485 iteration800, G Loss: 0.8289725184440613, D Loss: 1.109520435333252, alpha: 0.003540290220946485\n",
      "Saving content.\n",
      "epoch 486 iteration0, G Loss: 0.9382705688476562, D Loss: 1.2649297714233398, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration100, G Loss: 0.9155886173248291, D Loss: 1.2212103605270386, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration200, G Loss: 0.7971097230911255, D Loss: 1.1515971422195435, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration300, G Loss: 1.3642703294754028, D Loss: 1.3504440784454346, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration400, G Loss: 0.7216893434524536, D Loss: 1.2218313217163086, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration500, G Loss: 0.8482444286346436, D Loss: 1.1643092632293701, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration600, G Loss: 0.9361329674720764, D Loss: 1.2980382442474365, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration700, G Loss: 0.7463275790214539, D Loss: 1.0916786193847656, alpha: 0.0034566249536777116\n",
      "epoch 486 iteration800, G Loss: 0.8220968246459961, D Loss: 1.1642895936965942, alpha: 0.0034566249536777116\n",
      "Saving content.\n",
      "epoch 487 iteration0, G Loss: 0.7796162366867065, D Loss: 1.2758328914642334, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration100, G Loss: 0.6109365224838257, D Loss: 1.2344489097595215, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration200, G Loss: 0.7773752808570862, D Loss: 1.2336905002593994, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration300, G Loss: 0.7216587066650391, D Loss: 1.2132222652435303, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration400, G Loss: 0.8474329710006714, D Loss: 1.0731933116912842, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration500, G Loss: 0.9134775996208191, D Loss: 1.4077619314193726, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration600, G Loss: 0.8270764350891113, D Loss: 1.0715131759643555, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration700, G Loss: 0.6746521592140198, D Loss: 1.2690954208374023, alpha: 0.0033749301940461107\n",
      "epoch 487 iteration800, G Loss: 0.9850735664367676, D Loss: 1.1833363771438599, alpha: 0.0033749301940461107\n",
      "Saving content.\n",
      "epoch 488 iteration0, G Loss: 1.0141910314559937, D Loss: 1.2011197805404663, alpha: 0.003295159844945217\n",
      "epoch 488 iteration100, G Loss: 0.8644152879714966, D Loss: 1.0715280771255493, alpha: 0.003295159844945217\n",
      "epoch 488 iteration200, G Loss: 0.7234916687011719, D Loss: 1.2531238794326782, alpha: 0.003295159844945217\n",
      "epoch 488 iteration300, G Loss: 0.8436464071273804, D Loss: 1.062501072883606, alpha: 0.003295159844945217\n",
      "epoch 488 iteration400, G Loss: 1.194018006324768, D Loss: 1.2846522331237793, alpha: 0.003295159844945217\n",
      "epoch 488 iteration500, G Loss: 1.3775044679641724, D Loss: 1.320220708847046, alpha: 0.003295159844945217\n",
      "epoch 488 iteration600, G Loss: 0.8371932506561279, D Loss: 1.1201969385147095, alpha: 0.003295159844945217\n",
      "epoch 488 iteration700, G Loss: 0.7649359107017517, D Loss: 1.0639381408691406, alpha: 0.003295159844945217\n",
      "epoch 488 iteration800, G Loss: 0.9105792045593262, D Loss: 1.1603772640228271, alpha: 0.003295159844945217\n",
      "Saving content.\n",
      "epoch 489 iteration0, G Loss: 1.354849100112915, D Loss: 1.2473933696746826, alpha: 0.003217268873025092\n",
      "epoch 489 iteration100, G Loss: 0.6549320816993713, D Loss: 1.2817916870117188, alpha: 0.003217268873025092\n",
      "epoch 489 iteration200, G Loss: 0.7519973516464233, D Loss: 1.3227851390838623, alpha: 0.003217268873025092\n",
      "epoch 489 iteration300, G Loss: 1.2585344314575195, D Loss: 1.154113531112671, alpha: 0.003217268873025092\n",
      "epoch 489 iteration400, G Loss: 1.0132941007614136, D Loss: 1.1300928592681885, alpha: 0.003217268873025092\n",
      "epoch 489 iteration500, G Loss: 0.7162570357322693, D Loss: 1.1476984024047852, alpha: 0.003217268873025092\n",
      "epoch 489 iteration600, G Loss: 0.9427843689918518, D Loss: 1.1896092891693115, alpha: 0.003217268873025092\n",
      "epoch 489 iteration700, G Loss: 0.853397786617279, D Loss: 1.0931048393249512, alpha: 0.003217268873025092\n",
      "epoch 489 iteration800, G Loss: 0.6604682207107544, D Loss: 1.1908457279205322, alpha: 0.003217268873025092\n",
      "Saving content.\n",
      "epoch 490 iteration0, G Loss: 0.7544001340866089, D Loss: 1.1975784301757812, alpha: 0.003141213284829414\n",
      "epoch 490 iteration100, G Loss: 1.0206897258758545, D Loss: 1.2786318063735962, alpha: 0.003141213284829414\n",
      "epoch 490 iteration200, G Loss: 0.7120884656906128, D Loss: 1.2374484539031982, alpha: 0.003141213284829414\n",
      "epoch 490 iteration300, G Loss: 1.2454771995544434, D Loss: 1.2205930948257446, alpha: 0.003141213284829414\n",
      "epoch 490 iteration400, G Loss: 1.4410130977630615, D Loss: 1.012953519821167, alpha: 0.003141213284829414\n",
      "epoch 490 iteration500, G Loss: 1.5644288063049316, D Loss: 1.1461360454559326, alpha: 0.003141213284829414\n",
      "epoch 490 iteration600, G Loss: 1.145608901977539, D Loss: 1.271227478981018, alpha: 0.003141213284829414\n",
      "epoch 490 iteration700, G Loss: 1.0501024723052979, D Loss: 1.3038971424102783, alpha: 0.003141213284829414\n",
      "epoch 490 iteration800, G Loss: 0.9698755145072937, D Loss: 1.135688304901123, alpha: 0.003141213284829414\n",
      "Saving content.\n",
      "epoch 491 iteration0, G Loss: 1.133165717124939, D Loss: 1.282863974571228, alpha: 0.003066950103434496\n",
      "epoch 491 iteration100, G Loss: 0.9534481167793274, D Loss: 1.150605320930481, alpha: 0.003066950103434496\n",
      "epoch 491 iteration200, G Loss: 0.6960716843605042, D Loss: 1.1036393642425537, alpha: 0.003066950103434496\n",
      "epoch 491 iteration300, G Loss: 1.0211130380630493, D Loss: 1.1250863075256348, alpha: 0.003066950103434496\n",
      "epoch 491 iteration400, G Loss: 0.8680756688117981, D Loss: 1.1547709703445435, alpha: 0.003066950103434496\n",
      "epoch 491 iteration500, G Loss: 1.0165165662765503, D Loss: 1.1683919429779053, alpha: 0.003066950103434496\n",
      "epoch 491 iteration600, G Loss: 0.9069703221321106, D Loss: 1.1429471969604492, alpha: 0.003066950103434496\n",
      "epoch 491 iteration700, G Loss: 0.8354765176773071, D Loss: 1.1320693492889404, alpha: 0.003066950103434496\n",
      "epoch 491 iteration800, G Loss: 0.7832886576652527, D Loss: 1.1148407459259033, alpha: 0.003066950103434496\n",
      "Saving content.\n",
      "epoch 492 iteration0, G Loss: 1.3105647563934326, D Loss: 1.232886791229248, alpha: 0.002994437345583467\n",
      "epoch 492 iteration100, G Loss: 1.1922987699508667, D Loss: 1.0720243453979492, alpha: 0.002994437345583467\n",
      "epoch 492 iteration200, G Loss: 0.8915402293205261, D Loss: 1.044297218322754, alpha: 0.002994437345583467\n",
      "epoch 492 iteration300, G Loss: 1.4601463079452515, D Loss: 1.0780227184295654, alpha: 0.002994437345583467\n",
      "epoch 492 iteration400, G Loss: 0.9477210640907288, D Loss: 1.1131768226623535, alpha: 0.002994437345583467\n",
      "epoch 492 iteration500, G Loss: 1.0145336389541626, D Loss: 1.169715166091919, alpha: 0.002994437345583467\n",
      "epoch 492 iteration600, G Loss: 1.0001188516616821, D Loss: 1.1271042823791504, alpha: 0.002994437345583467\n",
      "epoch 492 iteration700, G Loss: 0.8963260650634766, D Loss: 1.1787846088409424, alpha: 0.002994437345583467\n",
      "epoch 492 iteration800, G Loss: 0.94269859790802, D Loss: 1.1970301866531372, alpha: 0.002994437345583467\n",
      "Saving content.\n",
      "epoch 493 iteration0, G Loss: 1.1065784692764282, D Loss: 1.0961484909057617, alpha: 0.002923633999304842\n",
      "epoch 493 iteration100, G Loss: 0.8273500800132751, D Loss: 1.073487639427185, alpha: 0.002923633999304842\n",
      "epoch 493 iteration200, G Loss: 0.7167556881904602, D Loss: 1.2498741149902344, alpha: 0.002923633999304842\n",
      "epoch 493 iteration300, G Loss: 1.0069650411605835, D Loss: 1.0874464511871338, alpha: 0.002923633999304842\n",
      "epoch 493 iteration400, G Loss: 0.7980667352676392, D Loss: 1.1156843900680542, alpha: 0.002923633999304842\n",
      "epoch 493 iteration500, G Loss: 0.7142980694770813, D Loss: 1.2075269222259521, alpha: 0.002923633999304842\n",
      "epoch 493 iteration600, G Loss: 1.0203033685684204, D Loss: 1.2276520729064941, alpha: 0.002923633999304842\n",
      "epoch 493 iteration700, G Loss: 1.142330288887024, D Loss: 1.231259822845459, alpha: 0.002923633999304842\n",
      "epoch 493 iteration800, G Loss: 0.8136330842971802, D Loss: 1.080256462097168, alpha: 0.002923633999304842\n",
      "Saving content.\n",
      "epoch 494 iteration0, G Loss: 0.9855941534042358, D Loss: 1.206319808959961, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration100, G Loss: 0.8826424479484558, D Loss: 1.2771306037902832, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration200, G Loss: 0.7781895399093628, D Loss: 1.214048147201538, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration300, G Loss: 0.7870722413063049, D Loss: 1.114300012588501, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration400, G Loss: 0.8219704031944275, D Loss: 1.177574634552002, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration500, G Loss: 0.8589841723442078, D Loss: 1.2046805620193481, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration600, G Loss: 0.9146950244903564, D Loss: 1.1315813064575195, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration700, G Loss: 0.7821938991546631, D Loss: 1.1311933994293213, alpha: 0.0028545000020072653\n",
      "epoch 494 iteration800, G Loss: 0.8502594232559204, D Loss: 1.137554407119751, alpha: 0.0028545000020072653\n",
      "Saving content.\n",
      "epoch 495 iteration0, G Loss: 0.786887526512146, D Loss: 1.0930988788604736, alpha: 0.002786996219042215\n",
      "epoch 495 iteration100, G Loss: 0.9910199642181396, D Loss: 1.0893433094024658, alpha: 0.002786996219042215\n",
      "epoch 495 iteration200, G Loss: 1.0990232229232788, D Loss: 1.226436972618103, alpha: 0.002786996219042215\n",
      "epoch 495 iteration300, G Loss: 0.9234189391136169, D Loss: 1.2120201587677002, alpha: 0.002786996219042215\n",
      "epoch 495 iteration400, G Loss: 0.9737856388092041, D Loss: 1.1390366554260254, alpha: 0.002786996219042215\n",
      "epoch 495 iteration500, G Loss: 0.8922086358070374, D Loss: 1.2336781024932861, alpha: 0.002786996219042215\n",
      "epoch 495 iteration600, G Loss: 1.0071519613265991, D Loss: 1.1038308143615723, alpha: 0.002786996219042215\n",
      "epoch 495 iteration700, G Loss: 1.087087631225586, D Loss: 1.1189165115356445, alpha: 0.002786996219042215\n",
      "epoch 495 iteration800, G Loss: 1.2561652660369873, D Loss: 1.1863322257995605, alpha: 0.002786996219042215\n",
      "Saving content.\n",
      "epoch 496 iteration0, G Loss: 0.7487677335739136, D Loss: 1.1878035068511963, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration100, G Loss: 0.7290368676185608, D Loss: 0.9777692556381226, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration200, G Loss: 1.0567834377288818, D Loss: 1.190803050994873, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration300, G Loss: 0.8497806787490845, D Loss: 1.222272515296936, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration400, G Loss: 0.9348193407058716, D Loss: 1.1844449043273926, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration500, G Loss: 0.9335193634033203, D Loss: 1.0824413299560547, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration600, G Loss: 1.3569499254226685, D Loss: 1.039578914642334, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration700, G Loss: 0.7775817513465881, D Loss: 1.0783758163452148, alpha: 0.0027210844227250064\n",
      "epoch 496 iteration800, G Loss: 0.7803096771240234, D Loss: 1.2652215957641602, alpha: 0.0027210844227250064\n",
      "Saving content.\n",
      "epoch 497 iteration0, G Loss: 0.9814215898513794, D Loss: 1.219080924987793, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration100, G Loss: 0.8403293490409851, D Loss: 1.1230406761169434, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration200, G Loss: 1.1972094774246216, D Loss: 1.1360362768173218, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration300, G Loss: 0.8099548816680908, D Loss: 1.073185920715332, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration400, G Loss: 0.9017988443374634, D Loss: 1.0799481868743896, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration500, G Loss: 0.8515222668647766, D Loss: 1.1033250093460083, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration600, G Loss: 0.7966793775558472, D Loss: 1.131734013557434, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration700, G Loss: 0.9354816675186157, D Loss: 1.1199355125427246, alpha: 0.0026567272718047708\n",
      "epoch 497 iteration800, G Loss: 0.7991016507148743, D Loss: 1.1456515789031982, alpha: 0.0026567272718047708\n",
      "Saving content.\n",
      "epoch 498 iteration0, G Loss: 1.345237135887146, D Loss: 1.1988286972045898, alpha: 0.002593888291378965\n",
      "epoch 498 iteration100, G Loss: 0.7307543158531189, D Loss: 1.0918077230453491, alpha: 0.002593888291378965\n",
      "epoch 498 iteration200, G Loss: 1.055002212524414, D Loss: 1.2363760471343994, alpha: 0.002593888291378965\n",
      "epoch 498 iteration300, G Loss: 0.9968807101249695, D Loss: 1.095081090927124, alpha: 0.002593888291378965\n",
      "epoch 498 iteration400, G Loss: 0.7959200143814087, D Loss: 1.1463134288787842, alpha: 0.002593888291378965\n",
      "epoch 498 iteration500, G Loss: 0.9113587141036987, D Loss: 1.1506699323654175, alpha: 0.002593888291378965\n",
      "epoch 498 iteration600, G Loss: 0.7032931447029114, D Loss: 1.299878478050232, alpha: 0.002593888291378965\n",
      "epoch 498 iteration700, G Loss: 0.5583094954490662, D Loss: 1.0997037887573242, alpha: 0.002593888291378965\n",
      "epoch 498 iteration800, G Loss: 0.9370007514953613, D Loss: 1.1666432619094849, alpha: 0.002593888291378965\n",
      "Saving content.\n",
      "epoch 499 iteration0, G Loss: 1.0416351556777954, D Loss: 1.1622718572616577, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration100, G Loss: 1.1868642568588257, D Loss: 1.133365511894226, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration200, G Loss: 0.9544761776924133, D Loss: 1.1607314348220825, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration300, G Loss: 0.9224789142608643, D Loss: 1.3048319816589355, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration400, G Loss: 0.9388523101806641, D Loss: 1.067094326019287, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration500, G Loss: 1.03622305393219, D Loss: 1.0379130840301514, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration600, G Loss: 0.7231481075286865, D Loss: 1.3304243087768555, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration700, G Loss: 0.9593876004219055, D Loss: 1.130313754081726, alpha: 0.0025325318532376517\n",
      "epoch 499 iteration800, G Loss: 0.6657888889312744, D Loss: 1.2416651248931885, alpha: 0.0025325318532376517\n",
      "Saving content.\n",
      "epoch 500 iteration0, G Loss: 0.962615966796875, D Loss: 1.1142886877059937, alpha: 0.002472623156634657\n",
      "epoch 500 iteration100, G Loss: 1.1314363479614258, D Loss: 1.0876827239990234, alpha: 0.002472623156634657\n",
      "epoch 500 iteration200, G Loss: 1.1114623546600342, D Loss: 1.1037591695785522, alpha: 0.002472623156634657\n",
      "epoch 500 iteration300, G Loss: 1.3470250368118286, D Loss: 1.2178983688354492, alpha: 0.002472623156634657\n",
      "epoch 500 iteration400, G Loss: 0.9092209339141846, D Loss: 1.2339725494384766, alpha: 0.002472623156634657\n",
      "epoch 500 iteration500, G Loss: 0.8261657357215881, D Loss: 1.1666228771209717, alpha: 0.002472623156634657\n",
      "epoch 500 iteration600, G Loss: 1.1747955083847046, D Loss: 1.1464271545410156, alpha: 0.002472623156634657\n",
      "epoch 500 iteration700, G Loss: 0.7088012099266052, D Loss: 1.1395437717437744, alpha: 0.002472623156634657\n",
      "epoch 500 iteration800, G Loss: 1.0017285346984863, D Loss: 1.0704562664031982, alpha: 0.002472623156634657\n",
      "Saving content.\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604b15a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "epoch 0 iteration0, G Loss: 0.13222576677799225, D Loss: 1.3904517889022827, alpha: 0.9975273768433652\n",
      "epoch 0 iteration100, G Loss: 0.8248666524887085, D Loss: 1.3960212469100952, alpha: 0.9975273768433652\n",
      "epoch 0 iteration200, G Loss: 4.68881368637085, D Loss: 3.4649715423583984, alpha: 0.9975273768433652\n",
      "epoch 0 iteration300, G Loss: 0.9256094694137573, D Loss: 1.3363502025604248, alpha: 0.9975273768433652\n",
      "epoch 0 iteration400, G Loss: 0.9438071846961975, D Loss: 1.372953176498413, alpha: 0.9975273768433652\n",
      "epoch 0 iteration500, G Loss: 1.2112414836883545, D Loss: 1.3222359418869019, alpha: 0.9975273768433652\n",
      "epoch 0 iteration600, G Loss: 0.9867650270462036, D Loss: 1.3569319248199463, alpha: 0.9975273768433652\n",
      "epoch 0 iteration700, G Loss: 0.48099738359451294, D Loss: 1.5032157897949219, alpha: 0.9975273768433652\n",
      "epoch 0 iteration800, G Loss: 1.000022053718567, D Loss: 1.3534870147705078, alpha: 0.9975273768433652\n",
      "Saving content.\n",
      "epoch 1 iteration0, G Loss: 1.351184368133545, D Loss: 1.3954142332077026, alpha: 0.9974674681467623\n",
      "epoch 1 iteration100, G Loss: 0.8841879963874817, D Loss: 1.359540581703186, alpha: 0.9974674681467623\n",
      "epoch 1 iteration200, G Loss: 0.4904063940048218, D Loss: 1.6561822891235352, alpha: 0.9974674681467623\n",
      "epoch 1 iteration300, G Loss: 0.7322113513946533, D Loss: 1.3387234210968018, alpha: 0.9974674681467623\n",
      "epoch 1 iteration400, G Loss: 1.1187314987182617, D Loss: 1.371126413345337, alpha: 0.9974674681467623\n",
      "epoch 1 iteration500, G Loss: 0.9305008053779602, D Loss: 1.3128001689910889, alpha: 0.9974674681467623\n",
      "epoch 1 iteration600, G Loss: 0.7852428555488586, D Loss: 1.417157530784607, alpha: 0.9974674681467623\n",
      "epoch 1 iteration700, G Loss: 1.2134708166122437, D Loss: 1.3602607250213623, alpha: 0.9974674681467623\n",
      "epoch 1 iteration800, G Loss: 1.1010123491287231, D Loss: 1.284052848815918, alpha: 0.9974674681467623\n",
      "Saving content.\n",
      "epoch 2 iteration0, G Loss: 0.9470294117927551, D Loss: 1.0727565288543701, alpha: 0.997406111708621\n",
      "epoch 2 iteration100, G Loss: 0.9928544163703918, D Loss: 1.3382465839385986, alpha: 0.997406111708621\n",
      "epoch 2 iteration200, G Loss: 1.960709571838379, D Loss: 1.4342907667160034, alpha: 0.997406111708621\n",
      "epoch 2 iteration300, G Loss: 0.8791535496711731, D Loss: 1.3751384019851685, alpha: 0.997406111708621\n",
      "epoch 2 iteration400, G Loss: 0.7958452105522156, D Loss: 1.4021623134613037, alpha: 0.997406111708621\n",
      "epoch 2 iteration500, G Loss: 1.1033040285110474, D Loss: 1.4151678085327148, alpha: 0.997406111708621\n",
      "epoch 2 iteration600, G Loss: 1.137215495109558, D Loss: 1.7721720933914185, alpha: 0.997406111708621\n",
      "epoch 2 iteration700, G Loss: 0.7996101379394531, D Loss: 1.3686583042144775, alpha: 0.997406111708621\n",
      "epoch 2 iteration800, G Loss: 0.4468814730644226, D Loss: 1.596842885017395, alpha: 0.997406111708621\n",
      "Saving content.\n",
      "epoch 3 iteration0, G Loss: 0.7808377146720886, D Loss: 1.3638371229171753, alpha: 0.9973432727281952\n",
      "epoch 3 iteration100, G Loss: 1.497367024421692, D Loss: 1.3776819705963135, alpha: 0.9973432727281952\n",
      "epoch 3 iteration200, G Loss: 0.8436312675476074, D Loss: 1.3655167818069458, alpha: 0.9973432727281952\n",
      "epoch 3 iteration300, G Loss: 0.45842379331588745, D Loss: 1.5287244319915771, alpha: 0.9973432727281952\n",
      "epoch 3 iteration400, G Loss: 0.9214058518409729, D Loss: 1.4237523078918457, alpha: 0.9973432727281952\n",
      "epoch 3 iteration500, G Loss: 1.251907467842102, D Loss: 1.4415417909622192, alpha: 0.9973432727281952\n",
      "epoch 3 iteration600, G Loss: 0.8490824699401855, D Loss: 1.439629077911377, alpha: 0.9973432727281952\n",
      "epoch 3 iteration700, G Loss: 0.5803353190422058, D Loss: 1.3681039810180664, alpha: 0.9973432727281952\n",
      "epoch 3 iteration800, G Loss: 1.0808899402618408, D Loss: 1.3531227111816406, alpha: 0.9973432727281952\n",
      "Saving content.\n",
      "epoch 4 iteration0, G Loss: 0.9256135821342468, D Loss: 1.374904990196228, alpha: 0.9972789155772751\n",
      "epoch 4 iteration100, G Loss: 0.9338420629501343, D Loss: 1.4503910541534424, alpha: 0.9972789155772751\n",
      "epoch 4 iteration200, G Loss: 0.6862135529518127, D Loss: 1.3063546419143677, alpha: 0.9972789155772751\n",
      "epoch 4 iteration300, G Loss: 0.9403253197669983, D Loss: 1.3467826843261719, alpha: 0.9972789155772751\n",
      "epoch 4 iteration400, G Loss: 1.0874382257461548, D Loss: 1.4518921375274658, alpha: 0.9972789155772751\n",
      "epoch 4 iteration500, G Loss: 1.057823896408081, D Loss: 1.2692204713821411, alpha: 0.9972789155772751\n",
      "epoch 4 iteration600, G Loss: 0.9247534275054932, D Loss: 1.3264412879943848, alpha: 0.9972789155772751\n",
      "epoch 4 iteration700, G Loss: 0.924958348274231, D Loss: 1.350393295288086, alpha: 0.9972789155772751\n",
      "epoch 4 iteration800, G Loss: 1.1844638586044312, D Loss: 1.2255516052246094, alpha: 0.9972789155772751\n",
      "Saving content.\n",
      "epoch 5 iteration0, G Loss: 1.3825899362564087, D Loss: 1.4187469482421875, alpha: 0.9972130037809577\n",
      "epoch 5 iteration100, G Loss: 1.1106191873550415, D Loss: 1.2997486591339111, alpha: 0.9972130037809577\n",
      "epoch 5 iteration200, G Loss: 0.8609987497329712, D Loss: 1.5067708492279053, alpha: 0.9972130037809577\n",
      "epoch 5 iteration300, G Loss: 0.5371848940849304, D Loss: 1.6682196855545044, alpha: 0.9972130037809577\n",
      "epoch 5 iteration400, G Loss: 1.0438520908355713, D Loss: 1.3393802642822266, alpha: 0.9972130037809577\n",
      "epoch 5 iteration500, G Loss: 1.3359460830688477, D Loss: 1.3114964962005615, alpha: 0.9972130037809577\n",
      "epoch 5 iteration600, G Loss: 1.4067457914352417, D Loss: 1.2822972536087036, alpha: 0.9972130037809577\n",
      "epoch 5 iteration700, G Loss: 3.090942859649658, D Loss: 1.5025506019592285, alpha: 0.9972130037809577\n",
      "epoch 5 iteration800, G Loss: 1.015333652496338, D Loss: 1.3941010236740112, alpha: 0.9972130037809577\n",
      "Saving content.\n",
      "epoch 6 iteration0, G Loss: 1.405110239982605, D Loss: 1.3449746370315552, alpha: 0.9971454999979927\n",
      "epoch 6 iteration100, G Loss: 0.949907124042511, D Loss: 1.3925926685333252, alpha: 0.9971454999979927\n",
      "epoch 6 iteration200, G Loss: 0.8293584585189819, D Loss: 1.4509050846099854, alpha: 0.9971454999979927\n",
      "epoch 6 iteration300, G Loss: 0.9932125210762024, D Loss: 1.387768268585205, alpha: 0.9971454999979927\n",
      "epoch 6 iteration400, G Loss: 1.0203787088394165, D Loss: 1.4251950979232788, alpha: 0.9971454999979927\n",
      "epoch 6 iteration500, G Loss: 0.9001163840293884, D Loss: 1.3101081848144531, alpha: 0.9971454999979927\n",
      "epoch 6 iteration600, G Loss: 1.0150678157806396, D Loss: 1.4722223281860352, alpha: 0.9971454999979927\n",
      "epoch 6 iteration700, G Loss: 1.053195834159851, D Loss: 1.4699360132217407, alpha: 0.9971454999979927\n",
      "epoch 6 iteration800, G Loss: 1.1729090213775635, D Loss: 1.3508937358856201, alpha: 0.9971454999979927\n",
      "Saving content.\n",
      "epoch 7 iteration0, G Loss: 0.4848344326019287, D Loss: 1.3134708404541016, alpha: 0.9970763660006952\n",
      "epoch 7 iteration100, G Loss: 0.8932414650917053, D Loss: 1.286958932876587, alpha: 0.9970763660006952\n",
      "epoch 7 iteration200, G Loss: 1.0649582147598267, D Loss: 1.323166847229004, alpha: 0.9970763660006952\n",
      "epoch 7 iteration300, G Loss: 0.9196970462799072, D Loss: 1.347373127937317, alpha: 0.9970763660006952\n",
      "epoch 7 iteration400, G Loss: 1.0207523107528687, D Loss: 1.4352376461029053, alpha: 0.9970763660006952\n",
      "epoch 7 iteration500, G Loss: 0.4859784245491028, D Loss: 1.6168066263198853, alpha: 0.9970763660006952\n",
      "epoch 7 iteration600, G Loss: 1.0956661701202393, D Loss: 1.3488969802856445, alpha: 0.9970763660006952\n",
      "epoch 7 iteration700, G Loss: 0.9694340229034424, D Loss: 1.3343784809112549, alpha: 0.9970763660006952\n",
      "epoch 7 iteration800, G Loss: 0.9584858417510986, D Loss: 1.3117725849151611, alpha: 0.9970763660006952\n",
      "Saving content.\n",
      "epoch 8 iteration0, G Loss: 1.201941728591919, D Loss: 1.4077551364898682, alpha: 0.9970055626544164\n",
      "epoch 8 iteration100, G Loss: 0.9934196472167969, D Loss: 1.2942686080932617, alpha: 0.9970055626544164\n",
      "epoch 8 iteration200, G Loss: 0.9580828547477722, D Loss: 1.324135661125183, alpha: 0.9970055626544164\n",
      "epoch 8 iteration300, G Loss: 1.1542398929595947, D Loss: 1.2174606323242188, alpha: 0.9970055626544164\n",
      "epoch 8 iteration400, G Loss: 0.9036628007888794, D Loss: 1.2880008220672607, alpha: 0.9970055626544164\n",
      "epoch 8 iteration500, G Loss: 1.5620834827423096, D Loss: 1.181816577911377, alpha: 0.9970055626544164\n",
      "epoch 8 iteration600, G Loss: 1.3983793258666992, D Loss: 1.2077014446258545, alpha: 0.9970055626544164\n",
      "epoch 8 iteration700, G Loss: 0.9888777136802673, D Loss: 1.240320086479187, alpha: 0.9970055626544164\n",
      "epoch 8 iteration800, G Loss: 1.3701356649398804, D Loss: 1.5232646465301514, alpha: 0.9970055626544164\n",
      "Saving content.\n",
      "epoch 9 iteration0, G Loss: 1.3874977827072144, D Loss: 1.30088472366333, alpha: 0.9969330498965654\n",
      "epoch 9 iteration100, G Loss: 2.627960205078125, D Loss: 1.8255912065505981, alpha: 0.9969330498965654\n",
      "epoch 9 iteration200, G Loss: 0.7532333135604858, D Loss: 1.3302416801452637, alpha: 0.9969330498965654\n",
      "epoch 9 iteration300, G Loss: 0.8434203863143921, D Loss: 1.2005054950714111, alpha: 0.9969330498965654\n",
      "epoch 9 iteration400, G Loss: 0.5903698801994324, D Loss: 1.2943682670593262, alpha: 0.9969330498965654\n",
      "epoch 9 iteration500, G Loss: 1.0710811614990234, D Loss: 1.2797468900680542, alpha: 0.9969330498965654\n",
      "epoch 9 iteration600, G Loss: 1.0232239961624146, D Loss: 1.4728652238845825, alpha: 0.9969330498965654\n",
      "epoch 9 iteration700, G Loss: 2.1137804985046387, D Loss: 1.2628841400146484, alpha: 0.9969330498965654\n",
      "epoch 9 iteration800, G Loss: 1.4825594425201416, D Loss: 1.5568695068359375, alpha: 0.9969330498965654\n",
      "Saving content.\n",
      "epoch 10 iteration0, G Loss: 1.157133936882019, D Loss: 1.271000862121582, alpha: 0.9968587867151706\n",
      "epoch 10 iteration100, G Loss: 0.8896055221557617, D Loss: 1.299940586090088, alpha: 0.9968587867151706\n",
      "epoch 10 iteration200, G Loss: 1.0613751411437988, D Loss: 1.3204196691513062, alpha: 0.9968587867151706\n",
      "epoch 10 iteration300, G Loss: 0.8930219411849976, D Loss: 1.3682855367660522, alpha: 0.9968587867151706\n",
      "epoch 10 iteration400, G Loss: 1.291816234588623, D Loss: 1.2270597219467163, alpha: 0.9968587867151706\n",
      "epoch 10 iteration500, G Loss: 1.2366795539855957, D Loss: 1.4010623693466187, alpha: 0.9968587867151706\n",
      "epoch 10 iteration600, G Loss: 1.0248572826385498, D Loss: 1.2752130031585693, alpha: 0.9968587867151706\n",
      "epoch 10 iteration700, G Loss: 1.73811674118042, D Loss: 1.397713541984558, alpha: 0.9968587867151706\n",
      "epoch 10 iteration800, G Loss: 1.030348777770996, D Loss: 1.3602341413497925, alpha: 0.9968587867151706\n",
      "Saving content.\n",
      "epoch 11 iteration0, G Loss: 1.1034835577011108, D Loss: 1.3056354522705078, alpha: 0.9967827311269749\n",
      "epoch 11 iteration100, G Loss: 0.726113498210907, D Loss: 1.391943097114563, alpha: 0.9967827311269749\n",
      "epoch 11 iteration200, G Loss: 0.7627891898155212, D Loss: 1.393585205078125, alpha: 0.9967827311269749\n",
      "epoch 11 iteration300, G Loss: 1.0607836246490479, D Loss: 1.3069167137145996, alpha: 0.9967827311269749\n",
      "epoch 11 iteration400, G Loss: 0.8574714660644531, D Loss: 1.3916914463043213, alpha: 0.9967827311269749\n",
      "epoch 11 iteration500, G Loss: 0.8481171131134033, D Loss: 1.3593604564666748, alpha: 0.9967827311269749\n",
      "epoch 11 iteration600, G Loss: 0.9651530981063843, D Loss: 1.352118968963623, alpha: 0.9967827311269749\n",
      "epoch 11 iteration700, G Loss: 0.8022457957267761, D Loss: 1.418312430381775, alpha: 0.9967827311269749\n",
      "epoch 11 iteration800, G Loss: 1.1874717473983765, D Loss: 1.3734376430511475, alpha: 0.9967827311269749\n",
      "Saving content.\n",
      "epoch 12 iteration0, G Loss: 0.886338472366333, D Loss: 1.347690224647522, alpha: 0.9967048401550548\n",
      "epoch 12 iteration100, G Loss: 1.1556655168533325, D Loss: 1.2949453592300415, alpha: 0.9967048401550548\n",
      "epoch 12 iteration200, G Loss: 1.2017323970794678, D Loss: 1.1714526414871216, alpha: 0.9967048401550548\n",
      "epoch 12 iteration300, G Loss: 1.191686987876892, D Loss: 1.849562406539917, alpha: 0.9967048401550548\n",
      "epoch 12 iteration400, G Loss: 0.8525537252426147, D Loss: 1.4335664510726929, alpha: 0.9967048401550548\n",
      "epoch 12 iteration500, G Loss: 1.5344704389572144, D Loss: 1.314120888710022, alpha: 0.9967048401550548\n",
      "epoch 12 iteration600, G Loss: 1.0311200618743896, D Loss: 1.2732698917388916, alpha: 0.9967048401550548\n",
      "epoch 12 iteration700, G Loss: 1.2365766763687134, D Loss: 1.459808349609375, alpha: 0.9967048401550548\n",
      "epoch 12 iteration800, G Loss: 1.0304726362228394, D Loss: 1.3072903156280518, alpha: 0.9967048401550548\n",
      "Saving content.\n",
      "epoch 13 iteration0, G Loss: 1.0067451000213623, D Loss: 1.254155158996582, alpha: 0.9966250698059539\n",
      "epoch 13 iteration100, G Loss: 1.4999667406082153, D Loss: 1.3971624374389648, alpha: 0.9966250698059539\n",
      "epoch 13 iteration200, G Loss: 1.0915383100509644, D Loss: 1.0866992473602295, alpha: 0.9966250698059539\n",
      "epoch 13 iteration300, G Loss: 1.0864018201828003, D Loss: 1.2062315940856934, alpha: 0.9966250698059539\n",
      "epoch 13 iteration400, G Loss: 1.0471529960632324, D Loss: 1.5420610904693604, alpha: 0.9966250698059539\n",
      "epoch 13 iteration500, G Loss: 1.4533195495605469, D Loss: 1.1500635147094727, alpha: 0.9966250698059539\n",
      "epoch 13 iteration600, G Loss: 1.6186773777008057, D Loss: 1.4185376167297363, alpha: 0.9966250698059539\n",
      "epoch 13 iteration700, G Loss: 1.1375728845596313, D Loss: 1.2940394878387451, alpha: 0.9966250698059539\n",
      "epoch 13 iteration800, G Loss: 0.7228186726570129, D Loss: 1.2554922103881836, alpha: 0.9966250698059539\n",
      "Saving content.\n",
      "epoch 14 iteration0, G Loss: 1.5773534774780273, D Loss: 1.6614362001419067, alpha: 0.9965433750463222\n",
      "epoch 14 iteration100, G Loss: 1.058663249015808, D Loss: 1.2052810192108154, alpha: 0.9965433750463222\n",
      "epoch 14 iteration200, G Loss: 0.8538270592689514, D Loss: 1.3519086837768555, alpha: 0.9965433750463222\n",
      "epoch 14 iteration300, G Loss: 1.4029216766357422, D Loss: 1.6334712505340576, alpha: 0.9965433750463222\n",
      "epoch 14 iteration400, G Loss: 1.2751973867416382, D Loss: 1.3171340227127075, alpha: 0.9965433750463222\n",
      "epoch 14 iteration500, G Loss: 1.1324224472045898, D Loss: 1.1845040321350098, alpha: 0.9965433750463222\n",
      "epoch 14 iteration600, G Loss: 0.794557511806488, D Loss: 1.3010201454162598, alpha: 0.9965433750463222\n",
      "epoch 14 iteration700, G Loss: 0.9357848763465881, D Loss: 1.2912185192108154, alpha: 0.9965433750463222\n",
      "epoch 14 iteration800, G Loss: 1.0999376773834229, D Loss: 1.2459509372711182, alpha: 0.9965433750463222\n",
      "Saving content.\n",
      "epoch 15 iteration0, G Loss: 1.6399874687194824, D Loss: 1.828995704650879, alpha: 0.9964597097790535\n",
      "epoch 15 iteration100, G Loss: 0.9974828958511353, D Loss: 1.2839024066925049, alpha: 0.9964597097790535\n",
      "epoch 15 iteration200, G Loss: 1.3843508958816528, D Loss: 1.3008495569229126, alpha: 0.9964597097790535\n",
      "epoch 15 iteration300, G Loss: 1.1638749837875366, D Loss: 1.2260820865631104, alpha: 0.9964597097790535\n",
      "epoch 15 iteration400, G Loss: 0.9805978536605835, D Loss: 1.2764118909835815, alpha: 0.9964597097790535\n",
      "epoch 15 iteration500, G Loss: 1.0754200220108032, D Loss: 1.1304696798324585, alpha: 0.9964597097790535\n",
      "epoch 15 iteration600, G Loss: 2.4894962310791016, D Loss: 1.4515939950942993, alpha: 0.9964597097790535\n",
      "epoch 15 iteration700, G Loss: 0.56479811668396, D Loss: 1.4469408988952637, alpha: 0.9964597097790535\n",
      "epoch 15 iteration800, G Loss: 1.1170427799224854, D Loss: 1.2067606449127197, alpha: 0.9964597097790535\n",
      "Saving content.\n",
      "epoch 16 iteration0, G Loss: 0.6812729835510254, D Loss: 2.283493995666504, alpha: 0.9963740268189091\n",
      "epoch 16 iteration100, G Loss: 1.2509982585906982, D Loss: 1.2326626777648926, alpha: 0.9963740268189091\n",
      "epoch 16 iteration200, G Loss: 1.042472243309021, D Loss: 1.2562828063964844, alpha: 0.9963740268189091\n",
      "epoch 16 iteration300, G Loss: 1.1226394176483154, D Loss: 1.2147505283355713, alpha: 0.9963740268189091\n",
      "epoch 16 iteration400, G Loss: 1.0486233234405518, D Loss: 1.327101707458496, alpha: 0.9963740268189091\n",
      "epoch 16 iteration500, G Loss: 1.3967479467391968, D Loss: 1.433280110359192, alpha: 0.9963740268189091\n",
      "epoch 16 iteration600, G Loss: 1.160064935684204, D Loss: 1.1100962162017822, alpha: 0.9963740268189091\n",
      "epoch 16 iteration700, G Loss: 0.9545249938964844, D Loss: 1.29673171043396, alpha: 0.9963740268189091\n",
      "epoch 16 iteration800, G Loss: 0.9855768084526062, D Loss: 1.2804609537124634, alpha: 0.9963740268189091\n",
      "Saving content.\n",
      "epoch 17 iteration0, G Loss: 0.9200378656387329, D Loss: 1.2268837690353394, alpha: 0.9962862778676213\n",
      "epoch 17 iteration100, G Loss: 0.47877901792526245, D Loss: 1.456662893295288, alpha: 0.9962862778676213\n",
      "epoch 17 iteration200, G Loss: 0.8151209354400635, D Loss: 1.4109454154968262, alpha: 0.9962862778676213\n",
      "epoch 17 iteration300, G Loss: 1.270850658416748, D Loss: 1.2964152097702026, alpha: 0.9962862778676213\n",
      "epoch 17 iteration400, G Loss: 0.5522797703742981, D Loss: 2.013725757598877, alpha: 0.9962862778676213\n",
      "epoch 17 iteration500, G Loss: 1.0454102754592896, D Loss: 1.4367179870605469, alpha: 0.9962862778676213\n",
      "epoch 17 iteration600, G Loss: 0.7250476479530334, D Loss: 1.1622505187988281, alpha: 0.9962862778676213\n",
      "epoch 17 iteration700, G Loss: 0.9992436766624451, D Loss: 1.3391010761260986, alpha: 0.9962862778676213\n",
      "epoch 17 iteration800, G Loss: 0.7106145024299622, D Loss: 1.2767555713653564, alpha: 0.9962862778676213\n",
      "Saving content.\n",
      "epoch 18 iteration0, G Loss: 0.9805150628089905, D Loss: 1.272587776184082, alpha: 0.9961964134884649\n",
      "epoch 18 iteration100, G Loss: 1.4836770296096802, D Loss: 1.4398620128631592, alpha: 0.9961964134884649\n",
      "epoch 18 iteration200, G Loss: 0.9777431488037109, D Loss: 1.295333743095398, alpha: 0.9961964134884649\n",
      "epoch 18 iteration300, G Loss: 0.9225567579269409, D Loss: 1.5189976692199707, alpha: 0.9961964134884649\n",
      "epoch 18 iteration400, G Loss: 1.2031960487365723, D Loss: 1.2619338035583496, alpha: 0.9961964134884649\n",
      "epoch 18 iteration500, G Loss: 1.7721450328826904, D Loss: 1.4332809448242188, alpha: 0.9961964134884649\n",
      "epoch 18 iteration600, G Loss: 1.2465593814849854, D Loss: 1.2516326904296875, alpha: 0.9961964134884649\n",
      "epoch 18 iteration700, G Loss: 1.0600652694702148, D Loss: 1.13944673538208, alpha: 0.9961964134884649\n",
      "epoch 18 iteration800, G Loss: 0.9956191182136536, D Loss: 1.276881217956543, alpha: 0.9961964134884649\n",
      "Saving content.\n",
      "epoch 19 iteration0, G Loss: 0.8060814142227173, D Loss: 1.3398712873458862, alpha: 0.9961043830802904\n",
      "epoch 19 iteration100, G Loss: 0.826865017414093, D Loss: 1.279623031616211, alpha: 0.9961043830802904\n",
      "epoch 19 iteration200, G Loss: 1.2741533517837524, D Loss: 1.3272981643676758, alpha: 0.9961043830802904\n",
      "epoch 19 iteration300, G Loss: 1.2706340551376343, D Loss: 1.118910551071167, alpha: 0.9961043830802904\n",
      "epoch 19 iteration400, G Loss: 1.575814127922058, D Loss: 1.213667869567871, alpha: 0.9961043830802904\n",
      "epoch 19 iteration500, G Loss: 0.885198712348938, D Loss: 1.247426986694336, alpha: 0.9961043830802904\n",
      "epoch 19 iteration600, G Loss: 0.7125943899154663, D Loss: 1.1646850109100342, alpha: 0.9961043830802904\n",
      "epoch 19 iteration700, G Loss: 2.4054064750671387, D Loss: 1.253857135772705, alpha: 0.9961043830802904\n",
      "epoch 19 iteration800, G Loss: 2.860461711883545, D Loss: 1.243889570236206, alpha: 0.9961043830802904\n",
      "Saving content.\n",
      "epoch 20 iteration0, G Loss: 1.504286766052246, D Loss: 1.287034273147583, alpha: 0.9960101348510059\n",
      "epoch 20 iteration100, G Loss: 1.4814640283584595, D Loss: 1.3448123931884766, alpha: 0.9960101348510059\n",
      "epoch 20 iteration200, G Loss: 0.9054341912269592, D Loss: 1.454353928565979, alpha: 0.9960101348510059\n",
      "epoch 20 iteration300, G Loss: 1.3075584173202515, D Loss: 1.2760051488876343, alpha: 0.9960101348510059\n",
      "epoch 20 iteration400, G Loss: 1.1502766609191895, D Loss: 1.2154361009597778, alpha: 0.9960101348510059\n",
      "epoch 20 iteration500, G Loss: 1.027237892150879, D Loss: 1.2916338443756104, alpha: 0.9960101348510059\n",
      "epoch 20 iteration600, G Loss: 0.7653886079788208, D Loss: 1.4873054027557373, alpha: 0.9960101348510059\n",
      "epoch 20 iteration700, G Loss: 1.8988442420959473, D Loss: 1.2091667652130127, alpha: 0.9960101348510059\n",
      "epoch 20 iteration800, G Loss: 0.8187445998191833, D Loss: 1.4618637561798096, alpha: 0.9960101348510059\n",
      "Saving content.\n",
      "epoch 21 iteration0, G Loss: 1.1810858249664307, D Loss: 1.1952935457229614, alpha: 0.9959136157905011\n",
      "epoch 21 iteration100, G Loss: 1.088114857673645, D Loss: 1.1676840782165527, alpha: 0.9959136157905011\n",
      "epoch 21 iteration200, G Loss: 0.8872624039649963, D Loss: 1.305898904800415, alpha: 0.9959136157905011\n",
      "epoch 21 iteration300, G Loss: 0.973147988319397, D Loss: 1.3154231309890747, alpha: 0.9959136157905011\n",
      "epoch 21 iteration400, G Loss: 1.1913630962371826, D Loss: 1.2610875368118286, alpha: 0.9959136157905011\n",
      "epoch 21 iteration500, G Loss: 1.1811120510101318, D Loss: 1.2090046405792236, alpha: 0.9959136157905011\n",
      "epoch 21 iteration600, G Loss: 1.1622471809387207, D Loss: 1.2561370134353638, alpha: 0.9959136157905011\n",
      "epoch 21 iteration700, G Loss: 1.7611967325210571, D Loss: 1.2684568166732788, alpha: 0.9959136157905011\n",
      "epoch 21 iteration800, G Loss: 0.8469587564468384, D Loss: 1.3278807401657104, alpha: 0.9959136157905011\n",
      "Saving content.\n",
      "epoch 22 iteration0, G Loss: 0.8316842317581177, D Loss: 1.2572906017303467, alpha: 0.9958147716430024\n",
      "epoch 22 iteration100, G Loss: 1.3058853149414062, D Loss: 1.956140160560608, alpha: 0.9958147716430024\n",
      "epoch 22 iteration200, G Loss: 1.0957667827606201, D Loss: 1.2503197193145752, alpha: 0.9958147716430024\n",
      "epoch 22 iteration300, G Loss: 0.844202995300293, D Loss: 1.5657086372375488, alpha: 0.9958147716430024\n",
      "epoch 22 iteration400, G Loss: 1.5176044702529907, D Loss: 1.3402180671691895, alpha: 0.9958147716430024\n",
      "epoch 22 iteration500, G Loss: 1.524370789527893, D Loss: 1.1475086212158203, alpha: 0.9958147716430024\n",
      "epoch 22 iteration600, G Loss: 0.9973059892654419, D Loss: 1.2483996152877808, alpha: 0.9958147716430024\n",
      "epoch 22 iteration700, G Loss: 1.1060194969177246, D Loss: 1.1648269891738892, alpha: 0.9958147716430024\n",
      "epoch 22 iteration800, G Loss: 1.126853346824646, D Loss: 1.2419072389602661, alpha: 0.9958147716430024\n",
      "Saving content.\n",
      "epoch 23 iteration0, G Loss: 0.9629589915275574, D Loss: 1.2949719429016113, alpha: 0.9957135468788503\n",
      "epoch 23 iteration100, G Loss: 1.1440989971160889, D Loss: 1.312610387802124, alpha: 0.9957135468788503\n",
      "epoch 23 iteration200, G Loss: 1.3070405721664429, D Loss: 1.18452787399292, alpha: 0.9957135468788503\n",
      "epoch 23 iteration300, G Loss: 1.195729374885559, D Loss: 1.297952651977539, alpha: 0.9957135468788503\n",
      "epoch 23 iteration400, G Loss: 1.103597640991211, D Loss: 1.2382893562316895, alpha: 0.9957135468788503\n",
      "epoch 23 iteration500, G Loss: 0.7427583932876587, D Loss: 1.2245292663574219, alpha: 0.9957135468788503\n",
      "epoch 23 iteration600, G Loss: 0.9996812343597412, D Loss: 1.2958416938781738, alpha: 0.9957135468788503\n",
      "epoch 23 iteration700, G Loss: 1.204734444618225, D Loss: 1.2562744617462158, alpha: 0.9957135468788503\n",
      "epoch 23 iteration800, G Loss: 0.9957202672958374, D Loss: 1.3481733798980713, alpha: 0.9957135468788503\n",
      "Saving content.\n",
      "epoch 24 iteration0, G Loss: 1.2332147359848022, D Loss: 1.2014544010162354, alpha: 0.995609884665689\n",
      "epoch 24 iteration100, G Loss: 1.0298253297805786, D Loss: 1.2326242923736572, alpha: 0.995609884665689\n",
      "epoch 24 iteration200, G Loss: 1.3028265237808228, D Loss: 1.396418571472168, alpha: 0.995609884665689\n",
      "epoch 24 iteration300, G Loss: 0.8947873711585999, D Loss: 1.3259027004241943, alpha: 0.995609884665689\n",
      "epoch 24 iteration400, G Loss: 1.1543022394180298, D Loss: 1.2377073764801025, alpha: 0.995609884665689\n",
      "epoch 24 iteration500, G Loss: 1.130098581314087, D Loss: 1.3220125436782837, alpha: 0.995609884665689\n",
      "epoch 24 iteration600, G Loss: 0.8433011770248413, D Loss: 1.2281674146652222, alpha: 0.995609884665689\n",
      "epoch 24 iteration700, G Loss: 0.9547889232635498, D Loss: 1.2563989162445068, alpha: 0.995609884665689\n",
      "epoch 24 iteration800, G Loss: 1.0575687885284424, D Loss: 1.2540562152862549, alpha: 0.995609884665689\n",
      "Saving content.\n",
      "epoch 25 iteration0, G Loss: 0.8690955638885498, D Loss: 1.2829763889312744, alpha: 0.9955037268390589\n",
      "epoch 25 iteration100, G Loss: 1.3245958089828491, D Loss: 1.2465145587921143, alpha: 0.9955037268390589\n",
      "epoch 25 iteration200, G Loss: 0.8702803254127502, D Loss: 1.3399004936218262, alpha: 0.9955037268390589\n",
      "epoch 25 iteration300, G Loss: 1.6990805864334106, D Loss: 1.2296870946884155, alpha: 0.9955037268390589\n",
      "epoch 25 iteration400, G Loss: 1.141108512878418, D Loss: 1.3152883052825928, alpha: 0.9955037268390589\n",
      "epoch 25 iteration500, G Loss: 1.3311872482299805, D Loss: 1.2302935123443604, alpha: 0.9955037268390589\n",
      "epoch 25 iteration600, G Loss: 1.2629029750823975, D Loss: 1.1980396509170532, alpha: 0.9955037268390589\n",
      "epoch 25 iteration700, G Loss: 1.3612339496612549, D Loss: 1.2516151666641235, alpha: 0.9955037268390589\n",
      "epoch 25 iteration800, G Loss: 1.0805975198745728, D Loss: 1.2968485355377197, alpha: 0.9955037268390589\n",
      "Saving content.\n",
      "epoch 26 iteration0, G Loss: 1.3601000308990479, D Loss: 1.597350001335144, alpha: 0.9953950138723812\n",
      "epoch 26 iteration100, G Loss: 1.2081568241119385, D Loss: 1.238300085067749, alpha: 0.9953950138723812\n",
      "epoch 26 iteration200, G Loss: 0.898021936416626, D Loss: 1.2899171113967896, alpha: 0.9953950138723812\n",
      "epoch 26 iteration300, G Loss: 0.9254924654960632, D Loss: 1.441873550415039, alpha: 0.9953950138723812\n",
      "epoch 26 iteration400, G Loss: 0.9766047596931458, D Loss: 1.250751256942749, alpha: 0.9953950138723812\n",
      "epoch 26 iteration500, G Loss: 0.8601251840591431, D Loss: 1.357579231262207, alpha: 0.9953950138723812\n",
      "epoch 26 iteration600, G Loss: 0.9332406520843506, D Loss: 1.3344244956970215, alpha: 0.9953950138723812\n",
      "epoch 26 iteration700, G Loss: 1.2077654600143433, D Loss: 1.1748510599136353, alpha: 0.9953950138723812\n",
      "epoch 26 iteration800, G Loss: 0.8620196580886841, D Loss: 1.3865686655044556, alpha: 0.9953950138723812\n",
      "Saving content.\n",
      "epoch 27 iteration0, G Loss: 0.9363378286361694, D Loss: 1.3575154542922974, alpha: 0.9952836848463282\n",
      "epoch 27 iteration100, G Loss: 1.0345309972763062, D Loss: 1.3485221862792969, alpha: 0.9952836848463282\n",
      "epoch 27 iteration200, G Loss: 0.7671828866004944, D Loss: 1.2566663026809692, alpha: 0.9952836848463282\n",
      "epoch 27 iteration300, G Loss: 0.8790578842163086, D Loss: 1.3146331310272217, alpha: 0.9952836848463282\n",
      "epoch 27 iteration400, G Loss: 0.7988831996917725, D Loss: 1.2808072566986084, alpha: 0.9952836848463282\n",
      "epoch 27 iteration500, G Loss: 0.8309100866317749, D Loss: 1.3256405591964722, alpha: 0.9952836848463282\n",
      "epoch 27 iteration600, G Loss: 0.9209253787994385, D Loss: 1.3387142419815063, alpha: 0.9952836848463282\n",
      "epoch 27 iteration700, G Loss: 0.7537708878517151, D Loss: 1.3059585094451904, alpha: 0.9952836848463282\n",
      "epoch 27 iteration800, G Loss: 0.9626779556274414, D Loss: 1.315271258354187, alpha: 0.9952836848463282\n",
      "Saving content.\n",
      "epoch 28 iteration0, G Loss: 0.9943812489509583, D Loss: 1.2504682540893555, alpha: 0.9951696774175651\n",
      "epoch 28 iteration100, G Loss: 1.032627820968628, D Loss: 1.283686876296997, alpha: 0.9951696774175651\n",
      "epoch 28 iteration200, G Loss: 1.0642356872558594, D Loss: 1.1294376850128174, alpha: 0.9951696774175651\n",
      "epoch 28 iteration300, G Loss: 1.3963944911956787, D Loss: 1.365537405014038, alpha: 0.9951696774175651\n",
      "epoch 28 iteration400, G Loss: 1.026171326637268, D Loss: 1.335235834121704, alpha: 0.9951696774175651\n",
      "epoch 28 iteration500, G Loss: 1.0888376235961914, D Loss: 1.2106907367706299, alpha: 0.9951696774175651\n",
      "epoch 28 iteration600, G Loss: 1.036832571029663, D Loss: 1.2868047952651978, alpha: 0.9951696774175651\n",
      "epoch 28 iteration700, G Loss: 0.8990498781204224, D Loss: 1.273200511932373, alpha: 0.9951696774175651\n",
      "epoch 28 iteration800, G Loss: 0.9936317205429077, D Loss: 1.316493034362793, alpha: 0.9951696774175651\n",
      "Saving content.\n",
      "epoch 29 iteration0, G Loss: 1.4627017974853516, D Loss: 1.3124486207962036, alpha: 0.9950529277868578\n",
      "epoch 29 iteration100, G Loss: 0.8375680446624756, D Loss: 1.4259910583496094, alpha: 0.9950529277868578\n",
      "epoch 29 iteration200, G Loss: 0.7546666860580444, D Loss: 1.2441765069961548, alpha: 0.9950529277868578\n",
      "epoch 29 iteration300, G Loss: 0.5893236994743347, D Loss: 1.4637959003448486, alpha: 0.9950529277868578\n",
      "epoch 29 iteration400, G Loss: 1.047420859336853, D Loss: 1.2927849292755127, alpha: 0.9950529277868578\n",
      "epoch 29 iteration500, G Loss: 0.9357880353927612, D Loss: 1.3225483894348145, alpha: 0.9950529277868578\n",
      "epoch 29 iteration600, G Loss: 1.3811078071594238, D Loss: 1.2166730165481567, alpha: 0.9950529277868578\n",
      "epoch 29 iteration700, G Loss: 1.9256525039672852, D Loss: 1.3610429763793945, alpha: 0.9950529277868578\n",
      "epoch 29 iteration800, G Loss: 1.2458515167236328, D Loss: 1.268749713897705, alpha: 0.9950529277868578\n",
      "Saving content.\n",
      "epoch 30 iteration0, G Loss: 1.0272761583328247, D Loss: 1.2441630363464355, alpha: 0.9949333706665338\n",
      "epoch 30 iteration100, G Loss: 1.5289992094039917, D Loss: 1.5671958923339844, alpha: 0.9949333706665338\n",
      "epoch 30 iteration200, G Loss: 0.757532000541687, D Loss: 1.4231947660446167, alpha: 0.9949333706665338\n",
      "epoch 30 iteration300, G Loss: 1.4115655422210693, D Loss: 1.1848151683807373, alpha: 0.9949333706665338\n",
      "epoch 30 iteration400, G Loss: 1.8559170961380005, D Loss: 1.4577062129974365, alpha: 0.9949333706665338\n",
      "epoch 30 iteration500, G Loss: 1.1094170808792114, D Loss: 1.212251901626587, alpha: 0.9949333706665338\n",
      "epoch 30 iteration600, G Loss: 1.434924840927124, D Loss: 1.2823749780654907, alpha: 0.9949333706665338\n",
      "epoch 30 iteration700, G Loss: 0.988045871257782, D Loss: 1.303936243057251, alpha: 0.9949333706665338\n",
      "epoch 30 iteration800, G Loss: 1.3986427783966064, D Loss: 1.6046209335327148, alpha: 0.9949333706665338\n",
      "Saving content.\n",
      "epoch 31 iteration0, G Loss: 1.0277174711227417, D Loss: 1.2914080619812012, alpha: 0.9948109392472895\n",
      "epoch 31 iteration100, G Loss: 0.7742493152618408, D Loss: 1.2304414510726929, alpha: 0.9948109392472895\n",
      "epoch 31 iteration200, G Loss: 0.9814537167549133, D Loss: 1.2667157649993896, alpha: 0.9948109392472895\n",
      "epoch 31 iteration300, G Loss: 0.9977844953536987, D Loss: 1.2899022102355957, alpha: 0.9948109392472895\n",
      "epoch 31 iteration400, G Loss: 0.8801940083503723, D Loss: 1.2935818433761597, alpha: 0.9948109392472895\n",
      "epoch 31 iteration500, G Loss: 1.2083683013916016, D Loss: 1.3397176265716553, alpha: 0.9948109392472895\n",
      "epoch 31 iteration600, G Loss: 1.562893033027649, D Loss: 1.3285915851593018, alpha: 0.9948109392472895\n",
      "epoch 31 iteration700, G Loss: 1.133011817932129, D Loss: 1.2101901769638062, alpha: 0.9948109392472895\n",
      "epoch 31 iteration800, G Loss: 1.021432876586914, D Loss: 1.1953582763671875, alpha: 0.9948109392472895\n",
      "Saving content.\n",
      "epoch 32 iteration0, G Loss: 1.0339245796203613, D Loss: 1.425634741783142, alpha: 0.9946855651643326\n",
      "epoch 32 iteration100, G Loss: 0.8571268320083618, D Loss: 1.2299085855484009, alpha: 0.9946855651643326\n",
      "epoch 32 iteration200, G Loss: 0.9431106448173523, D Loss: 1.2190589904785156, alpha: 0.9946855651643326\n",
      "epoch 32 iteration300, G Loss: 0.9132623672485352, D Loss: 1.2786202430725098, alpha: 0.9946855651643326\n",
      "epoch 32 iteration400, G Loss: 0.8744038343429565, D Loss: 1.4522886276245117, alpha: 0.9946855651643326\n",
      "epoch 32 iteration500, G Loss: 0.9914356470108032, D Loss: 1.1668469905853271, alpha: 0.9946855651643326\n",
      "epoch 32 iteration600, G Loss: 1.4149760007858276, D Loss: 1.3156875371932983, alpha: 0.9946855651643326\n",
      "epoch 32 iteration700, G Loss: 1.1749147176742554, D Loss: 1.1604948043823242, alpha: 0.9946855651643326\n",
      "epoch 32 iteration800, G Loss: 0.9673343300819397, D Loss: 1.1809234619140625, alpha: 0.9946855651643326\n",
      "Saving content.\n",
      "epoch 33 iteration0, G Loss: 0.961701512336731, D Loss: 1.196521282196045, alpha: 0.9945571784628505\n",
      "epoch 33 iteration100, G Loss: 1.1483694314956665, D Loss: 1.2504287958145142, alpha: 0.9945571784628505\n",
      "epoch 33 iteration200, G Loss: 0.9714611768722534, D Loss: 1.4976227283477783, alpha: 0.9945571784628505\n",
      "epoch 33 iteration300, G Loss: 1.3213096857070923, D Loss: 1.6190680265426636, alpha: 0.9945571784628505\n",
      "epoch 33 iteration400, G Loss: 1.3701518774032593, D Loss: 1.321958303451538, alpha: 0.9945571784628505\n",
      "epoch 33 iteration500, G Loss: 0.6817953586578369, D Loss: 1.3593132495880127, alpha: 0.9945571784628505\n",
      "epoch 33 iteration600, G Loss: 1.2163499593734741, D Loss: 1.1513662338256836, alpha: 0.9945571784628505\n",
      "epoch 33 iteration700, G Loss: 1.3667500019073486, D Loss: 1.1990265846252441, alpha: 0.9945571784628505\n",
      "epoch 33 iteration800, G Loss: 1.122646689414978, D Loss: 1.30153226852417, alpha: 0.9945571784628505\n",
      "Saving content.\n",
      "epoch 34 iteration0, G Loss: 1.0255857706069946, D Loss: 1.2750719785690308, alpha: 0.9944257075627952\n",
      "epoch 34 iteration100, G Loss: 0.9448638558387756, D Loss: 1.2370579242706299, alpha: 0.9944257075627952\n",
      "epoch 34 iteration200, G Loss: 1.0967639684677124, D Loss: 1.2806943655014038, alpha: 0.9944257075627952\n",
      "epoch 34 iteration300, G Loss: 1.3709590435028076, D Loss: 1.2662492990493774, alpha: 0.9944257075627952\n",
      "epoch 34 iteration400, G Loss: 1.3444359302520752, D Loss: 1.2896020412445068, alpha: 0.9944257075627952\n",
      "epoch 34 iteration500, G Loss: 1.3083661794662476, D Loss: 1.1372325420379639, alpha: 0.9944257075627952\n",
      "epoch 34 iteration600, G Loss: 1.0843572616577148, D Loss: 1.1500298976898193, alpha: 0.9944257075627952\n",
      "epoch 34 iteration700, G Loss: 1.0632745027542114, D Loss: 1.182586431503296, alpha: 0.9944257075627952\n",
      "epoch 34 iteration800, G Loss: 1.0535122156143188, D Loss: 1.3319108486175537, alpha: 0.9944257075627952\n",
      "Saving content.\n",
      "epoch 35 iteration0, G Loss: 0.6319251656532288, D Loss: 1.7073947191238403, alpha: 0.9942910792229767\n",
      "epoch 35 iteration100, G Loss: 0.9567801356315613, D Loss: 1.2125828266143799, alpha: 0.9942910792229767\n",
      "epoch 35 iteration200, G Loss: 0.8700047731399536, D Loss: 1.2843780517578125, alpha: 0.9942910792229767\n",
      "epoch 35 iteration300, G Loss: 1.3232219219207764, D Loss: 1.2058950662612915, alpha: 0.9942910792229767\n",
      "epoch 35 iteration400, G Loss: 0.7018523216247559, D Loss: 1.2697176933288574, alpha: 0.9942910792229767\n",
      "epoch 35 iteration500, G Loss: 0.8811452984809875, D Loss: 1.2421469688415527, alpha: 0.9942910792229767\n",
      "epoch 35 iteration600, G Loss: 1.218110203742981, D Loss: 1.3981773853302002, alpha: 0.9942910792229767\n",
      "epoch 35 iteration700, G Loss: 0.8610159158706665, D Loss: 1.4395606517791748, alpha: 0.9942910792229767\n",
      "epoch 35 iteration800, G Loss: 0.7770935297012329, D Loss: 1.3804677724838257, alpha: 0.9942910792229767\n",
      "Saving content.\n",
      "epoch 36 iteration0, G Loss: 1.6438883543014526, D Loss: 1.398288369178772, alpha: 0.9941532185044534\n",
      "epoch 36 iteration100, G Loss: 1.1036697626113892, D Loss: 1.0883549451828003, alpha: 0.9941532185044534\n",
      "epoch 36 iteration200, G Loss: 1.114628791809082, D Loss: 1.325095295906067, alpha: 0.9941532185044534\n",
      "epoch 36 iteration300, G Loss: 0.9254182577133179, D Loss: 1.1698040962219238, alpha: 0.9941532185044534\n",
      "epoch 36 iteration400, G Loss: 1.1458232402801514, D Loss: 1.1937038898468018, alpha: 0.9941532185044534\n",
      "epoch 36 iteration500, G Loss: 0.9191621541976929, D Loss: 1.3975709676742554, alpha: 0.9941532185044534\n",
      "epoch 36 iteration600, G Loss: 1.265429973602295, D Loss: 1.1558527946472168, alpha: 0.9941532185044534\n",
      "epoch 36 iteration700, G Loss: 0.8812775015830994, D Loss: 1.2944068908691406, alpha: 0.9941532185044534\n",
      "epoch 36 iteration800, G Loss: 0.8215449452400208, D Loss: 1.3341622352600098, alpha: 0.9941532185044534\n",
      "Saving content.\n",
      "epoch 37 iteration0, G Loss: 1.1230964660644531, D Loss: 1.3217675685882568, alpha: 0.9940120487332132\n",
      "epoch 37 iteration100, G Loss: 0.8361149430274963, D Loss: 1.315352201461792, alpha: 0.9940120487332132\n",
      "epoch 37 iteration200, G Loss: 1.0907762050628662, D Loss: 1.2295325994491577, alpha: 0.9940120487332132\n",
      "epoch 37 iteration300, G Loss: 1.07720148563385, D Loss: 1.2943265438079834, alpha: 0.9940120487332132\n",
      "epoch 37 iteration400, G Loss: 1.0077837705612183, D Loss: 1.320807933807373, alpha: 0.9940120487332132\n",
      "epoch 37 iteration500, G Loss: 1.1435364484786987, D Loss: 1.2564877271652222, alpha: 0.9940120487332132\n",
      "epoch 37 iteration600, G Loss: 0.8751308917999268, D Loss: 1.4883724451065063, alpha: 0.9940120487332132\n",
      "epoch 37 iteration700, G Loss: 0.996599018573761, D Loss: 1.2102608680725098, alpha: 0.9940120487332132\n",
      "epoch 37 iteration800, G Loss: 1.0389084815979004, D Loss: 1.1750454902648926, alpha: 0.9940120487332132\n",
      "Saving content.\n",
      "epoch 38 iteration0, G Loss: 0.9851268529891968, D Loss: 1.2260956764221191, alpha: 0.9938674914621343\n",
      "epoch 38 iteration100, G Loss: 1.3536641597747803, D Loss: 1.2240447998046875, alpha: 0.9938674914621343\n",
      "epoch 38 iteration200, G Loss: 1.4077770709991455, D Loss: 1.2073456048965454, alpha: 0.9938674914621343\n",
      "epoch 38 iteration300, G Loss: 0.5449074506759644, D Loss: 1.2325676679611206, alpha: 0.9938674914621343\n",
      "epoch 38 iteration400, G Loss: 1.604390263557434, D Loss: 1.2802081108093262, alpha: 0.9938674914621343\n",
      "epoch 38 iteration500, G Loss: 0.9432970285415649, D Loss: 1.3413735628128052, alpha: 0.9938674914621343\n",
      "epoch 38 iteration600, G Loss: 0.916576623916626, D Loss: 1.2536277770996094, alpha: 0.9938674914621343\n",
      "epoch 38 iteration700, G Loss: 0.9519490003585815, D Loss: 1.2487086057662964, alpha: 0.9938674914621343\n",
      "epoch 38 iteration800, G Loss: 0.7017938494682312, D Loss: 1.387746810913086, alpha: 0.9938674914621343\n",
      "Saving content.\n",
      "epoch 39 iteration0, G Loss: 0.8866996765136719, D Loss: 1.2845447063446045, alpha: 0.9937194664322172\n",
      "epoch 39 iteration100, G Loss: 1.2237989902496338, D Loss: 1.4595491886138916, alpha: 0.9937194664322172\n",
      "epoch 39 iteration200, G Loss: 2.049330949783325, D Loss: 1.565232276916504, alpha: 0.9937194664322172\n",
      "epoch 39 iteration300, G Loss: 1.4382315874099731, D Loss: 1.3774659633636475, alpha: 0.9937194664322172\n",
      "epoch 39 iteration400, G Loss: 1.74984872341156, D Loss: 1.3967150449752808, alpha: 0.9937194664322172\n",
      "epoch 39 iteration500, G Loss: 1.2468726634979248, D Loss: 1.3154460191726685, alpha: 0.9937194664322172\n",
      "epoch 39 iteration600, G Loss: 1.4487831592559814, D Loss: 1.187233805656433, alpha: 0.9937194664322172\n",
      "epoch 39 iteration700, G Loss: 0.8695743680000305, D Loss: 1.3241567611694336, alpha: 0.9937194664322172\n",
      "epoch 39 iteration800, G Loss: 0.8294026255607605, D Loss: 1.3713772296905518, alpha: 0.9937194664322172\n",
      "Saving content.\n",
      "epoch 40 iteration0, G Loss: 1.0212942361831665, D Loss: 1.247178316116333, alpha: 0.9935678915330813\n",
      "epoch 40 iteration100, G Loss: 1.1328601837158203, D Loss: 1.3035829067230225, alpha: 0.9935678915330813\n",
      "epoch 40 iteration200, G Loss: 0.9853143692016602, D Loss: 1.347688913345337, alpha: 0.9935678915330813\n",
      "epoch 40 iteration300, G Loss: 1.3535876274108887, D Loss: 1.225404977798462, alpha: 0.9935678915330813\n",
      "epoch 40 iteration400, G Loss: 1.2337695360183716, D Loss: 1.1559176445007324, alpha: 0.9935678915330813\n",
      "epoch 40 iteration500, G Loss: 1.0936338901519775, D Loss: 1.2352155447006226, alpha: 0.9935678915330813\n",
      "epoch 40 iteration600, G Loss: 1.0751386880874634, D Loss: 1.279620885848999, alpha: 0.9935678915330813\n",
      "epoch 40 iteration700, G Loss: 1.0777819156646729, D Loss: 1.2084718942642212, alpha: 0.9935678915330813\n",
      "epoch 40 iteration800, G Loss: 1.5188360214233398, D Loss: 1.3544907569885254, alpha: 0.9935678915330813\n",
      "Saving content.\n",
      "epoch 41 iteration0, G Loss: 1.358468770980835, D Loss: 1.3002967834472656, alpha: 0.9934126827627158\n",
      "epoch 41 iteration100, G Loss: 0.8420599102973938, D Loss: 1.2814031839370728, alpha: 0.9934126827627158\n",
      "epoch 41 iteration200, G Loss: 0.869408130645752, D Loss: 1.302748680114746, alpha: 0.9934126827627158\n",
      "epoch 41 iteration300, G Loss: 1.188976764678955, D Loss: 1.1642796993255615, alpha: 0.9934126827627158\n",
      "epoch 41 iteration400, G Loss: 1.1410109996795654, D Loss: 1.268559217453003, alpha: 0.9934126827627158\n",
      "epoch 41 iteration500, G Loss: 0.7629600763320923, D Loss: 1.1526819467544556, alpha: 0.9934126827627158\n",
      "epoch 41 iteration600, G Loss: 0.8909904360771179, D Loss: 1.2972190380096436, alpha: 0.9934126827627158\n",
      "epoch 41 iteration700, G Loss: 0.9352326989173889, D Loss: 1.2535886764526367, alpha: 0.9934126827627158\n",
      "epoch 41 iteration800, G Loss: 1.154011845588684, D Loss: 1.132785677909851, alpha: 0.9934126827627158\n",
      "Saving content.\n",
      "epoch 42 iteration0, G Loss: 0.7950494289398193, D Loss: 1.2335307598114014, alpha: 0.9932537541864765\n",
      "epoch 42 iteration100, G Loss: 1.3501743078231812, D Loss: 1.251633644104004, alpha: 0.9932537541864765\n",
      "epoch 42 iteration200, G Loss: 1.5906296968460083, D Loss: 1.4142976999282837, alpha: 0.9932537541864765\n",
      "epoch 42 iteration300, G Loss: 1.049767017364502, D Loss: 1.2528955936431885, alpha: 0.9932537541864765\n",
      "epoch 42 iteration400, G Loss: 1.0188297033309937, D Loss: 1.2173629999160767, alpha: 0.9932537541864765\n",
      "epoch 42 iteration500, G Loss: 1.154845952987671, D Loss: 1.2140002250671387, alpha: 0.9932537541864765\n",
      "epoch 42 iteration600, G Loss: 1.6302639245986938, D Loss: 1.1930025815963745, alpha: 0.9932537541864765\n",
      "epoch 42 iteration700, G Loss: 0.9673627614974976, D Loss: 1.2501271963119507, alpha: 0.9932537541864765\n",
      "epoch 42 iteration800, G Loss: 1.4879683256149292, D Loss: 1.4647367000579834, alpha: 0.9932537541864765\n",
      "Saving content.\n",
      "epoch 43 iteration0, G Loss: 1.1714835166931152, D Loss: 1.7675697803497314, alpha: 0.9930910178953235\n",
      "epoch 43 iteration100, G Loss: 0.9679611325263977, D Loss: 1.1991883516311646, alpha: 0.9930910178953235\n",
      "epoch 43 iteration200, G Loss: 0.8609396815299988, D Loss: 1.2151176929473877, alpha: 0.9930910178953235\n",
      "epoch 43 iteration300, G Loss: 0.7738031148910522, D Loss: 1.322392463684082, alpha: 0.9930910178953235\n",
      "epoch 43 iteration400, G Loss: 0.7470171451568604, D Loss: 1.2792582511901855, alpha: 0.9930910178953235\n",
      "epoch 43 iteration500, G Loss: 0.7067052125930786, D Loss: 1.214669108390808, alpha: 0.9930910178953235\n",
      "epoch 43 iteration600, G Loss: 1.0822597742080688, D Loss: 1.214548945426941, alpha: 0.9930910178953235\n",
      "epoch 43 iteration700, G Loss: 1.147443413734436, D Loss: 1.265742301940918, alpha: 0.9930910178953235\n",
      "epoch 43 iteration800, G Loss: 1.4661884307861328, D Loss: 1.2836735248565674, alpha: 0.9930910178953235\n",
      "Saving content.\n",
      "epoch 44 iteration0, G Loss: 0.8983502388000488, D Loss: 1.3121508359909058, alpha: 0.9929243839632887\n",
      "epoch 44 iteration100, G Loss: 0.811877965927124, D Loss: 1.239253044128418, alpha: 0.9929243839632887\n",
      "epoch 44 iteration200, G Loss: 0.8724472522735596, D Loss: 1.1800166368484497, alpha: 0.9929243839632887\n",
      "epoch 44 iteration300, G Loss: 1.0720162391662598, D Loss: 1.2850630283355713, alpha: 0.9929243839632887\n",
      "epoch 44 iteration400, G Loss: 1.0186699628829956, D Loss: 1.164040446281433, alpha: 0.9929243839632887\n",
      "epoch 44 iteration500, G Loss: 0.9636716842651367, D Loss: 1.270458698272705, alpha: 0.9929243839632887\n",
      "epoch 44 iteration600, G Loss: 1.4534505605697632, D Loss: 1.2775849103927612, alpha: 0.9929243839632887\n",
      "epoch 44 iteration700, G Loss: 1.1401269435882568, D Loss: 1.292746901512146, alpha: 0.9929243839632887\n",
      "epoch 44 iteration800, G Loss: 0.6562581062316895, D Loss: 1.3225882053375244, alpha: 0.9929243839632887\n",
      "Saving content.\n",
      "epoch 45 iteration0, G Loss: 1.0408542156219482, D Loss: 1.1854256391525269, alpha: 0.9927537604041685\n",
      "epoch 45 iteration100, G Loss: 0.9586410522460938, D Loss: 1.240023136138916, alpha: 0.9927537604041685\n",
      "epoch 45 iteration200, G Loss: 0.471360981464386, D Loss: 1.3161650896072388, alpha: 0.9927537604041685\n",
      "epoch 45 iteration300, G Loss: 2.5783231258392334, D Loss: 1.4694689512252808, alpha: 0.9927537604041685\n",
      "epoch 45 iteration400, G Loss: 0.8606597781181335, D Loss: 1.2322264909744263, alpha: 0.9927537604041685\n",
      "epoch 45 iteration500, G Loss: 0.9525274634361267, D Loss: 1.2801542282104492, alpha: 0.9927537604041685\n",
      "epoch 45 iteration600, G Loss: 1.2570717334747314, D Loss: 1.2437477111816406, alpha: 0.9927537604041685\n",
      "epoch 45 iteration700, G Loss: 0.947697639465332, D Loss: 1.26014244556427, alpha: 0.9927537604041685\n",
      "epoch 45 iteration800, G Loss: 0.9462472200393677, D Loss: 1.4951450824737549, alpha: 0.9927537604041685\n",
      "Saving content.\n",
      "epoch 46 iteration0, G Loss: 1.206351637840271, D Loss: 1.2559521198272705, alpha: 0.9925790531274342\n",
      "epoch 46 iteration100, G Loss: 1.1686017513275146, D Loss: 1.2272841930389404, alpha: 0.9925790531274342\n",
      "epoch 46 iteration200, G Loss: 0.9284705519676208, D Loss: 1.2352641820907593, alpha: 0.9925790531274342\n",
      "epoch 46 iteration300, G Loss: 1.0095584392547607, D Loss: 1.1885789632797241, alpha: 0.9925790531274342\n",
      "epoch 46 iteration400, G Loss: 0.9577315449714661, D Loss: 1.16436767578125, alpha: 0.9925790531274342\n",
      "epoch 46 iteration500, G Loss: 1.6644035577774048, D Loss: 1.3661999702453613, alpha: 0.9925790531274342\n",
      "epoch 46 iteration600, G Loss: 1.2097607851028442, D Loss: 1.2498046159744263, alpha: 0.9925790531274342\n",
      "epoch 46 iteration700, G Loss: 0.9532344341278076, D Loss: 1.3275344371795654, alpha: 0.9925790531274342\n",
      "epoch 46 iteration800, G Loss: 0.9388969540596008, D Loss: 1.2761508226394653, alpha: 0.9925790531274342\n",
      "Saving content.\n",
      "epoch 47 iteration0, G Loss: 1.0195043087005615, D Loss: 1.3115886449813843, alpha: 0.9924001658933519\n",
      "epoch 47 iteration100, G Loss: 0.7207112312316895, D Loss: 1.3674956560134888, alpha: 0.9924001658933519\n",
      "epoch 47 iteration200, G Loss: 1.1358239650726318, D Loss: 1.3557322025299072, alpha: 0.9924001658933519\n",
      "epoch 47 iteration300, G Loss: 1.362850308418274, D Loss: 1.4205827713012695, alpha: 0.9924001658933519\n",
      "epoch 47 iteration400, G Loss: 1.0252448320388794, D Loss: 1.2083396911621094, alpha: 0.9924001658933519\n",
      "epoch 47 iteration500, G Loss: 0.9328477382659912, D Loss: 1.2277355194091797, alpha: 0.9924001658933519\n",
      "epoch 47 iteration600, G Loss: 0.9463558793067932, D Loss: 1.2374540567398071, alpha: 0.9924001658933519\n",
      "epoch 47 iteration700, G Loss: 1.6057208776474, D Loss: 1.366126298904419, alpha: 0.9924001658933519\n",
      "epoch 47 iteration800, G Loss: 0.8429253101348877, D Loss: 1.284381628036499, alpha: 0.9924001658933519\n",
      "Saving content.\n",
      "epoch 48 iteration0, G Loss: 1.0868463516235352, D Loss: 1.1534062623977661, alpha: 0.9922170002673092\n",
      "epoch 48 iteration100, G Loss: 1.1010570526123047, D Loss: 1.250239610671997, alpha: 0.9922170002673092\n",
      "epoch 48 iteration200, G Loss: 1.1340010166168213, D Loss: 1.2331771850585938, alpha: 0.9922170002673092\n",
      "epoch 48 iteration300, G Loss: 0.8115170001983643, D Loss: 1.2266565561294556, alpha: 0.9922170002673092\n",
      "epoch 48 iteration400, G Loss: 0.6682738065719604, D Loss: 1.1636680364608765, alpha: 0.9922170002673092\n",
      "epoch 48 iteration500, G Loss: 1.6667999029159546, D Loss: 1.1537657976150513, alpha: 0.9922170002673092\n",
      "epoch 48 iteration600, G Loss: 1.09206223487854, D Loss: 1.1475390195846558, alpha: 0.9922170002673092\n",
      "epoch 48 iteration700, G Loss: 0.8822875022888184, D Loss: 1.248104453086853, alpha: 0.9922170002673092\n",
      "epoch 48 iteration800, G Loss: 0.9918659925460815, D Loss: 1.389634370803833, alpha: 0.9922170002673092\n",
      "Saving content.\n",
      "epoch 49 iteration0, G Loss: 1.170626163482666, D Loss: 1.1954602003097534, alpha: 0.9920294555733393\n",
      "epoch 49 iteration100, G Loss: 1.1561366319656372, D Loss: 1.405815601348877, alpha: 0.9920294555733393\n",
      "epoch 49 iteration200, G Loss: 0.8134227991104126, D Loss: 1.2976486682891846, alpha: 0.9920294555733393\n",
      "epoch 49 iteration300, G Loss: 0.8457812070846558, D Loss: 1.4737566709518433, alpha: 0.9920294555733393\n",
      "epoch 49 iteration400, G Loss: 0.9681451916694641, D Loss: 1.2341668605804443, alpha: 0.9920294555733393\n",
      "epoch 49 iteration500, G Loss: 1.074305772781372, D Loss: 1.1918965578079224, alpha: 0.9920294555733393\n",
      "epoch 49 iteration600, G Loss: 1.0022553205490112, D Loss: 1.265737771987915, alpha: 0.9920294555733393\n",
      "epoch 49 iteration700, G Loss: 1.3860621452331543, D Loss: 1.2172083854675293, alpha: 0.9920294555733393\n",
      "epoch 49 iteration800, G Loss: 0.5811409950256348, D Loss: 1.3649754524230957, alpha: 0.9920294555733393\n",
      "Saving content.\n",
      "epoch 50 iteration0, G Loss: 1.138877034187317, D Loss: 1.3070378303527832, alpha: 0.9918374288468401\n",
      "epoch 50 iteration100, G Loss: 1.2250359058380127, D Loss: 1.1530390977859497, alpha: 0.9918374288468401\n",
      "epoch 50 iteration200, G Loss: 0.9243274331092834, D Loss: 1.2964173555374146, alpha: 0.9918374288468401\n",
      "epoch 50 iteration300, G Loss: 0.9210267663002014, D Loss: 1.251871943473816, alpha: 0.9918374288468401\n",
      "epoch 50 iteration400, G Loss: 1.5287249088287354, D Loss: 1.311668038368225, alpha: 0.9918374288468401\n",
      "epoch 50 iteration500, G Loss: 0.6027388572692871, D Loss: 1.2385143041610718, alpha: 0.9918374288468401\n",
      "epoch 50 iteration600, G Loss: 1.0381847620010376, D Loss: 1.315178632736206, alpha: 0.9918374288468401\n",
      "epoch 50 iteration700, G Loss: 0.928453803062439, D Loss: 1.363171100616455, alpha: 0.9918374288468401\n",
      "epoch 50 iteration800, G Loss: 1.1592614650726318, D Loss: 1.3567055463790894, alpha: 0.9918374288468401\n",
      "Saving content.\n",
      "epoch 51 iteration0, G Loss: 1.072544813156128, D Loss: 1.271022915840149, alpha: 0.9916408147864824\n",
      "epoch 51 iteration100, G Loss: 0.9715380668640137, D Loss: 1.1334469318389893, alpha: 0.9916408147864824\n",
      "epoch 51 iteration200, G Loss: 1.4644148349761963, D Loss: 1.217099905014038, alpha: 0.9916408147864824\n",
      "epoch 51 iteration300, G Loss: 1.0030604600906372, D Loss: 1.2058279514312744, alpha: 0.9916408147864824\n",
      "epoch 51 iteration400, G Loss: 1.0696649551391602, D Loss: 1.1782244443893433, alpha: 0.9916408147864824\n",
      "epoch 51 iteration500, G Loss: 1.0667338371276855, D Loss: 1.22849702835083, alpha: 0.9916408147864824\n",
      "epoch 51 iteration600, G Loss: 1.0373448133468628, D Loss: 1.170591115951538, alpha: 0.9916408147864824\n",
      "epoch 51 iteration700, G Loss: 0.7057539820671082, D Loss: 1.2565348148345947, alpha: 0.9916408147864824\n",
      "epoch 51 iteration800, G Loss: 1.0616133213043213, D Loss: 1.2884283065795898, alpha: 0.9916408147864824\n",
      "Saving content.\n",
      "epoch 52 iteration0, G Loss: 0.7301979660987854, D Loss: 1.2701280117034912, alpha: 0.9914395057053028\n",
      "epoch 52 iteration100, G Loss: 0.8540096879005432, D Loss: 1.258020281791687, alpha: 0.9914395057053028\n",
      "epoch 52 iteration200, G Loss: 0.744022011756897, D Loss: 1.3062849044799805, alpha: 0.9914395057053028\n",
      "epoch 52 iteration300, G Loss: 0.8144052028656006, D Loss: 1.3389644622802734, alpha: 0.9914395057053028\n",
      "epoch 52 iteration400, G Loss: 1.600899577140808, D Loss: 1.2479580640792847, alpha: 0.9914395057053028\n",
      "epoch 52 iteration500, G Loss: 1.4701147079467773, D Loss: 1.2336442470550537, alpha: 0.9914395057053028\n",
      "epoch 52 iteration600, G Loss: 1.435266137123108, D Loss: 1.2986576557159424, alpha: 0.9914395057053028\n",
      "epoch 52 iteration700, G Loss: 0.8825960755348206, D Loss: 1.2352927923202515, alpha: 0.9914395057053028\n",
      "epoch 52 iteration800, G Loss: 0.8666093349456787, D Loss: 1.219988226890564, alpha: 0.9914395057053028\n",
      "Saving content.\n",
      "epoch 53 iteration0, G Loss: 1.30209481716156, D Loss: 1.574120044708252, alpha: 0.9912333914809791\n",
      "epoch 53 iteration100, G Loss: 1.084337830543518, D Loss: 1.3104891777038574, alpha: 0.9912333914809791\n",
      "epoch 53 iteration200, G Loss: 1.1309466361999512, D Loss: 1.121535301208496, alpha: 0.9912333914809791\n",
      "epoch 53 iteration300, G Loss: 1.028216004371643, D Loss: 1.454622507095337, alpha: 0.9912333914809791\n",
      "epoch 53 iteration400, G Loss: 1.1547372341156006, D Loss: 1.3227522373199463, alpha: 0.9912333914809791\n",
      "epoch 53 iteration500, G Loss: 0.901756763458252, D Loss: 1.1667945384979248, alpha: 0.9912333914809791\n",
      "epoch 53 iteration600, G Loss: 1.175437569618225, D Loss: 1.2323017120361328, alpha: 0.9912333914809791\n",
      "epoch 53 iteration700, G Loss: 0.8068029284477234, D Loss: 1.2304937839508057, alpha: 0.9912333914809791\n",
      "epoch 53 iteration800, G Loss: 1.3757497072219849, D Loss: 1.4701025485992432, alpha: 0.9912333914809791\n",
      "Saving content.\n",
      "epoch 54 iteration0, G Loss: 1.341659665107727, D Loss: 1.5517847537994385, alpha: 0.9910223595052832\n",
      "epoch 54 iteration100, G Loss: 1.2006943225860596, D Loss: 1.1406335830688477, alpha: 0.9910223595052832\n",
      "epoch 54 iteration200, G Loss: 1.326462745666504, D Loss: 1.5403320789337158, alpha: 0.9910223595052832\n",
      "epoch 54 iteration300, G Loss: 1.0099272727966309, D Loss: 1.2110272645950317, alpha: 0.9910223595052832\n",
      "epoch 54 iteration400, G Loss: 0.9331897497177124, D Loss: 1.335218071937561, alpha: 0.9910223595052832\n",
      "epoch 54 iteration500, G Loss: 0.9458177089691162, D Loss: 1.2426390647888184, alpha: 0.9910223595052832\n",
      "epoch 54 iteration600, G Loss: 0.8793936967849731, D Loss: 1.2731571197509766, alpha: 0.9910223595052832\n",
      "epoch 54 iteration700, G Loss: 0.9707854390144348, D Loss: 1.2799837589263916, alpha: 0.9910223595052832\n",
      "epoch 54 iteration800, G Loss: 1.2965960502624512, D Loss: 1.3147907257080078, alpha: 0.9910223595052832\n",
      "Saving content.\n",
      "epoch 55 iteration0, G Loss: 0.9932435750961304, D Loss: 1.3022276163101196, alpha: 0.9908062946327119\n",
      "epoch 55 iteration100, G Loss: 1.0377871990203857, D Loss: 1.3109084367752075, alpha: 0.9908062946327119\n",
      "epoch 55 iteration200, G Loss: 1.433005452156067, D Loss: 1.2414886951446533, alpha: 0.9908062946327119\n",
      "epoch 55 iteration300, G Loss: 0.6580671072006226, D Loss: 1.169091820716858, alpha: 0.9908062946327119\n",
      "epoch 55 iteration400, G Loss: 1.2190630435943604, D Loss: 1.1415207386016846, alpha: 0.9908062946327119\n",
      "epoch 55 iteration500, G Loss: 1.3327975273132324, D Loss: 1.4340617656707764, alpha: 0.9908062946327119\n",
      "epoch 55 iteration600, G Loss: 1.027124285697937, D Loss: 1.2247517108917236, alpha: 0.9908062946327119\n",
      "epoch 55 iteration700, G Loss: 1.3198866844177246, D Loss: 1.3056102991104126, alpha: 0.9908062946327119\n",
      "epoch 55 iteration800, G Loss: 0.9125561714172363, D Loss: 1.3915331363677979, alpha: 0.9908062946327119\n",
      "Saving content.\n",
      "epoch 56 iteration0, G Loss: 0.6699239015579224, D Loss: 1.2557027339935303, alpha: 0.9905850791282914\n",
      "epoch 56 iteration100, G Loss: 0.7709847092628479, D Loss: 1.2616018056869507, alpha: 0.9905850791282914\n",
      "epoch 56 iteration200, G Loss: 0.8960142135620117, D Loss: 1.455873966217041, alpha: 0.9905850791282914\n",
      "epoch 56 iteration300, G Loss: 0.9186400175094604, D Loss: 1.1668856143951416, alpha: 0.9905850791282914\n",
      "epoch 56 iteration400, G Loss: 1.0546940565109253, D Loss: 1.1968759298324585, alpha: 0.9905850791282914\n",
      "epoch 56 iteration500, G Loss: 0.872933030128479, D Loss: 1.336220622062683, alpha: 0.9905850791282914\n",
      "epoch 56 iteration600, G Loss: 0.9360455274581909, D Loss: 1.3528374433517456, alpha: 0.9905850791282914\n",
      "epoch 56 iteration700, G Loss: 0.8465960621833801, D Loss: 1.201664686203003, alpha: 0.9905850791282914\n",
      "epoch 56 iteration800, G Loss: 1.0169411897659302, D Loss: 1.279093861579895, alpha: 0.9905850791282914\n",
      "Saving content.\n",
      "epoch 57 iteration0, G Loss: 0.7449378967285156, D Loss: 1.2280383110046387, alpha: 0.9903585926145569\n",
      "epoch 57 iteration100, G Loss: 0.9689897298812866, D Loss: 1.269734263420105, alpha: 0.9903585926145569\n",
      "epoch 57 iteration200, G Loss: 0.6537166237831116, D Loss: 1.3353246450424194, alpha: 0.9903585926145569\n",
      "epoch 57 iteration300, G Loss: 0.8763893842697144, D Loss: 1.2332594394683838, alpha: 0.9903585926145569\n",
      "epoch 57 iteration400, G Loss: 0.9703885912895203, D Loss: 1.218870997428894, alpha: 0.9903585926145569\n",
      "epoch 57 iteration500, G Loss: 0.9338095188140869, D Loss: 1.1864540576934814, alpha: 0.9903585926145569\n",
      "epoch 57 iteration600, G Loss: 1.165400505065918, D Loss: 1.2640178203582764, alpha: 0.9903585926145569\n",
      "epoch 57 iteration700, G Loss: 1.1617398262023926, D Loss: 1.1866780519485474, alpha: 0.9903585926145569\n",
      "epoch 57 iteration800, G Loss: 0.7973684072494507, D Loss: 1.2452332973480225, alpha: 0.9903585926145569\n",
      "Saving content.\n",
      "epoch 58 iteration0, G Loss: 1.2159442901611328, D Loss: 1.1524688005447388, alpha: 0.990126712017706\n",
      "epoch 58 iteration100, G Loss: 0.9571040272712708, D Loss: 1.2168824672698975, alpha: 0.990126712017706\n",
      "epoch 58 iteration200, G Loss: 0.8760806322097778, D Loss: 1.243814468383789, alpha: 0.990126712017706\n",
      "epoch 58 iteration300, G Loss: 1.4234296083450317, D Loss: 1.4509906768798828, alpha: 0.990126712017706\n",
      "epoch 58 iteration400, G Loss: 0.9435199499130249, D Loss: 1.4180907011032104, alpha: 0.990126712017706\n",
      "epoch 58 iteration500, G Loss: 1.4264663457870483, D Loss: 1.3641462326049805, alpha: 0.990126712017706\n",
      "epoch 58 iteration600, G Loss: 0.7589643001556396, D Loss: 1.4676886796951294, alpha: 0.990126712017706\n",
      "epoch 58 iteration700, G Loss: 1.0375219583511353, D Loss: 1.1483365297317505, alpha: 0.990126712017706\n",
      "epoch 58 iteration800, G Loss: 0.8119096159934998, D Loss: 1.313530683517456, alpha: 0.990126712017706\n",
      "Saving content.\n",
      "epoch 59 iteration0, G Loss: 1.0375139713287354, D Loss: 1.267715573310852, alpha: 0.9898893115129276\n",
      "epoch 59 iteration100, G Loss: 1.413442611694336, D Loss: 1.2145986557006836, alpha: 0.9898893115129276\n",
      "epoch 59 iteration200, G Loss: 0.9408884048461914, D Loss: 1.2065459489822388, alpha: 0.9898893115129276\n",
      "epoch 59 iteration300, G Loss: 0.39583563804626465, D Loss: 1.6306967735290527, alpha: 0.9898893115129276\n",
      "epoch 59 iteration400, G Loss: 1.036622405052185, D Loss: 1.0753452777862549, alpha: 0.9898893115129276\n",
      "epoch 59 iteration500, G Loss: 1.0617759227752686, D Loss: 1.1845076084136963, alpha: 0.9898893115129276\n",
      "epoch 59 iteration600, G Loss: 0.7212432622909546, D Loss: 1.244170904159546, alpha: 0.9898893115129276\n",
      "epoch 59 iteration700, G Loss: 0.6533244848251343, D Loss: 1.328127145767212, alpha: 0.9898893115129276\n",
      "epoch 59 iteration800, G Loss: 1.0222563743591309, D Loss: 1.2126117944717407, alpha: 0.9898893115129276\n",
      "Saving content.\n",
      "epoch 60 iteration0, G Loss: 1.083508849143982, D Loss: 1.1557154655456543, alpha: 0.9896462624689083\n",
      "epoch 60 iteration100, G Loss: 1.3036216497421265, D Loss: 1.3248445987701416, alpha: 0.9896462624689083\n",
      "epoch 60 iteration200, G Loss: 0.8816914558410645, D Loss: 1.5124419927597046, alpha: 0.9896462624689083\n",
      "epoch 60 iteration300, G Loss: 0.9848077893257141, D Loss: 1.2283960580825806, alpha: 0.9896462624689083\n",
      "epoch 60 iteration400, G Loss: 0.9358158707618713, D Loss: 1.340275526046753, alpha: 0.9896462624689083\n",
      "epoch 60 iteration500, G Loss: 2.0149786472320557, D Loss: 1.1240428686141968, alpha: 0.9896462624689083\n",
      "epoch 60 iteration600, G Loss: 1.1124351024627686, D Loss: 1.2062240839004517, alpha: 0.9896462624689083\n",
      "epoch 60 iteration700, G Loss: 1.6010785102844238, D Loss: 1.1856293678283691, alpha: 0.9896462624689083\n",
      "epoch 60 iteration800, G Loss: 1.1716690063476562, D Loss: 1.3254287242889404, alpha: 0.9896462624689083\n",
      "Saving content.\n",
      "epoch 61 iteration0, G Loss: 1.2411181926727295, D Loss: 1.2668063640594482, alpha: 0.9893974333915181\n",
      "epoch 61 iteration100, G Loss: 0.8287688493728638, D Loss: 1.2387497425079346, alpha: 0.9893974333915181\n",
      "epoch 61 iteration200, G Loss: 1.4231162071228027, D Loss: 1.1722753047943115, alpha: 0.9893974333915181\n",
      "epoch 61 iteration300, G Loss: 1.199171781539917, D Loss: 1.219869613647461, alpha: 0.9893974333915181\n",
      "epoch 61 iteration400, G Loss: 0.8636513948440552, D Loss: 1.3242501020431519, alpha: 0.9893974333915181\n",
      "epoch 61 iteration500, G Loss: 1.0021038055419922, D Loss: 1.170701503753662, alpha: 0.9893974333915181\n",
      "epoch 61 iteration600, G Loss: 1.1371407508850098, D Loss: 1.2847411632537842, alpha: 0.9893974333915181\n",
      "epoch 61 iteration700, G Loss: 1.2119790315628052, D Loss: 1.274315357208252, alpha: 0.9893974333915181\n",
      "epoch 61 iteration800, G Loss: 1.0623493194580078, D Loss: 1.3212182521820068, alpha: 0.9893974333915181\n",
      "Saving content.\n",
      "epoch 62 iteration0, G Loss: 0.7344589233398438, D Loss: 1.5067650079727173, alpha: 0.9891426898666814\n",
      "epoch 62 iteration100, G Loss: 0.9572436809539795, D Loss: 1.2812364101409912, alpha: 0.9891426898666814\n",
      "epoch 62 iteration200, G Loss: 1.0689750909805298, D Loss: 1.1748061180114746, alpha: 0.9891426898666814\n",
      "epoch 62 iteration300, G Loss: 0.9029708504676819, D Loss: 1.2534947395324707, alpha: 0.9891426898666814\n",
      "epoch 62 iteration400, G Loss: 1.1609290838241577, D Loss: 1.2391093969345093, alpha: 0.9891426898666814\n",
      "epoch 62 iteration500, G Loss: 1.1283220052719116, D Loss: 1.1914870738983154, alpha: 0.9891426898666814\n",
      "epoch 62 iteration600, G Loss: 0.7872764468193054, D Loss: 1.1442806720733643, alpha: 0.9891426898666814\n",
      "epoch 62 iteration700, G Loss: 1.6385637521743774, D Loss: 1.2369935512542725, alpha: 0.9891426898666814\n",
      "epoch 62 iteration800, G Loss: 0.8410533666610718, D Loss: 1.2092076539993286, alpha: 0.9891426898666814\n",
      "Saving content.\n",
      "epoch 63 iteration0, G Loss: 0.9589436054229736, D Loss: 1.273229718208313, alpha: 0.9888818945024357\n",
      "epoch 63 iteration100, G Loss: 0.9727795124053955, D Loss: 1.171674132347107, alpha: 0.9888818945024357\n",
      "epoch 63 iteration200, G Loss: 1.5210151672363281, D Loss: 1.7422839403152466, alpha: 0.9888818945024357\n",
      "epoch 63 iteration300, G Loss: 1.007033109664917, D Loss: 1.211259365081787, alpha: 0.9888818945024357\n",
      "epoch 63 iteration400, G Loss: 0.5342644453048706, D Loss: 1.2690201997756958, alpha: 0.9888818945024357\n",
      "epoch 63 iteration500, G Loss: 1.0079548358917236, D Loss: 1.2851992845535278, alpha: 0.9888818945024357\n",
      "epoch 63 iteration600, G Loss: 1.2181577682495117, D Loss: 1.1261168718338013, alpha: 0.9888818945024357\n",
      "epoch 63 iteration700, G Loss: 0.8474245071411133, D Loss: 1.378263235092163, alpha: 0.9888818945024357\n",
      "epoch 63 iteration800, G Loss: 0.7942647933959961, D Loss: 1.375614881515503, alpha: 0.9888818945024357\n",
      "Saving content.\n",
      "epoch 64 iteration0, G Loss: 1.0584429502487183, D Loss: 1.238830327987671, alpha: 0.9886149068701868\n",
      "epoch 64 iteration100, G Loss: 1.1739338636398315, D Loss: 1.3375380039215088, alpha: 0.9886149068701868\n",
      "epoch 64 iteration200, G Loss: 1.0134285688400269, D Loss: 1.2104744911193848, alpha: 0.9886149068701868\n",
      "epoch 64 iteration300, G Loss: 0.9883290529251099, D Loss: 1.3497154712677002, alpha: 0.9886149068701868\n",
      "epoch 64 iteration400, G Loss: 0.9692128896713257, D Loss: 1.2058820724487305, alpha: 0.9886149068701868\n",
      "epoch 64 iteration500, G Loss: 1.0786322355270386, D Loss: 1.2184782028198242, alpha: 0.9886149068701868\n",
      "epoch 64 iteration600, G Loss: 0.8235332369804382, D Loss: 1.4029170274734497, alpha: 0.9886149068701868\n",
      "epoch 64 iteration700, G Loss: 1.29908287525177, D Loss: 1.2953176498413086, alpha: 0.9886149068701868\n",
      "epoch 64 iteration800, G Loss: 1.2421432733535767, D Loss: 1.3600109815597534, alpha: 0.9886149068701868\n",
      "Saving content.\n",
      "epoch 65 iteration0, G Loss: 1.0987887382507324, D Loss: 1.1857279539108276, alpha: 0.9883415834451669\n",
      "epoch 65 iteration100, G Loss: 1.2610580921173096, D Loss: 1.287583589553833, alpha: 0.9883415834451669\n",
      "epoch 65 iteration200, G Loss: 0.9753050804138184, D Loss: 1.2184927463531494, alpha: 0.9883415834451669\n",
      "epoch 65 iteration300, G Loss: 1.2171316146850586, D Loss: 1.2421786785125732, alpha: 0.9883415834451669\n",
      "epoch 65 iteration400, G Loss: 0.6671954393386841, D Loss: 1.3101606369018555, alpha: 0.9883415834451669\n",
      "epoch 65 iteration500, G Loss: 0.9247307777404785, D Loss: 1.2678139209747314, alpha: 0.9883415834451669\n",
      "epoch 65 iteration600, G Loss: 1.4296324253082275, D Loss: 1.2033429145812988, alpha: 0.9883415834451669\n",
      "epoch 65 iteration700, G Loss: 0.8452533483505249, D Loss: 1.223841667175293, alpha: 0.9883415834451669\n",
      "epoch 65 iteration800, G Loss: 1.0575265884399414, D Loss: 1.2768219709396362, alpha: 0.9883415834451669\n",
      "Saving content.\n",
      "epoch 66 iteration0, G Loss: 0.8226062655448914, D Loss: 1.3076180219650269, alpha: 0.9880617775461034\n",
      "epoch 66 iteration100, G Loss: 0.7070527672767639, D Loss: 1.6331589221954346, alpha: 0.9880617775461034\n",
      "epoch 66 iteration200, G Loss: 0.794113278388977, D Loss: 1.517144799232483, alpha: 0.9880617775461034\n",
      "epoch 66 iteration300, G Loss: 0.8128289580345154, D Loss: 1.2878501415252686, alpha: 0.9880617775461034\n",
      "epoch 66 iteration400, G Loss: 1.2136235237121582, D Loss: 1.3711934089660645, alpha: 0.9880617775461034\n",
      "epoch 66 iteration500, G Loss: 0.8291664123535156, D Loss: 1.4882681369781494, alpha: 0.9880617775461034\n",
      "epoch 66 iteration600, G Loss: 1.3291523456573486, D Loss: 1.2286717891693115, alpha: 0.9880617775461034\n",
      "epoch 66 iteration700, G Loss: 1.07069730758667, D Loss: 1.1976796388626099, alpha: 0.9880617775461034\n",
      "epoch 66 iteration800, G Loss: 0.7407130599021912, D Loss: 1.6601693630218506, alpha: 0.9880617775461034\n",
      "Saving content.\n",
      "epoch 67 iteration0, G Loss: 0.807374894618988, D Loss: 1.2438945770263672, alpha: 0.9877753392741111\n",
      "epoch 67 iteration100, G Loss: 0.9235644340515137, D Loss: 1.3169301748275757, alpha: 0.9877753392741111\n",
      "epoch 67 iteration200, G Loss: 0.7599682807922363, D Loss: 1.2705687284469604, alpha: 0.9877753392741111\n",
      "epoch 67 iteration300, G Loss: 0.7939803004264832, D Loss: 1.3461989164352417, alpha: 0.9877753392741111\n",
      "epoch 67 iteration400, G Loss: 0.9308438301086426, D Loss: 1.236722707748413, alpha: 0.9877753392741111\n",
      "epoch 67 iteration500, G Loss: 0.823511004447937, D Loss: 1.9518988132476807, alpha: 0.9877753392741111\n",
      "epoch 67 iteration600, G Loss: 0.7084224224090576, D Loss: 1.3977680206298828, alpha: 0.9877753392741111\n",
      "epoch 67 iteration700, G Loss: 1.3458762168884277, D Loss: 1.198676347732544, alpha: 0.9877753392741111\n",
      "epoch 67 iteration800, G Loss: 0.8844188451766968, D Loss: 1.3703570365905762, alpha: 0.9877753392741111\n",
      "Saving content.\n",
      "epoch 68 iteration0, G Loss: 0.8360823392868042, D Loss: 1.1839449405670166, alpha: 0.9874821154508174\n",
      "epoch 68 iteration100, G Loss: 0.9187458753585815, D Loss: 1.1555943489074707, alpha: 0.9874821154508174\n",
      "epoch 68 iteration200, G Loss: 0.7592162489891052, D Loss: 1.308624267578125, alpha: 0.9874821154508174\n",
      "epoch 68 iteration300, G Loss: 1.2089903354644775, D Loss: 1.3455787897109985, alpha: 0.9874821154508174\n",
      "epoch 68 iteration400, G Loss: 1.014976978302002, D Loss: 1.2179046869277954, alpha: 0.9874821154508174\n",
      "epoch 68 iteration500, G Loss: 0.8829818964004517, D Loss: 1.2525359392166138, alpha: 0.9874821154508174\n",
      "epoch 68 iteration600, G Loss: 1.1117849349975586, D Loss: 1.189396619796753, alpha: 0.9874821154508174\n",
      "epoch 68 iteration700, G Loss: 1.2003066539764404, D Loss: 1.1032928228378296, alpha: 0.9874821154508174\n",
      "epoch 68 iteration800, G Loss: 0.882046103477478, D Loss: 1.242095708847046, alpha: 0.9874821154508174\n",
      "Saving content.\n",
      "epoch 69 iteration0, G Loss: 1.04649019241333, D Loss: 1.2015984058380127, alpha: 0.9871819495557353\n",
      "epoch 69 iteration100, G Loss: 1.6106733083724976, D Loss: 1.4552937746047974, alpha: 0.9871819495557353\n",
      "epoch 69 iteration200, G Loss: 0.7588080167770386, D Loss: 1.2370637655258179, alpha: 0.9871819495557353\n",
      "epoch 69 iteration300, G Loss: 0.8591252565383911, D Loss: 1.2703793048858643, alpha: 0.9871819495557353\n",
      "epoch 69 iteration400, G Loss: 0.6616304516792297, D Loss: 1.102806806564331, alpha: 0.9871819495557353\n",
      "epoch 69 iteration500, G Loss: 1.002223253250122, D Loss: 1.138008713722229, alpha: 0.9871819495557353\n",
      "epoch 69 iteration600, G Loss: 0.9229894280433655, D Loss: 1.2222139835357666, alpha: 0.9871819495557353\n",
      "epoch 69 iteration700, G Loss: 0.7023258209228516, D Loss: 1.3039751052856445, alpha: 0.9871819495557353\n",
      "epoch 69 iteration800, G Loss: 1.4717395305633545, D Loss: 1.2443697452545166, alpha: 0.9871819495557353\n",
      "Saving content.\n",
      "epoch 70 iteration0, G Loss: 0.8895558714866638, D Loss: 1.3217236995697021, alpha: 0.9868746816628972\n",
      "epoch 70 iteration100, G Loss: 1.5286122560501099, D Loss: 1.3284575939178467, alpha: 0.9868746816628972\n",
      "epoch 70 iteration200, G Loss: 1.026374340057373, D Loss: 1.2277281284332275, alpha: 0.9868746816628972\n",
      "epoch 70 iteration300, G Loss: 1.1483309268951416, D Loss: 1.0898950099945068, alpha: 0.9868746816628972\n",
      "epoch 70 iteration400, G Loss: 0.8738558292388916, D Loss: 1.3851478099822998, alpha: 0.9868746816628972\n",
      "epoch 70 iteration500, G Loss: 1.0186740159988403, D Loss: 1.3186304569244385, alpha: 0.9868746816628972\n",
      "epoch 70 iteration600, G Loss: 1.0695139169692993, D Loss: 1.2487547397613525, alpha: 0.9868746816628972\n",
      "epoch 70 iteration700, G Loss: 1.036558985710144, D Loss: 1.2560620307922363, alpha: 0.9868746816628972\n",
      "epoch 70 iteration800, G Loss: 0.9858593940734863, D Loss: 1.3514683246612549, alpha: 0.9868746816628972\n",
      "Saving content.\n",
      "epoch 71 iteration0, G Loss: 1.1416077613830566, D Loss: 1.371158242225647, alpha: 0.986560148376769\n",
      "epoch 71 iteration100, G Loss: 1.2162617444992065, D Loss: 1.2461295127868652, alpha: 0.986560148376769\n",
      "epoch 71 iteration200, G Loss: 1.1627458333969116, D Loss: 1.3273346424102783, alpha: 0.986560148376769\n",
      "epoch 71 iteration300, G Loss: 1.2065987586975098, D Loss: 1.206226110458374, alpha: 0.986560148376769\n",
      "epoch 71 iteration400, G Loss: 0.9772136807441711, D Loss: 1.155099630355835, alpha: 0.986560148376769\n",
      "epoch 71 iteration500, G Loss: 0.6234599947929382, D Loss: 1.279977560043335, alpha: 0.986560148376769\n",
      "epoch 71 iteration600, G Loss: 1.2930669784545898, D Loss: 1.2427864074707031, alpha: 0.986560148376769\n",
      "epoch 71 iteration700, G Loss: 1.208815097808838, D Loss: 1.2194359302520752, alpha: 0.986560148376769\n",
      "epoch 71 iteration800, G Loss: 0.6898064613342285, D Loss: 1.2736425399780273, alpha: 0.986560148376769\n",
      "Saving content.\n",
      "epoch 72 iteration0, G Loss: 0.9907146096229553, D Loss: 1.4860742092132568, alpha: 0.9862381827674608\n",
      "epoch 72 iteration100, G Loss: 0.9129736423492432, D Loss: 1.2461042404174805, alpha: 0.9862381827674608\n",
      "epoch 72 iteration200, G Loss: 0.9454375505447388, D Loss: 1.2673616409301758, alpha: 0.9862381827674608\n",
      "epoch 72 iteration300, G Loss: 1.0370149612426758, D Loss: 1.1534156799316406, alpha: 0.9862381827674608\n",
      "epoch 72 iteration400, G Loss: 1.2516899108886719, D Loss: 1.2800734043121338, alpha: 0.9862381827674608\n",
      "epoch 72 iteration500, G Loss: 1.1283411979675293, D Loss: 1.2030744552612305, alpha: 0.9862381827674608\n",
      "epoch 72 iteration600, G Loss: 0.9387561082839966, D Loss: 1.2247779369354248, alpha: 0.9862381827674608\n",
      "epoch 72 iteration700, G Loss: 1.3125576972961426, D Loss: 1.3002595901489258, alpha: 0.9862381827674608\n",
      "epoch 72 iteration800, G Loss: 0.943137526512146, D Loss: 1.2759603261947632, alpha: 0.9862381827674608\n",
      "Saving content.\n",
      "epoch 73 iteration0, G Loss: 1.1627672910690308, D Loss: 1.244786262512207, alpha: 0.9859086143052553\n",
      "epoch 73 iteration100, G Loss: 0.829433798789978, D Loss: 1.400567889213562, alpha: 0.9859086143052553\n",
      "epoch 73 iteration200, G Loss: 0.7645405530929565, D Loss: 1.2890392541885376, alpha: 0.9859086143052553\n",
      "epoch 73 iteration300, G Loss: 1.2294551134109497, D Loss: 1.275308609008789, alpha: 0.9859086143052553\n",
      "epoch 73 iteration400, G Loss: 0.9120655059814453, D Loss: 1.219002366065979, alpha: 0.9859086143052553\n",
      "epoch 73 iteration500, G Loss: 0.43878334760665894, D Loss: 1.277414321899414, alpha: 0.9859086143052553\n",
      "epoch 73 iteration600, G Loss: 1.1998933553695679, D Loss: 1.2287578582763672, alpha: 0.9859086143052553\n",
      "epoch 73 iteration700, G Loss: 1.2270216941833496, D Loss: 1.2281866073608398, alpha: 0.9859086143052553\n",
      "epoch 73 iteration800, G Loss: 1.096764087677002, D Loss: 1.2787158489227295, alpha: 0.9859086143052553\n",
      "Saving content.\n",
      "epoch 74 iteration0, G Loss: 1.0639457702636719, D Loss: 1.2221851348876953, alpha: 0.9855712687944772\n",
      "epoch 74 iteration100, G Loss: 1.1651811599731445, D Loss: 1.1928523778915405, alpha: 0.9855712687944772\n",
      "epoch 74 iteration200, G Loss: 0.8943341970443726, D Loss: 1.3669283390045166, alpha: 0.9855712687944772\n",
      "epoch 74 iteration300, G Loss: 1.5056192874908447, D Loss: 1.3157548904418945, alpha: 0.9855712687944772\n",
      "epoch 74 iteration400, G Loss: 1.0215272903442383, D Loss: 1.3047409057617188, alpha: 0.9855712687944772\n",
      "epoch 74 iteration500, G Loss: 1.1375324726104736, D Loss: 1.2100467681884766, alpha: 0.9855712687944772\n",
      "epoch 74 iteration600, G Loss: 0.8957220911979675, D Loss: 1.1659377813339233, alpha: 0.9855712687944772\n",
      "epoch 74 iteration700, G Loss: 0.845356822013855, D Loss: 1.205896019935608, alpha: 0.9855712687944772\n",
      "epoch 74 iteration800, G Loss: 0.984176754951477, D Loss: 1.2395884990692139, alpha: 0.9855712687944772\n",
      "Saving content.\n",
      "epoch 75 iteration0, G Loss: 1.0604394674301147, D Loss: 1.2608356475830078, alpha: 0.9852259683067269\n",
      "epoch 75 iteration100, G Loss: 1.0135828256607056, D Loss: 1.2133047580718994, alpha: 0.9852259683067269\n",
      "epoch 75 iteration200, G Loss: 1.048630714416504, D Loss: 1.1937700510025024, alpha: 0.9852259683067269\n",
      "epoch 75 iteration300, G Loss: 1.031796932220459, D Loss: 1.2570844888687134, alpha: 0.9852259683067269\n",
      "epoch 75 iteration400, G Loss: 0.9857017397880554, D Loss: 1.1916054487228394, alpha: 0.9852259683067269\n",
      "epoch 75 iteration500, G Loss: 0.7194738984107971, D Loss: 1.2414400577545166, alpha: 0.9852259683067269\n",
      "epoch 75 iteration600, G Loss: 0.915203332901001, D Loss: 1.2614705562591553, alpha: 0.9852259683067269\n",
      "epoch 75 iteration700, G Loss: 0.7605674266815186, D Loss: 1.2878174781799316, alpha: 0.9852259683067269\n",
      "epoch 75 iteration800, G Loss: 1.7426167726516724, D Loss: 1.1661404371261597, alpha: 0.9852259683067269\n",
      "Saving content.\n",
      "epoch 76 iteration0, G Loss: 1.000566840171814, D Loss: 1.2475576400756836, alpha: 0.9848725311135066\n",
      "epoch 76 iteration100, G Loss: 0.9720560312271118, D Loss: 1.2065553665161133, alpha: 0.9848725311135066\n",
      "epoch 76 iteration200, G Loss: 1.4171136617660522, D Loss: 1.062852382659912, alpha: 0.9848725311135066\n",
      "epoch 76 iteration300, G Loss: 0.9039157629013062, D Loss: 1.2463759183883667, alpha: 0.9848725311135066\n",
      "epoch 76 iteration400, G Loss: 0.891586184501648, D Loss: 1.4320460557937622, alpha: 0.9848725311135066\n",
      "epoch 76 iteration500, G Loss: 1.0834288597106934, D Loss: 1.339299201965332, alpha: 0.9848725311135066\n",
      "epoch 76 iteration600, G Loss: 0.8397254347801208, D Loss: 1.3240827322006226, alpha: 0.9848725311135066\n",
      "epoch 76 iteration700, G Loss: 1.0365173816680908, D Loss: 1.2188799381256104, alpha: 0.9848725311135066\n",
      "epoch 76 iteration800, G Loss: 0.6837960481643677, D Loss: 1.2612055540084839, alpha: 0.9848725311135066\n",
      "Saving content.\n",
      "epoch 77 iteration0, G Loss: 0.6793676614761353, D Loss: 1.537471055984497, alpha: 0.984510771618267\n",
      "epoch 77 iteration100, G Loss: 0.8356042504310608, D Loss: 1.2593059539794922, alpha: 0.984510771618267\n",
      "epoch 77 iteration200, G Loss: 0.6636512875556946, D Loss: 1.469159483909607, alpha: 0.984510771618267\n",
      "epoch 77 iteration300, G Loss: 1.421821117401123, D Loss: 1.3554023504257202, alpha: 0.984510771618267\n",
      "epoch 77 iteration400, G Loss: 1.1365699768066406, D Loss: 1.2829252481460571, alpha: 0.984510771618267\n",
      "epoch 77 iteration500, G Loss: 0.8442752957344055, D Loss: 1.3613511323928833, alpha: 0.984510771618267\n",
      "epoch 77 iteration600, G Loss: 0.9798357486724854, D Loss: 1.2471750974655151, alpha: 0.984510771618267\n",
      "epoch 77 iteration700, G Loss: 0.7896233797073364, D Loss: 1.2288089990615845, alpha: 0.984510771618267\n",
      "epoch 77 iteration800, G Loss: 1.1721413135528564, D Loss: 1.194854736328125, alpha: 0.984510771618267\n",
      "Saving content.\n",
      "epoch 78 iteration0, G Loss: 1.3470638990402222, D Loss: 1.1756070852279663, alpha: 0.9841405002879078\n",
      "epoch 78 iteration100, G Loss: 0.7912701964378357, D Loss: 1.2098972797393799, alpha: 0.9841405002879078\n",
      "epoch 78 iteration200, G Loss: 0.9502325654029846, D Loss: 1.2658038139343262, alpha: 0.9841405002879078\n",
      "epoch 78 iteration300, G Loss: 1.3150560855865479, D Loss: 1.1794809103012085, alpha: 0.9841405002879078\n",
      "epoch 78 iteration400, G Loss: 0.7836893796920776, D Loss: 1.1749999523162842, alpha: 0.9841405002879078\n",
      "epoch 78 iteration500, G Loss: 0.7269130945205688, D Loss: 1.3781194686889648, alpha: 0.9841405002879078\n",
      "epoch 78 iteration600, G Loss: 0.8606562614440918, D Loss: 1.223464012145996, alpha: 0.9841405002879078\n",
      "epoch 78 iteration700, G Loss: 1.0137479305267334, D Loss: 1.2145400047302246, alpha: 0.9841405002879078\n",
      "epoch 78 iteration800, G Loss: 0.5955725908279419, D Loss: 1.3749264478683472, alpha: 0.9841405002879078\n",
      "Saving content.\n",
      "epoch 79 iteration0, G Loss: 1.2024531364440918, D Loss: 1.2662925720214844, alpha: 0.9837615235837642\n",
      "epoch 79 iteration100, G Loss: 0.7643200159072876, D Loss: 1.1637629270553589, alpha: 0.9837615235837642\n",
      "epoch 79 iteration200, G Loss: 0.7313867211341858, D Loss: 1.2553694248199463, alpha: 0.9837615235837642\n",
      "epoch 79 iteration300, G Loss: 1.1825426816940308, D Loss: 1.2575290203094482, alpha: 0.9837615235837642\n",
      "epoch 79 iteration400, G Loss: 2.004765748977661, D Loss: 1.1734365224838257, alpha: 0.9837615235837642\n",
      "epoch 79 iteration500, G Loss: 0.6606113910675049, D Loss: 1.2242127656936646, alpha: 0.9837615235837642\n",
      "epoch 79 iteration600, G Loss: 0.6982197761535645, D Loss: 1.5355240106582642, alpha: 0.9837615235837642\n",
      "epoch 79 iteration700, G Loss: 1.0939569473266602, D Loss: 1.2239975929260254, alpha: 0.9837615235837642\n",
      "epoch 79 iteration800, G Loss: 1.0049283504486084, D Loss: 1.2494697570800781, alpha: 0.9837615235837642\n",
      "Saving content.\n",
      "epoch 80 iteration0, G Loss: 1.8377649784088135, D Loss: 1.9532966613769531, alpha: 0.9833736438921183\n",
      "epoch 80 iteration100, G Loss: 1.066570520401001, D Loss: 1.1544729471206665, alpha: 0.9833736438921183\n",
      "epoch 80 iteration200, G Loss: 0.8483275175094604, D Loss: 1.190690517425537, alpha: 0.9833736438921183\n",
      "epoch 80 iteration300, G Loss: 1.3491997718811035, D Loss: 1.2430822849273682, alpha: 0.9833736438921183\n",
      "epoch 80 iteration400, G Loss: 1.1667094230651855, D Loss: 1.2527594566345215, alpha: 0.9833736438921183\n",
      "epoch 80 iteration500, G Loss: 0.8630902767181396, D Loss: 1.2885239124298096, alpha: 0.9833736438921183\n",
      "epoch 80 iteration600, G Loss: 0.7730298042297363, D Loss: 1.806053638458252, alpha: 0.9833736438921183\n",
      "epoch 80 iteration700, G Loss: 0.9004268646240234, D Loss: 1.3002052307128906, alpha: 0.9833736438921183\n",
      "epoch 80 iteration800, G Loss: 1.9371119737625122, D Loss: 1.228488564491272, alpha: 0.9833736438921183\n",
      "Saving content.\n",
      "epoch 81 iteration0, G Loss: 0.9033507704734802, D Loss: 1.196307897567749, alpha: 0.9829766594542746\n",
      "epoch 81 iteration100, G Loss: 0.7745309472084045, D Loss: 1.104835867881775, alpha: 0.9829766594542746\n",
      "epoch 81 iteration200, G Loss: 0.9158799648284912, D Loss: 1.2802319526672363, alpha: 0.9829766594542746\n",
      "epoch 81 iteration300, G Loss: 0.7514263987541199, D Loss: 1.279352068901062, alpha: 0.9829766594542746\n",
      "epoch 81 iteration400, G Loss: 0.9850720167160034, D Loss: 1.2673028707504272, alpha: 0.9829766594542746\n",
      "epoch 81 iteration500, G Loss: 0.9145804643630981, D Loss: 1.287832260131836, alpha: 0.9829766594542746\n",
      "epoch 81 iteration600, G Loss: 1.1036440134048462, D Loss: 1.261918544769287, alpha: 0.9829766594542746\n",
      "epoch 81 iteration700, G Loss: 1.0964794158935547, D Loss: 1.2890000343322754, alpha: 0.9829766594542746\n",
      "epoch 81 iteration800, G Loss: 1.2482372522354126, D Loss: 1.2735424041748047, alpha: 0.9829766594542746\n",
      "Saving content.\n",
      "epoch 82 iteration0, G Loss: 1.066161870956421, D Loss: 1.2246627807617188, alpha: 0.9825703642962413\n",
      "epoch 82 iteration100, G Loss: 0.7361591458320618, D Loss: 1.5739240646362305, alpha: 0.9825703642962413\n",
      "epoch 82 iteration200, G Loss: 0.8375266790390015, D Loss: 1.1886661052703857, alpha: 0.9825703642962413\n",
      "epoch 82 iteration300, G Loss: 0.7443596124649048, D Loss: 1.2445034980773926, alpha: 0.9825703642962413\n",
      "epoch 82 iteration400, G Loss: 1.2551990747451782, D Loss: 1.274621844291687, alpha: 0.9825703642962413\n",
      "epoch 82 iteration500, G Loss: 1.672622561454773, D Loss: 1.1938238143920898, alpha: 0.9825703642962413\n",
      "epoch 82 iteration600, G Loss: 1.1318550109863281, D Loss: 1.2285661697387695, alpha: 0.9825703642962413\n",
      "epoch 82 iteration700, G Loss: 0.790523886680603, D Loss: 1.5578200817108154, alpha: 0.9825703642962413\n",
      "epoch 82 iteration800, G Loss: 0.941909909248352, D Loss: 1.2464475631713867, alpha: 0.9825703642962413\n",
      "Saving content.\n",
      "epoch 83 iteration0, G Loss: 0.8978678584098816, D Loss: 1.1943490505218506, alpha: 0.9821545481580658\n",
      "epoch 83 iteration100, G Loss: 1.1880035400390625, D Loss: 1.2030141353607178, alpha: 0.9821545481580658\n",
      "epoch 83 iteration200, G Loss: 0.9423969984054565, D Loss: 1.2254793643951416, alpha: 0.9821545481580658\n",
      "epoch 83 iteration300, G Loss: 1.0866795778274536, D Loss: 1.2478917837142944, alpha: 0.9821545481580658\n",
      "epoch 83 iteration400, G Loss: 0.619612991809845, D Loss: 1.4022202491760254, alpha: 0.9821545481580658\n",
      "epoch 83 iteration500, G Loss: 1.0443744659423828, D Loss: 1.255504846572876, alpha: 0.9821545481580658\n",
      "epoch 83 iteration600, G Loss: 1.1534039974212646, D Loss: 1.255361557006836, alpha: 0.9821545481580658\n",
      "epoch 83 iteration700, G Loss: 1.0531021356582642, D Loss: 1.406592845916748, alpha: 0.9821545481580658\n",
      "epoch 83 iteration800, G Loss: 1.1304740905761719, D Loss: 1.2037181854248047, alpha: 0.9821545481580658\n",
      "Saving content.\n",
      "epoch 84 iteration0, G Loss: 1.06914484500885, D Loss: 1.2084167003631592, alpha: 0.9817289964228708\n",
      "epoch 84 iteration100, G Loss: 1.367854356765747, D Loss: 1.1186628341674805, alpha: 0.9817289964228708\n",
      "epoch 84 iteration200, G Loss: 1.3034017086029053, D Loss: 1.273268699645996, alpha: 0.9817289964228708\n",
      "epoch 84 iteration300, G Loss: 1.1266862154006958, D Loss: 1.2436959743499756, alpha: 0.9817289964228708\n",
      "epoch 84 iteration400, G Loss: 1.3218636512756348, D Loss: 1.3581883907318115, alpha: 0.9817289964228708\n",
      "epoch 84 iteration500, G Loss: 1.0842111110687256, D Loss: 1.1902892589569092, alpha: 0.9817289964228708\n",
      "epoch 84 iteration600, G Loss: 1.1102802753448486, D Loss: 1.340489149093628, alpha: 0.9817289964228708\n",
      "epoch 84 iteration700, G Loss: 1.3133087158203125, D Loss: 1.3075295686721802, alpha: 0.9817289964228708\n",
      "epoch 84 iteration800, G Loss: 0.8950555324554443, D Loss: 1.3706908226013184, alpha: 0.9817289964228708\n",
      "Saving content.\n",
      "epoch 85 iteration0, G Loss: 1.0852880477905273, D Loss: 1.2739050388336182, alpha: 0.9812934900456454\n",
      "epoch 85 iteration100, G Loss: 1.1677148342132568, D Loss: 1.1708495616912842, alpha: 0.9812934900456454\n",
      "epoch 85 iteration200, G Loss: 0.920418918132782, D Loss: 1.3663103580474854, alpha: 0.9812934900456454\n",
      "epoch 85 iteration300, G Loss: 1.428956151008606, D Loss: 1.2733299732208252, alpha: 0.9812934900456454\n",
      "epoch 85 iteration400, G Loss: 1.63632333278656, D Loss: 1.4285269975662231, alpha: 0.9812934900456454\n",
      "epoch 85 iteration500, G Loss: 1.1954412460327148, D Loss: 1.2793242931365967, alpha: 0.9812934900456454\n",
      "epoch 85 iteration600, G Loss: 1.5152291059494019, D Loss: 1.2491592168807983, alpha: 0.9812934900456454\n",
      "epoch 85 iteration700, G Loss: 0.705227255821228, D Loss: 1.1220078468322754, alpha: 0.9812934900456454\n",
      "epoch 85 iteration800, G Loss: 1.4282817840576172, D Loss: 1.2450851202011108, alpha: 0.9812934900456454\n",
      "Saving content.\n",
      "epoch 86 iteration0, G Loss: 1.5620014667510986, D Loss: 1.2112367153167725, alpha: 0.9808478054818466\n",
      "epoch 86 iteration100, G Loss: 1.0110502243041992, D Loss: 1.1914052963256836, alpha: 0.9808478054818466\n",
      "epoch 86 iteration200, G Loss: 1.0494136810302734, D Loss: 1.30965256690979, alpha: 0.9808478054818466\n",
      "epoch 86 iteration300, G Loss: 0.825465202331543, D Loss: 1.2525538206100464, alpha: 0.9808478054818466\n",
      "epoch 86 iteration400, G Loss: 0.9092260599136353, D Loss: 1.3326908349990845, alpha: 0.9808478054818466\n",
      "epoch 86 iteration500, G Loss: 1.009129285812378, D Loss: 1.2089594602584839, alpha: 0.9808478054818466\n",
      "epoch 86 iteration600, G Loss: 1.133029580116272, D Loss: 1.2360925674438477, alpha: 0.9808478054818466\n",
      "epoch 86 iteration700, G Loss: 1.5642396211624146, D Loss: 1.179142713546753, alpha: 0.9808478054818466\n",
      "epoch 86 iteration800, G Loss: 0.747750997543335, D Loss: 1.4096317291259766, alpha: 0.9808478054818466\n",
      "Saving content.\n",
      "epoch 87 iteration0, G Loss: 0.8968868255615234, D Loss: 1.2873059511184692, alpha: 0.9803917146158709\n",
      "epoch 87 iteration100, G Loss: 1.3327564001083374, D Loss: 1.1707677841186523, alpha: 0.9803917146158709\n",
      "epoch 87 iteration200, G Loss: 1.0354444980621338, D Loss: 1.2723572254180908, alpha: 0.9803917146158709\n",
      "epoch 87 iteration300, G Loss: 0.9602389335632324, D Loss: 1.2048768997192383, alpha: 0.9803917146158709\n",
      "epoch 87 iteration400, G Loss: 1.0847442150115967, D Loss: 1.2843587398529053, alpha: 0.9803917146158709\n",
      "epoch 87 iteration500, G Loss: 1.3181713819503784, D Loss: 1.3616684675216675, alpha: 0.9803917146158709\n",
      "epoch 87 iteration600, G Loss: 0.6906310319900513, D Loss: 1.268118143081665, alpha: 0.9803917146158709\n",
      "epoch 87 iteration700, G Loss: 0.7869645953178406, D Loss: 1.4340136051177979, alpha: 0.9803917146158709\n",
      "epoch 87 iteration800, G Loss: 0.9077146649360657, D Loss: 1.3380236625671387, alpha: 0.9803917146158709\n",
      "Saving content.\n",
      "epoch 88 iteration0, G Loss: 0.8801043629646301, D Loss: 1.2206887006759644, alpha: 0.9799249846894594\n",
      "epoch 88 iteration100, G Loss: 1.0543652772903442, D Loss: 1.1501100063323975, alpha: 0.9799249846894594\n",
      "epoch 88 iteration200, G Loss: 0.9630852341651917, D Loss: 1.237163782119751, alpha: 0.9799249846894594\n",
      "epoch 88 iteration300, G Loss: 1.2142293453216553, D Loss: 1.1875239610671997, alpha: 0.9799249846894594\n",
      "epoch 88 iteration400, G Loss: 0.9041262865066528, D Loss: 1.3206708431243896, alpha: 0.9799249846894594\n",
      "epoch 88 iteration500, G Loss: 0.9192402362823486, D Loss: 1.1371510028839111, alpha: 0.9799249846894594\n",
      "epoch 88 iteration600, G Loss: 1.190580129623413, D Loss: 1.1362841129302979, alpha: 0.9799249846894594\n",
      "epoch 88 iteration700, G Loss: 0.7557082176208496, D Loss: 1.2086560726165771, alpha: 0.9799249846894594\n",
      "epoch 88 iteration800, G Loss: 1.1045441627502441, D Loss: 1.2010233402252197, alpha: 0.9799249846894594\n",
      "Saving content.\n",
      "epoch 89 iteration0, G Loss: 0.7651829123497009, D Loss: 1.2502222061157227, alpha: 0.9794473782301051\n",
      "epoch 89 iteration100, G Loss: 1.0109875202178955, D Loss: 1.1489455699920654, alpha: 0.9794473782301051\n",
      "epoch 89 iteration200, G Loss: 1.1357477903366089, D Loss: 1.2746798992156982, alpha: 0.9794473782301051\n",
      "epoch 89 iteration300, G Loss: 1.1218713521957397, D Loss: 1.3504807949066162, alpha: 0.9794473782301051\n",
      "epoch 89 iteration400, G Loss: 0.568157970905304, D Loss: 1.3495643138885498, alpha: 0.9794473782301051\n",
      "epoch 89 iteration500, G Loss: 0.6618325710296631, D Loss: 1.6722090244293213, alpha: 0.9794473782301051\n",
      "epoch 89 iteration600, G Loss: 1.588631510734558, D Loss: 1.3567532300949097, alpha: 0.9794473782301051\n",
      "epoch 89 iteration700, G Loss: 0.9488141536712646, D Loss: 1.1870381832122803, alpha: 0.9794473782301051\n",
      "epoch 89 iteration800, G Loss: 0.9609341621398926, D Loss: 1.1946582794189453, alpha: 0.9794473782301051\n",
      "Saving content.\n",
      "epoch 90 iteration0, G Loss: 1.1047699451446533, D Loss: 1.324654221534729, alpha: 0.9789586529795318\n",
      "epoch 90 iteration100, G Loss: 1.3085438013076782, D Loss: 1.1006578207015991, alpha: 0.9789586529795318\n",
      "epoch 90 iteration200, G Loss: 1.1069246530532837, D Loss: 1.2490994930267334, alpha: 0.9789586529795318\n",
      "epoch 90 iteration300, G Loss: 1.1927272081375122, D Loss: 1.2551302909851074, alpha: 0.9789586529795318\n",
      "epoch 90 iteration400, G Loss: 1.278313159942627, D Loss: 1.2284079790115356, alpha: 0.9789586529795318\n",
      "epoch 90 iteration500, G Loss: 0.9184179306030273, D Loss: 1.2840967178344727, alpha: 0.9789586529795318\n",
      "epoch 90 iteration600, G Loss: 1.6527787446975708, D Loss: 1.3043205738067627, alpha: 0.9789586529795318\n",
      "epoch 90 iteration700, G Loss: 0.953175961971283, D Loss: 1.359548568725586, alpha: 0.9789586529795318\n",
      "epoch 90 iteration800, G Loss: 0.696540117263794, D Loss: 1.2517961263656616, alpha: 0.9789586529795318\n",
      "Saving content.\n",
      "epoch 91 iteration0, G Loss: 0.879542887210846, D Loss: 1.1858447790145874, alpha: 0.9784585618223235\n",
      "epoch 91 iteration100, G Loss: 0.8596093654632568, D Loss: 1.2704107761383057, alpha: 0.9784585618223235\n",
      "epoch 91 iteration200, G Loss: 1.0575110912322998, D Loss: 1.1984095573425293, alpha: 0.9784585618223235\n",
      "epoch 91 iteration300, G Loss: 1.0240064859390259, D Loss: 1.1136608123779297, alpha: 0.9784585618223235\n",
      "epoch 91 iteration400, G Loss: 1.4760080575942993, D Loss: 1.2330572605133057, alpha: 0.9784585618223235\n",
      "epoch 91 iteration500, G Loss: 0.9675605893135071, D Loss: 1.3657078742980957, alpha: 0.9784585618223235\n",
      "epoch 91 iteration600, G Loss: 0.9365781545639038, D Loss: 1.138620376586914, alpha: 0.9784585618223235\n",
      "epoch 91 iteration700, G Loss: 0.9905151128768921, D Loss: 1.3666796684265137, alpha: 0.9784585618223235\n",
      "epoch 91 iteration800, G Loss: 1.1423708200454712, D Loss: 1.192043662071228, alpha: 0.9784585618223235\n",
      "Saving content.\n",
      "epoch 92 iteration0, G Loss: 1.0235211849212646, D Loss: 1.2109386920928955, alpha: 0.977946852714783\n",
      "epoch 92 iteration100, G Loss: 0.8480793833732605, D Loss: 1.162733793258667, alpha: 0.977946852714783\n",
      "epoch 92 iteration200, G Loss: 0.9577445983886719, D Loss: 1.1962429285049438, alpha: 0.977946852714783\n",
      "epoch 92 iteration300, G Loss: 0.8381045460700989, D Loss: 1.2929868698120117, alpha: 0.977946852714783\n",
      "epoch 92 iteration400, G Loss: 1.0834118127822876, D Loss: 1.238465666770935, alpha: 0.977946852714783\n",
      "epoch 92 iteration500, G Loss: 0.8504994511604309, D Loss: 1.1888668537139893, alpha: 0.977946852714783\n",
      "epoch 92 iteration600, G Loss: 0.8643174171447754, D Loss: 1.2823514938354492, alpha: 0.977946852714783\n",
      "epoch 92 iteration700, G Loss: 1.2161798477172852, D Loss: 1.416079044342041, alpha: 0.977946852714783\n",
      "epoch 92 iteration800, G Loss: 0.8247812390327454, D Loss: 1.4347437620162964, alpha: 0.977946852714783\n",
      "Saving content.\n",
      "epoch 93 iteration0, G Loss: 0.9951777458190918, D Loss: 1.238630771636963, alpha: 0.9774232686141044\n",
      "epoch 93 iteration100, G Loss: 0.8434398770332336, D Loss: 1.2016301155090332, alpha: 0.9774232686141044\n",
      "epoch 93 iteration200, G Loss: 0.796320915222168, D Loss: 1.2713768482208252, alpha: 0.9774232686141044\n",
      "epoch 93 iteration300, G Loss: 0.9362168312072754, D Loss: 1.3932040929794312, alpha: 0.9774232686141044\n",
      "epoch 93 iteration400, G Loss: 1.193877935409546, D Loss: 1.4618563652038574, alpha: 0.9774232686141044\n",
      "epoch 93 iteration500, G Loss: 0.6539735794067383, D Loss: 1.1591079235076904, alpha: 0.9774232686141044\n",
      "epoch 93 iteration600, G Loss: 0.8209705352783203, D Loss: 1.2131098508834839, alpha: 0.9774232686141044\n",
      "epoch 93 iteration700, G Loss: 1.0207630395889282, D Loss: 1.1758716106414795, alpha: 0.9774232686141044\n",
      "epoch 93 iteration800, G Loss: 0.8078445196151733, D Loss: 1.2128901481628418, alpha: 0.9774232686141044\n",
      "Saving content.\n",
      "epoch 94 iteration0, G Loss: 1.3363851308822632, D Loss: 1.2077720165252686, alpha: 0.9768875474079524\n",
      "epoch 94 iteration100, G Loss: 1.0368887186050415, D Loss: 1.2282390594482422, alpha: 0.9768875474079524\n",
      "epoch 94 iteration200, G Loss: 0.9199579954147339, D Loss: 1.177100419998169, alpha: 0.9768875474079524\n",
      "epoch 94 iteration300, G Loss: 1.0311715602874756, D Loss: 1.2714414596557617, alpha: 0.9768875474079524\n",
      "epoch 94 iteration400, G Loss: 0.8737369775772095, D Loss: 1.23280930519104, alpha: 0.9768875474079524\n",
      "epoch 94 iteration500, G Loss: 0.8784576058387756, D Loss: 1.2781192064285278, alpha: 0.9768875474079524\n",
      "epoch 94 iteration600, G Loss: 0.9124482274055481, D Loss: 1.2666599750518799, alpha: 0.9768875474079524\n",
      "epoch 94 iteration700, G Loss: 0.9526546001434326, D Loss: 1.192450761795044, alpha: 0.9768875474079524\n",
      "epoch 94 iteration800, G Loss: 0.8669595718383789, D Loss: 1.1773818731307983, alpha: 0.9768875474079524\n",
      "Saving content.\n",
      "epoch 95 iteration0, G Loss: 1.3816605806350708, D Loss: 1.2411844730377197, alpha: 0.9763394218445388\n",
      "epoch 95 iteration100, G Loss: 1.0634281635284424, D Loss: 1.185982346534729, alpha: 0.9763394218445388\n",
      "epoch 95 iteration200, G Loss: 0.673743486404419, D Loss: 1.390143632888794, alpha: 0.9763394218445388\n",
      "epoch 95 iteration300, G Loss: 0.9674089550971985, D Loss: 1.232058048248291, alpha: 0.9763394218445388\n",
      "epoch 95 iteration400, G Loss: 1.0382213592529297, D Loss: 1.1052098274230957, alpha: 0.9763394218445388\n",
      "epoch 95 iteration500, G Loss: 0.9542075395584106, D Loss: 1.2713623046875, alpha: 0.9763394218445388\n",
      "epoch 95 iteration600, G Loss: 1.3294191360473633, D Loss: 1.6717126369476318, alpha: 0.9763394218445388\n",
      "epoch 95 iteration700, G Loss: 1.0997283458709717, D Loss: 1.302847146987915, alpha: 0.9763394218445388\n",
      "epoch 95 iteration800, G Loss: 0.9487845301628113, D Loss: 1.249626636505127, alpha: 0.9763394218445388\n",
      "Saving content.\n",
      "epoch 96 iteration0, G Loss: 1.1222178936004639, D Loss: 1.2867639064788818, alpha: 0.9757786194633021\n",
      "epoch 96 iteration100, G Loss: 1.098743200302124, D Loss: 1.309423804283142, alpha: 0.9757786194633021\n",
      "epoch 96 iteration200, G Loss: 1.4781898260116577, D Loss: 1.2607752084732056, alpha: 0.9757786194633021\n",
      "epoch 96 iteration300, G Loss: 1.2109990119934082, D Loss: 1.2848644256591797, alpha: 0.9757786194633021\n",
      "epoch 96 iteration400, G Loss: 0.9775777459144592, D Loss: 1.245455265045166, alpha: 0.9757786194633021\n",
      "epoch 96 iteration500, G Loss: 1.1759989261627197, D Loss: 1.292945384979248, alpha: 0.9757786194633021\n",
      "epoch 96 iteration600, G Loss: 0.7705844640731812, D Loss: 1.209580659866333, alpha: 0.9757786194633021\n",
      "epoch 96 iteration700, G Loss: 0.9072262644767761, D Loss: 1.173741340637207, alpha: 0.9757786194633021\n",
      "epoch 96 iteration800, G Loss: 1.5402462482452393, D Loss: 1.2070025205612183, alpha: 0.9757786194633021\n",
      "Saving content.\n",
      "epoch 97 iteration0, G Loss: 0.8552466630935669, D Loss: 1.2920706272125244, alpha: 0.9752048625262908\n",
      "epoch 97 iteration100, G Loss: 0.9502090215682983, D Loss: 1.2563965320587158, alpha: 0.9752048625262908\n",
      "epoch 97 iteration200, G Loss: 0.7865684032440186, D Loss: 1.2642508745193481, alpha: 0.9752048625262908\n",
      "epoch 97 iteration300, G Loss: 0.902253270149231, D Loss: 1.2289950847625732, alpha: 0.9752048625262908\n",
      "epoch 97 iteration400, G Loss: 0.9554771184921265, D Loss: 1.2264752388000488, alpha: 0.9752048625262908\n",
      "epoch 97 iteration500, G Loss: 1.1008286476135254, D Loss: 1.186049222946167, alpha: 0.9752048625262908\n",
      "epoch 97 iteration600, G Loss: 0.7126785516738892, D Loss: 1.2243478298187256, alpha: 0.9752048625262908\n",
      "epoch 97 iteration700, G Loss: 1.1144088506698608, D Loss: 1.190952181816101, alpha: 0.9752048625262908\n",
      "epoch 97 iteration800, G Loss: 0.898584246635437, D Loss: 1.2702869176864624, alpha: 0.9752048625262908\n",
      "Saving content.\n",
      "epoch 98 iteration0, G Loss: 1.3486409187316895, D Loss: 1.2493617534637451, alpha: 0.974617867950365\n",
      "epoch 98 iteration100, G Loss: 0.8896900415420532, D Loss: 1.1854807138442993, alpha: 0.974617867950365\n",
      "epoch 98 iteration200, G Loss: 0.7523847222328186, D Loss: 1.231290578842163, alpha: 0.974617867950365\n",
      "epoch 98 iteration300, G Loss: 0.9038006067276001, D Loss: 1.2712831497192383, alpha: 0.974617867950365\n",
      "epoch 98 iteration400, G Loss: 1.2390825748443604, D Loss: 1.231321096420288, alpha: 0.974617867950365\n",
      "epoch 98 iteration500, G Loss: 1.4335317611694336, D Loss: 1.284252405166626, alpha: 0.974617867950365\n",
      "epoch 98 iteration600, G Loss: 1.0427238941192627, D Loss: 1.263731598854065, alpha: 0.974617867950365\n",
      "epoch 98 iteration700, G Loss: 0.7647362947463989, D Loss: 1.1708955764770508, alpha: 0.974617867950365\n",
      "epoch 98 iteration800, G Loss: 0.9909369349479675, D Loss: 1.2734200954437256, alpha: 0.974617867950365\n",
      "Saving content.\n",
      "epoch 99 iteration0, G Loss: 1.1486138105392456, D Loss: 1.1786226034164429, alpha: 0.9740173472403311\n",
      "epoch 99 iteration100, G Loss: 0.7801695466041565, D Loss: 1.225756049156189, alpha: 0.9740173472403311\n",
      "epoch 99 iteration200, G Loss: 0.6869572401046753, D Loss: 1.2404451370239258, alpha: 0.9740173472403311\n",
      "epoch 99 iteration300, G Loss: 0.8100164532661438, D Loss: 1.2281982898712158, alpha: 0.9740173472403311\n",
      "epoch 99 iteration400, G Loss: 1.108459711074829, D Loss: 1.270046591758728, alpha: 0.9740173472403311\n",
      "epoch 99 iteration500, G Loss: 1.0153840780258179, D Loss: 1.285386562347412, alpha: 0.9740173472403311\n",
      "epoch 99 iteration600, G Loss: 0.9866133332252502, D Loss: 1.220411777496338, alpha: 0.9740173472403311\n",
      "epoch 99 iteration700, G Loss: 1.144166350364685, D Loss: 1.3138573169708252, alpha: 0.9740173472403311\n",
      "epoch 99 iteration800, G Loss: 0.6358563899993896, D Loss: 1.3022470474243164, alpha: 0.9740173472403311\n",
      "Saving content.\n",
      "epoch 100 iteration0, G Loss: 1.2173513174057007, D Loss: 1.1854517459869385, alpha: 0.9734030064231342\n",
      "epoch 100 iteration100, G Loss: 1.5186370611190796, D Loss: 1.206268072128296, alpha: 0.9734030064231342\n",
      "epoch 100 iteration200, G Loss: 1.0842998027801514, D Loss: 1.0712075233459473, alpha: 0.9734030064231342\n",
      "epoch 100 iteration300, G Loss: 1.1100527048110962, D Loss: 1.2268728017807007, alpha: 0.9734030064231342\n",
      "epoch 100 iteration400, G Loss: 1.3919988870620728, D Loss: 1.2083334922790527, alpha: 0.9734030064231342\n",
      "epoch 100 iteration500, G Loss: 1.036894679069519, D Loss: 1.2215964794158936, alpha: 0.9734030064231342\n",
      "epoch 100 iteration600, G Loss: 0.7976332902908325, D Loss: 1.4957494735717773, alpha: 0.9734030064231342\n",
      "epoch 100 iteration700, G Loss: 0.936604380607605, D Loss: 1.601395845413208, alpha: 0.9734030064231342\n",
      "epoch 100 iteration800, G Loss: 1.0575183629989624, D Loss: 1.1480975151062012, alpha: 0.9734030064231342\n",
      "Saving content.\n",
      "epoch 101 iteration0, G Loss: 1.0084812641143799, D Loss: 1.2546560764312744, alpha: 0.9727745459832368\n",
      "epoch 101 iteration100, G Loss: 0.9480526447296143, D Loss: 1.2648260593414307, alpha: 0.9727745459832368\n",
      "epoch 101 iteration200, G Loss: 1.2601732015609741, D Loss: 1.2293317317962646, alpha: 0.9727745459832368\n",
      "epoch 101 iteration300, G Loss: 1.0760895013809204, D Loss: 1.3461612462997437, alpha: 0.9727745459832368\n",
      "epoch 101 iteration400, G Loss: 0.8377148509025574, D Loss: 1.3469066619873047, alpha: 0.9727745459832368\n",
      "epoch 101 iteration500, G Loss: 1.1294851303100586, D Loss: 1.2520718574523926, alpha: 0.9727745459832368\n",
      "epoch 101 iteration600, G Loss: 1.058619499206543, D Loss: 1.1938570737838745, alpha: 0.9727745459832368\n",
      "epoch 101 iteration700, G Loss: 1.1373543739318848, D Loss: 1.1914629936218262, alpha: 0.9727745459832368\n",
      "epoch 101 iteration800, G Loss: 1.0130246877670288, D Loss: 1.272514820098877, alpha: 0.9727745459832368\n",
      "Saving content.\n",
      "epoch 102 iteration0, G Loss: 1.1886028051376343, D Loss: 1.2612764835357666, alpha: 0.9721316607993185\n",
      "epoch 102 iteration100, G Loss: 1.3248544931411743, D Loss: 1.4601114988327026, alpha: 0.9721316607993185\n",
      "epoch 102 iteration200, G Loss: 1.090592861175537, D Loss: 1.1950926780700684, alpha: 0.9721316607993185\n",
      "epoch 102 iteration300, G Loss: 1.2423697710037231, D Loss: 1.2104496955871582, alpha: 0.9721316607993185\n",
      "epoch 102 iteration400, G Loss: 0.8183894157409668, D Loss: 1.1801999807357788, alpha: 0.9721316607993185\n",
      "epoch 102 iteration500, G Loss: 1.0044664144515991, D Loss: 1.1624767780303955, alpha: 0.9721316607993185\n",
      "epoch 102 iteration600, G Loss: 1.1502360105514526, D Loss: 1.3418734073638916, alpha: 0.9721316607993185\n",
      "epoch 102 iteration700, G Loss: 1.1277110576629639, D Loss: 1.1355323791503906, alpha: 0.9721316607993185\n",
      "epoch 102 iteration800, G Loss: 1.3269139528274536, D Loss: 1.2778339385986328, alpha: 0.9721316607993185\n",
      "Saving content.\n",
      "epoch 103 iteration0, G Loss: 1.2687164545059204, D Loss: 1.2288200855255127, alpha: 0.9714740400824391\n",
      "epoch 103 iteration100, G Loss: 1.3012603521347046, D Loss: 1.2536003589630127, alpha: 0.9714740400824391\n",
      "epoch 103 iteration200, G Loss: 0.7915254235267639, D Loss: 1.5636738538742065, alpha: 0.9714740400824391\n",
      "epoch 103 iteration300, G Loss: 0.6345672011375427, D Loss: 1.3447265625, alpha: 0.9714740400824391\n",
      "epoch 103 iteration400, G Loss: 0.854364812374115, D Loss: 1.3462152481079102, alpha: 0.9714740400824391\n",
      "epoch 103 iteration500, G Loss: 0.8831980228424072, D Loss: 1.1726353168487549, alpha: 0.9714740400824391\n",
      "epoch 103 iteration600, G Loss: 1.4947108030319214, D Loss: 1.280412197113037, alpha: 0.9714740400824391\n",
      "epoch 103 iteration700, G Loss: 0.7569671869277954, D Loss: 1.3462159633636475, alpha: 0.9714740400824391\n",
      "epoch 103 iteration800, G Loss: 0.8131877779960632, D Loss: 1.452485203742981, alpha: 0.9714740400824391\n",
      "Saving content.\n",
      "epoch 104 iteration0, G Loss: 0.9397246837615967, D Loss: 1.255521297454834, alpha: 0.9708013673158152\n",
      "epoch 104 iteration100, G Loss: 0.9801799654960632, D Loss: 1.2030354738235474, alpha: 0.9708013673158152\n",
      "epoch 104 iteration200, G Loss: 1.040662407875061, D Loss: 1.2305419445037842, alpha: 0.9708013673158152\n",
      "epoch 104 iteration300, G Loss: 1.2124245166778564, D Loss: 1.0929783582687378, alpha: 0.9708013673158152\n",
      "epoch 104 iteration400, G Loss: 0.8675881624221802, D Loss: 1.2421793937683105, alpha: 0.9708013673158152\n",
      "epoch 104 iteration500, G Loss: 1.1988219022750854, D Loss: 1.2881475687026978, alpha: 0.9708013673158152\n",
      "epoch 104 iteration600, G Loss: 1.1208350658416748, D Loss: 1.1906951665878296, alpha: 0.9708013673158152\n",
      "epoch 104 iteration700, G Loss: 1.1254463195800781, D Loss: 1.2412097454071045, alpha: 0.9708013673158152\n",
      "epoch 104 iteration800, G Loss: 1.2093538045883179, D Loss: 1.1799187660217285, alpha: 0.9708013673158152\n",
      "Saving content.\n",
      "epoch 105 iteration0, G Loss: 0.8913541436195374, D Loss: 1.184988260269165, alpha: 0.9701133201963638\n",
      "epoch 105 iteration100, G Loss: 1.0360441207885742, D Loss: 1.1568368673324585, alpha: 0.9701133201963638\n",
      "epoch 105 iteration200, G Loss: 1.1292219161987305, D Loss: 1.3430750370025635, alpha: 0.9701133201963638\n",
      "epoch 105 iteration300, G Loss: 1.2152347564697266, D Loss: 1.214038372039795, alpha: 0.9701133201963638\n",
      "epoch 105 iteration400, G Loss: 1.4007303714752197, D Loss: 1.291430115699768, alpha: 0.9701133201963638\n",
      "epoch 105 iteration500, G Loss: 1.1943122148513794, D Loss: 1.1803326606750488, alpha: 0.9701133201963638\n",
      "epoch 105 iteration600, G Loss: 1.0118634700775146, D Loss: 1.1710582971572876, alpha: 0.9701133201963638\n",
      "epoch 105 iteration700, G Loss: 1.2270864248275757, D Loss: 1.2463979721069336, alpha: 0.9701133201963638\n",
      "epoch 105 iteration800, G Loss: 0.952073335647583, D Loss: 1.2846777439117432, alpha: 0.9701133201963638\n",
      "Saving content.\n",
      "epoch 106 iteration0, G Loss: 0.6052602529525757, D Loss: 1.3977159261703491, alpha: 0.9694095705781761\n",
      "epoch 106 iteration100, G Loss: 1.539790391921997, D Loss: 1.1817405223846436, alpha: 0.9694095705781761\n",
      "epoch 106 iteration200, G Loss: 0.9124135375022888, D Loss: 1.2834588289260864, alpha: 0.9694095705781761\n",
      "epoch 106 iteration300, G Loss: 0.8215023875236511, D Loss: 1.5495476722717285, alpha: 0.9694095705781761\n",
      "epoch 106 iteration400, G Loss: 0.9546322822570801, D Loss: 1.5444586277008057, alpha: 0.9694095705781761\n",
      "epoch 106 iteration500, G Loss: 0.8353105187416077, D Loss: 1.3395437002182007, alpha: 0.9694095705781761\n",
      "epoch 106 iteration600, G Loss: 1.142686128616333, D Loss: 1.2666547298431396, alpha: 0.9694095705781761\n",
      "epoch 106 iteration700, G Loss: 1.7193189859390259, D Loss: 1.3131622076034546, alpha: 0.9694095705781761\n",
      "epoch 106 iteration800, G Loss: 1.1525988578796387, D Loss: 1.2025234699249268, alpha: 0.9694095705781761\n",
      "Saving content.\n",
      "epoch 107 iteration0, G Loss: 0.9882322549819946, D Loss: 1.2675371170043945, alpha: 0.968689784418094\n",
      "epoch 107 iteration100, G Loss: 0.8765020370483398, D Loss: 1.1661146879196167, alpha: 0.968689784418094\n",
      "epoch 107 iteration200, G Loss: 0.9517055153846741, D Loss: 1.2026355266571045, alpha: 0.968689784418094\n",
      "epoch 107 iteration300, G Loss: 0.9459631443023682, D Loss: 1.2628997564315796, alpha: 0.968689784418094\n",
      "epoch 107 iteration400, G Loss: 0.9305413365364075, D Loss: 1.213152289390564, alpha: 0.968689784418094\n",
      "epoch 107 iteration500, G Loss: 1.0927883386611938, D Loss: 1.3498167991638184, alpha: 0.968689784418094\n",
      "epoch 107 iteration600, G Loss: 0.8587285280227661, D Loss: 1.2896814346313477, alpha: 0.968689784418094\n",
      "epoch 107 iteration700, G Loss: 0.7763121128082275, D Loss: 1.2237550020217896, alpha: 0.968689784418094\n",
      "epoch 107 iteration800, G Loss: 0.9948777556419373, D Loss: 1.3886326551437378, alpha: 0.968689784418094\n",
      "Saving content.\n",
      "epoch 108 iteration0, G Loss: 0.9631011486053467, D Loss: 1.2827622890472412, alpha: 0.9679536217235628\n",
      "epoch 108 iteration100, G Loss: 0.8211355805397034, D Loss: 1.314133644104004, alpha: 0.9679536217235628\n",
      "epoch 108 iteration200, G Loss: 1.0809590816497803, D Loss: 1.226900577545166, alpha: 0.9679536217235628\n",
      "epoch 108 iteration300, G Loss: 0.9515879154205322, D Loss: 1.1034119129180908, alpha: 0.9679536217235628\n",
      "epoch 108 iteration400, G Loss: 0.6794102191925049, D Loss: 1.31076979637146, alpha: 0.9679536217235628\n",
      "epoch 108 iteration500, G Loss: 1.288590669631958, D Loss: 1.2534987926483154, alpha: 0.9679536217235628\n",
      "epoch 108 iteration600, G Loss: 0.8527543544769287, D Loss: 1.2246781587600708, alpha: 0.9679536217235628\n",
      "epoch 108 iteration700, G Loss: 0.7991452217102051, D Loss: 1.1857707500457764, alpha: 0.9679536217235628\n",
      "epoch 108 iteration800, G Loss: 1.0758322477340698, D Loss: 1.1301674842834473, alpha: 0.9679536217235628\n",
      "Saving content.\n",
      "epoch 109 iteration0, G Loss: 1.3399018049240112, D Loss: 1.2558064460754395, alpha: 0.9672007365029498\n",
      "epoch 109 iteration100, G Loss: 0.7415356636047363, D Loss: 1.1481235027313232, alpha: 0.9672007365029498\n",
      "epoch 109 iteration200, G Loss: 0.645470380783081, D Loss: 1.3419275283813477, alpha: 0.9672007365029498\n",
      "epoch 109 iteration300, G Loss: 1.4382418394088745, D Loss: 1.3153964281082153, alpha: 0.9672007365029498\n",
      "epoch 109 iteration400, G Loss: 1.2184797525405884, D Loss: 1.3520234823226929, alpha: 0.9672007365029498\n",
      "epoch 109 iteration500, G Loss: 1.011066198348999, D Loss: 1.2848529815673828, alpha: 0.9672007365029498\n",
      "epoch 109 iteration600, G Loss: 0.9740433692932129, D Loss: 1.2010111808776855, alpha: 0.9672007365029498\n",
      "epoch 109 iteration700, G Loss: 1.2034778594970703, D Loss: 1.2141156196594238, alpha: 0.9672007365029498\n",
      "epoch 109 iteration800, G Loss: 1.1504220962524414, D Loss: 1.2582037448883057, alpha: 0.9672007365029498\n",
      "Saving content.\n",
      "epoch 110 iteration0, G Loss: 1.2430893182754517, D Loss: 1.2639296054840088, alpha: 0.9664307767185175\n",
      "epoch 110 iteration100, G Loss: 1.2600165605545044, D Loss: 1.1769275665283203, alpha: 0.9664307767185175\n",
      "epoch 110 iteration200, G Loss: 0.9691840410232544, D Loss: 1.2587147951126099, alpha: 0.9664307767185175\n",
      "epoch 110 iteration300, G Loss: 1.1789923906326294, D Loss: 1.1781026124954224, alpha: 0.9664307767185175\n",
      "epoch 110 iteration400, G Loss: 1.4447561502456665, D Loss: 1.3131083250045776, alpha: 0.9664307767185175\n",
      "epoch 110 iteration500, G Loss: 1.132116675376892, D Loss: 1.217111587524414, alpha: 0.9664307767185175\n",
      "epoch 110 iteration600, G Loss: 0.9041478633880615, D Loss: 1.2752615213394165, alpha: 0.9664307767185175\n",
      "epoch 110 iteration700, G Loss: 0.8338112831115723, D Loss: 1.314469337463379, alpha: 0.9664307767185175\n",
      "epoch 110 iteration800, G Loss: 1.007020354270935, D Loss: 1.2422130107879639, alpha: 0.9664307767185175\n",
      "Saving content.\n",
      "epoch 111 iteration0, G Loss: 1.121297836303711, D Loss: 1.1705422401428223, alpha: 0.9656433842422564\n",
      "epoch 111 iteration100, G Loss: 1.0606727600097656, D Loss: 1.1720129251480103, alpha: 0.9656433842422564\n",
      "epoch 111 iteration200, G Loss: 1.2253788709640503, D Loss: 1.2473634481430054, alpha: 0.9656433842422564\n",
      "epoch 111 iteration300, G Loss: 1.0153331756591797, D Loss: 1.2282322645187378, alpha: 0.9656433842422564\n",
      "epoch 111 iteration400, G Loss: 1.0879831314086914, D Loss: 1.239863395690918, alpha: 0.9656433842422564\n",
      "epoch 111 iteration500, G Loss: 1.0484859943389893, D Loss: 1.2140932083129883, alpha: 0.9656433842422564\n",
      "epoch 111 iteration600, G Loss: 1.1828325986862183, D Loss: 1.2134954929351807, alpha: 0.9656433842422564\n",
      "epoch 111 iteration700, G Loss: 0.9999009966850281, D Loss: 1.2246721982955933, alpha: 0.9656433842422564\n",
      "epoch 111 iteration800, G Loss: 1.1030291318893433, D Loss: 1.2713993787765503, alpha: 0.9656433842422564\n",
      "Saving content.\n",
      "epoch 112 iteration0, G Loss: 1.572525978088379, D Loss: 1.4804208278656006, alpha: 0.9648381948147847\n",
      "epoch 112 iteration100, G Loss: 0.6117396354675293, D Loss: 1.3280878067016602, alpha: 0.9648381948147847\n",
      "epoch 112 iteration200, G Loss: 0.8584903478622437, D Loss: 1.2922792434692383, alpha: 0.9648381948147847\n",
      "epoch 112 iteration300, G Loss: 1.326137661933899, D Loss: 1.1891809701919556, alpha: 0.9648381948147847\n",
      "epoch 112 iteration400, G Loss: 0.9507403373718262, D Loss: 1.2152457237243652, alpha: 0.9648381948147847\n",
      "epoch 112 iteration500, G Loss: 1.2391235828399658, D Loss: 1.198866844177246, alpha: 0.9648381948147847\n",
      "epoch 112 iteration600, G Loss: 0.6451845765113831, D Loss: 1.5706945657730103, alpha: 0.9648381948147847\n",
      "epoch 112 iteration700, G Loss: 0.8270543813705444, D Loss: 1.3729920387268066, alpha: 0.9648381948147847\n",
      "epoch 112 iteration800, G Loss: 1.0365632772445679, D Loss: 1.225260615348816, alpha: 0.9648381948147847\n",
      "Saving content.\n",
      "epoch 113 iteration0, G Loss: 0.8835537433624268, D Loss: 1.2388474941253662, alpha: 0.9640148380075328\n",
      "epoch 113 iteration100, G Loss: 1.0299407243728638, D Loss: 1.180655598640442, alpha: 0.9640148380075328\n",
      "epoch 113 iteration200, G Loss: 1.369715690612793, D Loss: 1.6612123250961304, alpha: 0.9640148380075328\n",
      "epoch 113 iteration300, G Loss: 1.2692131996154785, D Loss: 1.3700292110443115, alpha: 0.9640148380075328\n",
      "epoch 113 iteration400, G Loss: 0.9913080334663391, D Loss: 1.1647776365280151, alpha: 0.9640148380075328\n",
      "epoch 113 iteration500, G Loss: 0.5967954397201538, D Loss: 1.3989465236663818, alpha: 0.9640148380075328\n",
      "epoch 113 iteration600, G Loss: 1.2476253509521484, D Loss: 1.4129136800765991, alpha: 0.9640148380075328\n",
      "epoch 113 iteration700, G Loss: 0.912519097328186, D Loss: 1.2478888034820557, alpha: 0.9640148380075328\n",
      "epoch 113 iteration800, G Loss: 0.863874077796936, D Loss: 1.2458858489990234, alpha: 0.9640148380075328\n",
      "Saving content.\n",
      "epoch 114 iteration0, G Loss: 1.0385210514068604, D Loss: 1.1829525232315063, alpha: 0.9631729371884394\n",
      "epoch 114 iteration100, G Loss: 0.9478632211685181, D Loss: 1.24754798412323, alpha: 0.9631729371884394\n",
      "epoch 114 iteration200, G Loss: 1.0531563758850098, D Loss: 1.2519795894622803, alpha: 0.9631729371884394\n",
      "epoch 114 iteration300, G Loss: 0.8335762023925781, D Loss: 1.336496114730835, alpha: 0.9631729371884394\n",
      "epoch 114 iteration400, G Loss: 1.0340701341629028, D Loss: 1.2283668518066406, alpha: 0.9631729371884394\n",
      "epoch 114 iteration500, G Loss: 1.1643123626708984, D Loss: 1.1965807676315308, alpha: 0.9631729371884394\n",
      "epoch 114 iteration600, G Loss: 1.271395206451416, D Loss: 1.1642650365829468, alpha: 0.9631729371884394\n",
      "epoch 114 iteration700, G Loss: 1.1937875747680664, D Loss: 1.2458133697509766, alpha: 0.9631729371884394\n",
      "epoch 114 iteration800, G Loss: 1.3889778852462769, D Loss: 1.3386025428771973, alpha: 0.9631729371884394\n",
      "Saving content.\n",
      "epoch 115 iteration0, G Loss: 1.1411840915679932, D Loss: 1.2035772800445557, alpha: 0.9623121094913941\n",
      "epoch 115 iteration100, G Loss: 1.1694740056991577, D Loss: 1.2410064935684204, alpha: 0.9623121094913941\n",
      "epoch 115 iteration200, G Loss: 1.2661384344100952, D Loss: 1.2453625202178955, alpha: 0.9623121094913941\n",
      "epoch 115 iteration300, G Loss: 0.8394414782524109, D Loss: 1.1436207294464111, alpha: 0.9623121094913941\n",
      "epoch 115 iteration400, G Loss: 0.781893789768219, D Loss: 1.316741704940796, alpha: 0.9623121094913941\n",
      "epoch 115 iteration500, G Loss: 0.9086716771125793, D Loss: 1.2635369300842285, alpha: 0.9623121094913941\n",
      "epoch 115 iteration600, G Loss: 0.6132084131240845, D Loss: 1.3663007020950317, alpha: 0.9623121094913941\n",
      "epoch 115 iteration700, G Loss: 0.9182562828063965, D Loss: 1.210942029953003, alpha: 0.9623121094913941\n",
      "epoch 115 iteration800, G Loss: 1.0418784618377686, D Loss: 1.223524808883667, alpha: 0.9623121094913941\n",
      "Saving content.\n",
      "epoch 116 iteration0, G Loss: 1.0458403825759888, D Loss: 1.1968882083892822, alpha: 0.9614319657896698\n",
      "epoch 116 iteration100, G Loss: 0.5029019117355347, D Loss: 1.4224112033843994, alpha: 0.9614319657896698\n",
      "epoch 116 iteration200, G Loss: 1.301737904548645, D Loss: 1.2232701778411865, alpha: 0.9614319657896698\n",
      "epoch 116 iteration300, G Loss: 1.017504096031189, D Loss: 1.2834978103637695, alpha: 0.9614319657896698\n",
      "epoch 116 iteration400, G Loss: 0.7913120985031128, D Loss: 1.227384090423584, alpha: 0.9614319657896698\n",
      "epoch 116 iteration500, G Loss: 0.9099212884902954, D Loss: 1.3080010414123535, alpha: 0.9614319657896698\n",
      "epoch 116 iteration600, G Loss: 0.9627212285995483, D Loss: 1.252869725227356, alpha: 0.9614319657896698\n",
      "epoch 116 iteration700, G Loss: 1.5397140979766846, D Loss: 1.2800920009613037, alpha: 0.9614319657896698\n",
      "epoch 116 iteration800, G Loss: 0.7782536745071411, D Loss: 1.665865182876587, alpha: 0.9614319657896698\n",
      "Saving content.\n",
      "epoch 117 iteration0, G Loss: 1.4641706943511963, D Loss: 1.3535093069076538, alpha: 0.9605321106735979\n",
      "epoch 117 iteration100, G Loss: 1.0577176809310913, D Loss: 1.2494983673095703, alpha: 0.9605321106735979\n",
      "epoch 117 iteration200, G Loss: 1.0736960172653198, D Loss: 1.3011834621429443, alpha: 0.9605321106735979\n",
      "epoch 117 iteration300, G Loss: 0.8953099846839905, D Loss: 1.2147263288497925, alpha: 0.9605321106735979\n",
      "epoch 117 iteration400, G Loss: 1.0554457902908325, D Loss: 1.2784357070922852, alpha: 0.9605321106735979\n",
      "epoch 117 iteration500, G Loss: 0.9722826480865479, D Loss: 1.1895304918289185, alpha: 0.9605321106735979\n",
      "epoch 117 iteration600, G Loss: 0.7501204609870911, D Loss: 1.2412638664245605, alpha: 0.9605321106735979\n",
      "epoch 117 iteration700, G Loss: 0.8492249250411987, D Loss: 1.2768654823303223, alpha: 0.9605321106735979\n",
      "epoch 117 iteration800, G Loss: 0.8984255790710449, D Loss: 1.3878569602966309, alpha: 0.9605321106735979\n",
      "Saving content.\n",
      "epoch 118 iteration0, G Loss: 0.9596068263053894, D Loss: 1.150099515914917, alpha: 0.959612142432748\n",
      "epoch 118 iteration100, G Loss: 1.0766429901123047, D Loss: 1.186673641204834, alpha: 0.959612142432748\n",
      "epoch 118 iteration200, G Loss: 0.9511508941650391, D Loss: 1.2741992473602295, alpha: 0.959612142432748\n",
      "epoch 118 iteration300, G Loss: 1.1012190580368042, D Loss: 1.238568902015686, alpha: 0.959612142432748\n",
      "epoch 118 iteration400, G Loss: 0.9662625789642334, D Loss: 1.2967994213104248, alpha: 0.959612142432748\n",
      "epoch 118 iteration500, G Loss: 0.8147708773612976, D Loss: 1.2241573333740234, alpha: 0.959612142432748\n",
      "epoch 118 iteration600, G Loss: 0.9066830277442932, D Loss: 1.1798468828201294, alpha: 0.959612142432748\n",
      "epoch 118 iteration700, G Loss: 0.8275320529937744, D Loss: 1.2359014749526978, alpha: 0.959612142432748\n",
      "epoch 118 iteration800, G Loss: 0.7946276664733887, D Loss: 1.2797653675079346, alpha: 0.959612142432748\n",
      "Saving content.\n",
      "epoch 119 iteration0, G Loss: 1.4044184684753418, D Loss: 1.751346230506897, alpha: 0.9586716530428824\n",
      "epoch 119 iteration100, G Loss: 1.2712301015853882, D Loss: 1.1750555038452148, alpha: 0.9586716530428824\n",
      "epoch 119 iteration200, G Loss: 0.9696646928787231, D Loss: 1.2514245510101318, alpha: 0.9586716530428824\n",
      "epoch 119 iteration300, G Loss: 0.7862096428871155, D Loss: 1.3094396591186523, alpha: 0.9586716530428824\n",
      "epoch 119 iteration400, G Loss: 1.430809736251831, D Loss: 1.1549792289733887, alpha: 0.9586716530428824\n",
      "epoch 119 iteration500, G Loss: 0.8823812007904053, D Loss: 1.1928985118865967, alpha: 0.9586716530428824\n",
      "epoch 119 iteration600, G Loss: 1.0422275066375732, D Loss: 1.245361566543579, alpha: 0.9586716530428824\n",
      "epoch 119 iteration700, G Loss: 0.7665984630584717, D Loss: 1.292343258857727, alpha: 0.9586716530428824\n",
      "epoch 119 iteration800, G Loss: 0.9686474800109863, D Loss: 1.1940810680389404, alpha: 0.9586716530428824\n",
      "Saving content.\n",
      "epoch 120 iteration0, G Loss: 1.0030618906021118, D Loss: 1.2634820938110352, alpha: 0.9577102281579662\n",
      "epoch 120 iteration100, G Loss: 1.3136595487594604, D Loss: 1.1599113941192627, alpha: 0.9577102281579662\n",
      "epoch 120 iteration200, G Loss: 1.143841028213501, D Loss: 1.2167861461639404, alpha: 0.9577102281579662\n",
      "epoch 120 iteration300, G Loss: 1.2788374423980713, D Loss: 1.1175861358642578, alpha: 0.9577102281579662\n",
      "epoch 120 iteration400, G Loss: 1.0733822584152222, D Loss: 1.1802984476089478, alpha: 0.9577102281579662\n",
      "epoch 120 iteration500, G Loss: 1.5819616317749023, D Loss: 1.2425180673599243, alpha: 0.9577102281579662\n",
      "epoch 120 iteration600, G Loss: 0.7913509607315063, D Loss: 1.3007653951644897, alpha: 0.9577102281579662\n",
      "epoch 120 iteration700, G Loss: 1.4459773302078247, D Loss: 1.7382416725158691, alpha: 0.9577102281579662\n",
      "epoch 120 iteration800, G Loss: 1.3826463222503662, D Loss: 1.1463855504989624, alpha: 0.9577102281579662\n",
      "Saving content.\n",
      "epoch 121 iteration0, G Loss: 1.165977954864502, D Loss: 1.1525845527648926, alpha: 0.9567274471075219\n",
      "epoch 121 iteration100, G Loss: 1.2434452772140503, D Loss: 1.161816120147705, alpha: 0.9567274471075219\n",
      "epoch 121 iteration200, G Loss: 0.9880818128585815, D Loss: 1.2826462984085083, alpha: 0.9567274471075219\n",
      "epoch 121 iteration300, G Loss: 1.1900368928909302, D Loss: 1.1836810111999512, alpha: 0.9567274471075219\n",
      "epoch 121 iteration400, G Loss: 1.1214364767074585, D Loss: 1.2425669431686401, alpha: 0.9567274471075219\n",
      "epoch 121 iteration500, G Loss: 0.9359566569328308, D Loss: 1.139714241027832, alpha: 0.9567274471075219\n",
      "epoch 121 iteration600, G Loss: 1.3189020156860352, D Loss: 1.1171891689300537, alpha: 0.9567274471075219\n",
      "epoch 121 iteration700, G Loss: 1.0483609437942505, D Loss: 1.2653350830078125, alpha: 0.9567274471075219\n",
      "epoch 121 iteration800, G Loss: 1.1877397298812866, D Loss: 1.2243709564208984, alpha: 0.9567274471075219\n",
      "Saving content.\n",
      "epoch 122 iteration0, G Loss: 1.145302653312683, D Loss: 1.2953972816467285, alpha: 0.9557228828996265\n",
      "epoch 122 iteration100, G Loss: 0.9556602239608765, D Loss: 1.2341688871383667, alpha: 0.9557228828996265\n",
      "epoch 122 iteration200, G Loss: 1.2819076776504517, D Loss: 1.2725939750671387, alpha: 0.9557228828996265\n",
      "epoch 122 iteration300, G Loss: 0.7912116646766663, D Loss: 1.2326403856277466, alpha: 0.9557228828996265\n",
      "epoch 122 iteration400, G Loss: 1.2285535335540771, D Loss: 1.316659927368164, alpha: 0.9557228828996265\n",
      "epoch 122 iteration500, G Loss: 1.047789216041565, D Loss: 1.1920146942138672, alpha: 0.9557228828996265\n",
      "epoch 122 iteration600, G Loss: 0.8207528591156006, D Loss: 1.2102341651916504, alpha: 0.9557228828996265\n",
      "epoch 122 iteration700, G Loss: 1.075594425201416, D Loss: 1.203536033630371, alpha: 0.9557228828996265\n",
      "epoch 122 iteration800, G Loss: 1.1640729904174805, D Loss: 1.2916377782821655, alpha: 0.9557228828996265\n",
      "Saving content.\n",
      "epoch 123 iteration0, G Loss: 1.2515569925308228, D Loss: 1.1627256870269775, alpha: 0.9546961022298595\n",
      "epoch 123 iteration100, G Loss: 0.9805780649185181, D Loss: 1.1480146646499634, alpha: 0.9546961022298595\n",
      "epoch 123 iteration200, G Loss: 0.8418874740600586, D Loss: 1.2077974081039429, alpha: 0.9546961022298595\n",
      "epoch 123 iteration300, G Loss: 0.7062351703643799, D Loss: 1.4704785346984863, alpha: 0.9546961022298595\n",
      "epoch 123 iteration400, G Loss: 0.9731722474098206, D Loss: 1.3265584707260132, alpha: 0.9546961022298595\n",
      "epoch 123 iteration500, G Loss: 1.11542546749115, D Loss: 1.2090122699737549, alpha: 0.9546961022298595\n",
      "epoch 123 iteration600, G Loss: 0.7931593656539917, D Loss: 1.348490595817566, alpha: 0.9546961022298595\n",
      "epoch 123 iteration700, G Loss: 0.9204541444778442, D Loss: 1.1722787618637085, alpha: 0.9546961022298595\n",
      "epoch 123 iteration800, G Loss: 1.0536515712738037, D Loss: 1.2515870332717896, alpha: 0.9546961022298595\n",
      "Saving content.\n",
      "epoch 124 iteration0, G Loss: 0.8701629638671875, D Loss: 1.2089977264404297, alpha: 0.9536466654965192\n",
      "epoch 124 iteration100, G Loss: 0.8752130270004272, D Loss: 1.1835178136825562, alpha: 0.9536466654965192\n",
      "epoch 124 iteration200, G Loss: 0.838681697845459, D Loss: 1.2415356636047363, alpha: 0.9536466654965192\n",
      "epoch 124 iteration300, G Loss: 0.9195418357849121, D Loss: 1.2294970750808716, alpha: 0.9536466654965192\n",
      "epoch 124 iteration400, G Loss: 1.4307150840759277, D Loss: 1.434759497642517, alpha: 0.9536466654965192\n",
      "epoch 124 iteration500, G Loss: 0.9243687391281128, D Loss: 1.3076419830322266, alpha: 0.9536466654965192\n",
      "epoch 124 iteration600, G Loss: 1.0169175863265991, D Loss: 1.2930771112442017, alpha: 0.9536466654965192\n",
      "epoch 124 iteration700, G Loss: 1.0569813251495361, D Loss: 1.264657735824585, alpha: 0.9536466654965192\n",
      "epoch 124 iteration800, G Loss: 1.1901248693466187, D Loss: 1.4295870065689087, alpha: 0.9536466654965192\n",
      "Saving content.\n",
      "epoch 125 iteration0, G Loss: 1.236268401145935, D Loss: 1.209689736366272, alpha: 0.9525741268224333\n",
      "epoch 125 iteration100, G Loss: 1.2542047500610352, D Loss: 1.1255650520324707, alpha: 0.9525741268224333\n",
      "epoch 125 iteration200, G Loss: 1.3758476972579956, D Loss: 1.246835708618164, alpha: 0.9525741268224333\n",
      "epoch 125 iteration300, G Loss: 0.9737682342529297, D Loss: 1.165048599243164, alpha: 0.9525741268224333\n",
      "epoch 125 iteration400, G Loss: 0.9483338594436646, D Loss: 1.1914892196655273, alpha: 0.9525741268224333\n",
      "epoch 125 iteration500, G Loss: 1.1536730527877808, D Loss: 1.2031035423278809, alpha: 0.9525741268224333\n",
      "epoch 125 iteration600, G Loss: 1.2234476804733276, D Loss: 1.2050590515136719, alpha: 0.9525741268224333\n",
      "epoch 125 iteration700, G Loss: 1.3037501573562622, D Loss: 1.3643887042999268, alpha: 0.9525741268224333\n",
      "epoch 125 iteration800, G Loss: 0.8593181371688843, D Loss: 1.2677654027938843, alpha: 0.9525741268224333\n",
      "Saving content.\n",
      "epoch 126 iteration0, G Loss: 0.9699414968490601, D Loss: 1.1391863822937012, alpha: 0.9514780340836981\n",
      "epoch 126 iteration100, G Loss: 0.7633827924728394, D Loss: 1.246867060661316, alpha: 0.9514780340836981\n",
      "epoch 126 iteration200, G Loss: 1.2577669620513916, D Loss: 1.2761942148208618, alpha: 0.9514780340836981\n",
      "epoch 126 iteration300, G Loss: 1.361706018447876, D Loss: 1.3647022247314453, alpha: 0.9514780340836981\n",
      "epoch 126 iteration400, G Loss: 1.0885560512542725, D Loss: 1.2717125415802002, alpha: 0.9514780340836981\n",
      "epoch 126 iteration500, G Loss: 1.2022525072097778, D Loss: 1.3031072616577148, alpha: 0.9514780340836981\n",
      "epoch 126 iteration600, G Loss: 0.9263451099395752, D Loss: 1.1463418006896973, alpha: 0.9514780340836981\n",
      "epoch 126 iteration700, G Loss: 2.2731359004974365, D Loss: 1.3104896545410156, alpha: 0.9514780340836981\n",
      "epoch 126 iteration800, G Loss: 1.561748743057251, D Loss: 1.317939043045044, alpha: 0.9514780340836981\n",
      "Saving content.\n",
      "epoch 127 iteration0, G Loss: 1.1138300895690918, D Loss: 1.1626927852630615, alpha: 0.9503579289456944\n",
      "epoch 127 iteration100, G Loss: 1.2147599458694458, D Loss: 1.1944870948791504, alpha: 0.9503579289456944\n",
      "epoch 127 iteration200, G Loss: 1.0190752744674683, D Loss: 1.4086905717849731, alpha: 0.9503579289456944\n",
      "epoch 127 iteration300, G Loss: 0.8646903038024902, D Loss: 1.2075550556182861, alpha: 0.9503579289456944\n",
      "epoch 127 iteration400, G Loss: 0.5661380290985107, D Loss: 1.2952213287353516, alpha: 0.9503579289456944\n",
      "epoch 127 iteration500, G Loss: 1.3079249858856201, D Loss: 1.2001017332077026, alpha: 0.9503579289456944\n",
      "epoch 127 iteration600, G Loss: 0.9833985567092896, D Loss: 1.284971833229065, alpha: 0.9503579289456944\n",
      "epoch 127 iteration700, G Loss: 1.0708540678024292, D Loss: 1.2389839887619019, alpha: 0.9503579289456944\n",
      "epoch 127 iteration800, G Loss: 0.9582339525222778, D Loss: 1.2100818157196045, alpha: 0.9503579289456944\n",
      "Saving content.\n",
      "epoch 128 iteration0, G Loss: 0.6216490268707275, D Loss: 1.4185160398483276, alpha: 0.9492133469067291\n",
      "epoch 128 iteration100, G Loss: 1.0098884105682373, D Loss: 1.2895746231079102, alpha: 0.9492133469067291\n",
      "epoch 128 iteration200, G Loss: 1.2055509090423584, D Loss: 1.1792125701904297, alpha: 0.9492133469067291\n",
      "epoch 128 iteration300, G Loss: 1.1889420747756958, D Loss: 1.1199711561203003, alpha: 0.9492133469067291\n",
      "epoch 128 iteration400, G Loss: 1.1079472303390503, D Loss: 1.2634708881378174, alpha: 0.9492133469067291\n",
      "epoch 128 iteration500, G Loss: 0.8919624090194702, D Loss: 1.2501014471054077, alpha: 0.9492133469067291\n",
      "epoch 128 iteration600, G Loss: 1.2126147747039795, D Loss: 1.3769152164459229, alpha: 0.9492133469067291\n",
      "epoch 128 iteration700, G Loss: 0.9917439222335815, D Loss: 1.3100109100341797, alpha: 0.9492133469067291\n",
      "epoch 128 iteration800, G Loss: 1.1748933792114258, D Loss: 1.22075355052948, alpha: 0.9492133469067291\n",
      "Saving content.\n",
      "epoch 129 iteration0, G Loss: 0.8990070819854736, D Loss: 1.2220009565353394, alpha: 0.9480438173496692\n",
      "epoch 129 iteration100, G Loss: 1.7237507104873657, D Loss: 1.3697309494018555, alpha: 0.9480438173496692\n",
      "epoch 129 iteration200, G Loss: 1.0339969396591187, D Loss: 1.1378209590911865, alpha: 0.9480438173496692\n",
      "epoch 129 iteration300, G Loss: 0.9729154706001282, D Loss: 1.338870882987976, alpha: 0.9480438173496692\n",
      "epoch 129 iteration400, G Loss: 1.1499533653259277, D Loss: 1.2544500827789307, alpha: 0.9480438173496692\n",
      "epoch 129 iteration500, G Loss: 1.1678049564361572, D Loss: 1.2203872203826904, alpha: 0.9480438173496692\n",
      "epoch 129 iteration600, G Loss: 1.8028690814971924, D Loss: 1.348607063293457, alpha: 0.9480438173496692\n",
      "epoch 129 iteration700, G Loss: 1.1450285911560059, D Loss: 1.2602250576019287, alpha: 0.9480438173496692\n",
      "epoch 129 iteration800, G Loss: 1.254011869430542, D Loss: 1.0894429683685303, alpha: 0.9480438173496692\n",
      "Saving content.\n",
      "epoch 130 iteration0, G Loss: 1.06986665725708, D Loss: 1.1687310934066772, alpha: 0.9468488636019363\n",
      "epoch 130 iteration100, G Loss: 1.1033086776733398, D Loss: 1.1784250736236572, alpha: 0.9468488636019363\n",
      "epoch 130 iteration200, G Loss: 0.9946548342704773, D Loss: 1.1767805814743042, alpha: 0.9468488636019363\n",
      "epoch 130 iteration300, G Loss: 1.2345733642578125, D Loss: 1.2788872718811035, alpha: 0.9468488636019363\n",
      "epoch 130 iteration400, G Loss: 1.2385400533676147, D Loss: 1.290787696838379, alpha: 0.9468488636019363\n",
      "epoch 130 iteration500, G Loss: 0.9649800062179565, D Loss: 1.1372802257537842, alpha: 0.9468488636019363\n",
      "epoch 130 iteration600, G Loss: 0.8764305710792542, D Loss: 1.3388290405273438, alpha: 0.9468488636019363\n",
      "epoch 130 iteration700, G Loss: 1.6842135190963745, D Loss: 1.2857186794281006, alpha: 0.9468488636019363\n",
      "epoch 130 iteration800, G Loss: 1.1000661849975586, D Loss: 1.3087446689605713, alpha: 0.9468488636019363\n",
      "Saving content.\n",
      "epoch 131 iteration0, G Loss: 0.9932492971420288, D Loss: 1.272367000579834, alpha: 0.9456280030042419\n",
      "epoch 131 iteration100, G Loss: 1.3114184141159058, D Loss: 1.2439053058624268, alpha: 0.9456280030042419\n",
      "epoch 131 iteration200, G Loss: 1.3865635395050049, D Loss: 1.339133858680725, alpha: 0.9456280030042419\n",
      "epoch 131 iteration300, G Loss: 1.032204270362854, D Loss: 1.1920390129089355, alpha: 0.9456280030042419\n",
      "epoch 131 iteration400, G Loss: 0.8853346109390259, D Loss: 1.2196258306503296, alpha: 0.9456280030042419\n",
      "epoch 131 iteration500, G Loss: 0.911370038986206, D Loss: 1.3234484195709229, alpha: 0.9456280030042419\n",
      "epoch 131 iteration600, G Loss: 1.0853755474090576, D Loss: 1.2625041007995605, alpha: 0.9456280030042419\n",
      "epoch 131 iteration700, G Loss: 0.9736772775650024, D Loss: 1.33432936668396, alpha: 0.9456280030042419\n",
      "epoch 131 iteration800, G Loss: 1.1340885162353516, D Loss: 1.1888678073883057, alpha: 0.9456280030042419\n",
      "Saving content.\n",
      "epoch 132 iteration0, G Loss: 0.8818886280059814, D Loss: 1.1696884632110596, alpha: 0.9443807469884519\n",
      "epoch 132 iteration100, G Loss: 0.9088064432144165, D Loss: 1.359065294265747, alpha: 0.9443807469884519\n",
      "epoch 132 iteration200, G Loss: 0.8417408466339111, D Loss: 1.249298095703125, alpha: 0.9443807469884519\n",
      "epoch 132 iteration300, G Loss: 1.0722943544387817, D Loss: 1.172119140625, alpha: 0.9443807469884519\n",
      "epoch 132 iteration400, G Loss: 0.9142075777053833, D Loss: 1.209437370300293, alpha: 0.9443807469884519\n",
      "epoch 132 iteration500, G Loss: 1.252646565437317, D Loss: 1.1736435890197754, alpha: 0.9443807469884519\n",
      "epoch 132 iteration600, G Loss: 0.8659255504608154, D Loss: 1.1678028106689453, alpha: 0.9443807469884519\n",
      "epoch 132 iteration700, G Loss: 0.6407641768455505, D Loss: 1.2630099058151245, alpha: 0.9443807469884519\n",
      "epoch 132 iteration800, G Loss: 1.4570298194885254, D Loss: 1.1038730144500732, alpha: 0.9443807469884519\n",
      "Saving content.\n",
      "epoch 133 iteration0, G Loss: 1.7688359022140503, D Loss: 1.2761075496673584, alpha: 0.9431066011649731\n",
      "epoch 133 iteration100, G Loss: 0.847598135471344, D Loss: 1.187414288520813, alpha: 0.9431066011649731\n",
      "epoch 133 iteration200, G Loss: 0.9833836555480957, D Loss: 1.4823763370513916, alpha: 0.9431066011649731\n",
      "epoch 133 iteration300, G Loss: 0.9907156229019165, D Loss: 1.3530058860778809, alpha: 0.9431066011649731\n",
      "epoch 133 iteration400, G Loss: 1.0349147319793701, D Loss: 1.266016960144043, alpha: 0.9431066011649731\n",
      "epoch 133 iteration500, G Loss: 1.0493711233139038, D Loss: 1.2722880840301514, alpha: 0.9431066011649731\n",
      "epoch 133 iteration600, G Loss: 0.6861352920532227, D Loss: 1.2123253345489502, alpha: 0.9431066011649731\n",
      "epoch 133 iteration700, G Loss: 0.9925296902656555, D Loss: 1.1634849309921265, alpha: 0.9431066011649731\n",
      "epoch 133 iteration800, G Loss: 0.8997629880905151, D Loss: 1.2506271600723267, alpha: 0.9431066011649731\n",
      "Saving content.\n",
      "epoch 134 iteration0, G Loss: 0.7255178689956665, D Loss: 1.2895632982254028, alpha: 0.9418050654200673\n",
      "epoch 134 iteration100, G Loss: 1.1551700830459595, D Loss: 1.21712327003479, alpha: 0.9418050654200673\n",
      "epoch 134 iteration200, G Loss: 1.1464343070983887, D Loss: 1.1539822816848755, alpha: 0.9418050654200673\n",
      "epoch 134 iteration300, G Loss: 0.8476958274841309, D Loss: 1.261674404144287, alpha: 0.9418050654200673\n",
      "epoch 134 iteration400, G Loss: 0.8601866960525513, D Loss: 1.2097798585891724, alpha: 0.9418050654200673\n",
      "epoch 134 iteration500, G Loss: 0.996887743473053, D Loss: 1.2677125930786133, alpha: 0.9418050654200673\n",
      "epoch 134 iteration600, G Loss: 1.0018649101257324, D Loss: 1.3438794612884521, alpha: 0.9418050654200673\n",
      "epoch 134 iteration700, G Loss: 1.050758957862854, D Loss: 1.2821396589279175, alpha: 0.9418050654200673\n",
      "epoch 134 iteration800, G Loss: 0.9455252885818481, D Loss: 1.2858200073242188, alpha: 0.9418050654200673\n",
      "Saving content.\n",
      "epoch 135 iteration0, G Loss: 1.1807856559753418, D Loss: 1.1983017921447754, alpha: 0.9404756340234984\n",
      "epoch 135 iteration100, G Loss: 1.2972747087478638, D Loss: 1.1833348274230957, alpha: 0.9404756340234984\n",
      "epoch 135 iteration200, G Loss: 1.2439141273498535, D Loss: 1.3206161260604858, alpha: 0.9404756340234984\n",
      "epoch 135 iteration300, G Loss: 0.8770312070846558, D Loss: 1.2359857559204102, alpha: 0.9404756340234984\n",
      "epoch 135 iteration400, G Loss: 1.0989233255386353, D Loss: 1.2207071781158447, alpha: 0.9404756340234984\n",
      "epoch 135 iteration500, G Loss: 0.972597599029541, D Loss: 1.222672700881958, alpha: 0.9404756340234984\n",
      "epoch 135 iteration600, G Loss: 0.8296334743499756, D Loss: 1.1910793781280518, alpha: 0.9404756340234984\n",
      "epoch 135 iteration700, G Loss: 0.8946789503097534, D Loss: 1.2640758752822876, alpha: 0.9404756340234984\n",
      "epoch 135 iteration800, G Loss: 1.05143141746521, D Loss: 1.2508213520050049, alpha: 0.9404756340234984\n",
      "Saving content.\n",
      "epoch 136 iteration0, G Loss: 0.6997736692428589, D Loss: 1.311479091644287, alpha: 0.9391177957469332\n",
      "epoch 136 iteration100, G Loss: 1.2985732555389404, D Loss: 1.2406123876571655, alpha: 0.9391177957469332\n",
      "epoch 136 iteration200, G Loss: 1.6772692203521729, D Loss: 1.2346603870391846, alpha: 0.9391177957469332\n",
      "epoch 136 iteration300, G Loss: 1.0943892002105713, D Loss: 1.411510705947876, alpha: 0.9391177957469332\n",
      "epoch 136 iteration400, G Loss: 1.0711815357208252, D Loss: 1.308408260345459, alpha: 0.9391177957469332\n",
      "epoch 136 iteration500, G Loss: 1.2215652465820312, D Loss: 1.1579803228378296, alpha: 0.9391177957469332\n",
      "epoch 136 iteration600, G Loss: 1.0468811988830566, D Loss: 1.2316577434539795, alpha: 0.9391177957469332\n",
      "epoch 136 iteration700, G Loss: 1.2655284404754639, D Loss: 1.3197882175445557, alpha: 0.9391177957469332\n",
      "epoch 136 iteration800, G Loss: 1.2309414148330688, D Loss: 1.2167084217071533, alpha: 0.9391177957469332\n",
      "Saving content.\n",
      "epoch 137 iteration0, G Loss: 1.0322388410568237, D Loss: 1.375810146331787, alpha: 0.9377310339935125\n",
      "epoch 137 iteration100, G Loss: 0.8913602232933044, D Loss: 1.2619283199310303, alpha: 0.9377310339935125\n",
      "epoch 137 iteration200, G Loss: 0.9968531131744385, D Loss: 1.2666795253753662, alpha: 0.9377310339935125\n",
      "epoch 137 iteration300, G Loss: 0.8363416194915771, D Loss: 1.3070135116577148, alpha: 0.9377310339935125\n",
      "epoch 137 iteration400, G Loss: 0.7964937686920166, D Loss: 1.2301791906356812, alpha: 0.9377310339935125\n",
      "epoch 137 iteration500, G Loss: 0.8720139861106873, D Loss: 1.2647961378097534, alpha: 0.9377310339935125\n",
      "epoch 137 iteration600, G Loss: 0.9305946826934814, D Loss: 1.2088767290115356, alpha: 0.9377310339935125\n",
      "epoch 137 iteration700, G Loss: 0.8945009708404541, D Loss: 1.2279502153396606, alpha: 0.9377310339935125\n",
      "epoch 137 iteration800, G Loss: 0.7019717693328857, D Loss: 1.3046566247940063, alpha: 0.9377310339935125\n",
      "Saving content.\n",
      "epoch 138 iteration0, G Loss: 0.8052408695220947, D Loss: 1.3304648399353027, alpha: 0.9363148269390238\n",
      "epoch 138 iteration100, G Loss: 0.8746703863143921, D Loss: 1.2388991117477417, alpha: 0.9363148269390238\n",
      "epoch 138 iteration200, G Loss: 0.8409183621406555, D Loss: 1.2410705089569092, alpha: 0.9363148269390238\n",
      "epoch 138 iteration300, G Loss: 0.9088740348815918, D Loss: 1.233407735824585, alpha: 0.9363148269390238\n",
      "epoch 138 iteration400, G Loss: 0.8658730387687683, D Loss: 1.0961277484893799, alpha: 0.9363148269390238\n",
      "epoch 138 iteration500, G Loss: 1.0965973138809204, D Loss: 1.3127756118774414, alpha: 0.9363148269390238\n",
      "epoch 138 iteration600, G Loss: 0.8893008232116699, D Loss: 1.2191239595413208, alpha: 0.9363148269390238\n",
      "epoch 138 iteration700, G Loss: 1.247252106666565, D Loss: 1.217315435409546, alpha: 0.9363148269390238\n",
      "epoch 138 iteration800, G Loss: 0.9855769276618958, D Loss: 1.1866523027420044, alpha: 0.9363148269390238\n",
      "Saving content.\n",
      "epoch 139 iteration0, G Loss: 0.9727926254272461, D Loss: 1.2782871723175049, alpha: 0.934868647685104\n",
      "epoch 139 iteration100, G Loss: 0.6625898480415344, D Loss: 1.2989389896392822, alpha: 0.934868647685104\n",
      "epoch 139 iteration200, G Loss: 0.7311503887176514, D Loss: 1.1772818565368652, alpha: 0.934868647685104\n",
      "epoch 139 iteration300, G Loss: 1.5215123891830444, D Loss: 1.2151296138763428, alpha: 0.934868647685104\n",
      "epoch 139 iteration400, G Loss: 1.148362398147583, D Loss: 1.3046081066131592, alpha: 0.934868647685104\n",
      "epoch 139 iteration500, G Loss: 1.1170952320098877, D Loss: 1.1192383766174316, alpha: 0.934868647685104\n",
      "epoch 139 iteration600, G Loss: 0.8903874158859253, D Loss: 1.2446892261505127, alpha: 0.934868647685104\n",
      "epoch 139 iteration700, G Loss: 1.231783390045166, D Loss: 1.3536639213562012, alpha: 0.934868647685104\n",
      "epoch 139 iteration800, G Loss: 1.3331102132797241, D Loss: 1.2458405494689941, alpha: 0.934868647685104\n",
      "Saving content.\n",
      "epoch 140 iteration0, G Loss: 1.1132159233093262, D Loss: 1.2286615371704102, alpha: 0.9333919644249093\n",
      "epoch 140 iteration100, G Loss: 1.0666940212249756, D Loss: 1.3236746788024902, alpha: 0.9333919644249093\n",
      "epoch 140 iteration200, G Loss: 1.0959372520446777, D Loss: 1.1830453872680664, alpha: 0.9333919644249093\n",
      "epoch 140 iteration300, G Loss: 1.2220255136489868, D Loss: 1.2366769313812256, alpha: 0.9333919644249093\n",
      "epoch 140 iteration400, G Loss: 1.2577760219573975, D Loss: 1.1539547443389893, alpha: 0.9333919644249093\n",
      "epoch 140 iteration500, G Loss: 1.2431880235671997, D Loss: 1.145078182220459, alpha: 0.9333919644249093\n",
      "epoch 140 iteration600, G Loss: 0.9084296226501465, D Loss: 1.3046152591705322, alpha: 0.9333919644249093\n",
      "epoch 140 iteration700, G Loss: 1.3267673254013062, D Loss: 1.2665021419525146, alpha: 0.9333919644249093\n",
      "epoch 140 iteration800, G Loss: 1.478329062461853, D Loss: 1.262696385383606, alpha: 0.9333919644249093\n",
      "Saving content.\n",
      "epoch 141 iteration0, G Loss: 1.2251805067062378, D Loss: 1.2553329467773438, alpha: 0.9318842406216898\n",
      "epoch 141 iteration100, G Loss: 0.9679523706436157, D Loss: 1.183171033859253, alpha: 0.9318842406216898\n",
      "epoch 141 iteration200, G Loss: 1.2543762922286987, D Loss: 1.2470567226409912, alpha: 0.9318842406216898\n",
      "epoch 141 iteration300, G Loss: 0.9852932691574097, D Loss: 1.2943933010101318, alpha: 0.9318842406216898\n",
      "epoch 141 iteration400, G Loss: 1.2541955709457397, D Loss: 1.1850214004516602, alpha: 0.9318842406216898\n",
      "epoch 141 iteration500, G Loss: 1.0017486810684204, D Loss: 1.2530298233032227, alpha: 0.9318842406216898\n",
      "epoch 141 iteration600, G Loss: 0.9965986013412476, D Loss: 1.2853448390960693, alpha: 0.9318842406216898\n",
      "epoch 141 iteration700, G Loss: 0.9809920191764832, D Loss: 1.2283120155334473, alpha: 0.9318842406216898\n",
      "epoch 141 iteration800, G Loss: 1.2605899572372437, D Loss: 1.1705734729766846, alpha: 0.9318842406216898\n",
      "Saving content.\n",
      "epoch 142 iteration0, G Loss: 1.1480047702789307, D Loss: 1.224326729774475, alpha: 0.9303449352007099\n",
      "epoch 142 iteration100, G Loss: 1.0025147199630737, D Loss: 1.2396372556686401, alpha: 0.9303449352007099\n",
      "epoch 142 iteration200, G Loss: 0.8405030369758606, D Loss: 1.191082239151001, alpha: 0.9303449352007099\n",
      "epoch 142 iteration300, G Loss: 1.0804909467697144, D Loss: 1.3148198127746582, alpha: 0.9303449352007099\n",
      "epoch 142 iteration400, G Loss: 0.8138571977615356, D Loss: 1.2295317649841309, alpha: 0.9303449352007099\n",
      "epoch 142 iteration500, G Loss: 1.1060709953308105, D Loss: 1.38639235496521, alpha: 0.9303449352007099\n",
      "epoch 142 iteration600, G Loss: 0.5406036376953125, D Loss: 1.3107348680496216, alpha: 0.9303449352007099\n",
      "epoch 142 iteration700, G Loss: 0.8711676001548767, D Loss: 1.1715184450149536, alpha: 0.9303449352007099\n",
      "epoch 142 iteration800, G Loss: 1.3960884809494019, D Loss: 1.2095283269882202, alpha: 0.9303449352007099\n",
      "Saving content.\n",
      "epoch 143 iteration0, G Loss: 0.9802910089492798, D Loss: 1.0984671115875244, alpha: 0.9287735027549558\n",
      "epoch 143 iteration100, G Loss: 0.8169631958007812, D Loss: 1.373498797416687, alpha: 0.9287735027549558\n",
      "epoch 143 iteration200, G Loss: 0.9414548277854919, D Loss: 1.247504711151123, alpha: 0.9287735027549558\n",
      "epoch 143 iteration300, G Loss: 0.9506239295005798, D Loss: 1.1968821287155151, alpha: 0.9287735027549558\n",
      "epoch 143 iteration400, G Loss: 0.8197081685066223, D Loss: 1.2677652835845947, alpha: 0.9287735027549558\n",
      "epoch 143 iteration500, G Loss: 0.8099657893180847, D Loss: 1.3518617153167725, alpha: 0.9287735027549558\n",
      "epoch 143 iteration600, G Loss: 1.3808281421661377, D Loss: 1.2151411771774292, alpha: 0.9287735027549558\n",
      "epoch 143 iteration700, G Loss: 1.4705922603607178, D Loss: 1.3452998399734497, alpha: 0.9287735027549558\n",
      "epoch 143 iteration800, G Loss: 0.8025240898132324, D Loss: 1.2418352365493774, alpha: 0.9287735027549558\n",
      "Saving content.\n",
      "epoch 144 iteration0, G Loss: 0.9924715757369995, D Loss: 1.235964059829712, alpha: 0.9271693937650729\n",
      "epoch 144 iteration100, G Loss: 1.0857093334197998, D Loss: 1.2738149166107178, alpha: 0.9271693937650729\n",
      "epoch 144 iteration200, G Loss: 1.1182610988616943, D Loss: 1.2241294384002686, alpha: 0.9271693937650729\n",
      "epoch 144 iteration300, G Loss: 0.9547004103660583, D Loss: 1.2634289264678955, alpha: 0.9271693937650729\n",
      "epoch 144 iteration400, G Loss: 1.2409714460372925, D Loss: 1.3249660730361938, alpha: 0.9271693937650729\n",
      "epoch 144 iteration500, G Loss: 0.8697001338005066, D Loss: 1.271047592163086, alpha: 0.9271693937650729\n",
      "epoch 144 iteration600, G Loss: 1.1053721904754639, D Loss: 1.271864891052246, alpha: 0.9271693937650729\n",
      "epoch 144 iteration700, G Loss: 1.1079343557357788, D Loss: 1.2548108100891113, alpha: 0.9271693937650729\n",
      "epoch 144 iteration800, G Loss: 1.0413280725479126, D Loss: 1.2140731811523438, alpha: 0.9271693937650729\n",
      "Saving content.\n",
      "epoch 145 iteration0, G Loss: 0.9286790490150452, D Loss: 1.2165170907974243, alpha: 0.9255320548339719\n",
      "epoch 145 iteration100, G Loss: 1.761418342590332, D Loss: 1.2088308334350586, alpha: 0.9255320548339719\n",
      "epoch 145 iteration200, G Loss: 0.8914238214492798, D Loss: 1.2171770334243774, alpha: 0.9255320548339719\n",
      "epoch 145 iteration300, G Loss: 1.1693456172943115, D Loss: 1.1893647909164429, alpha: 0.9255320548339719\n",
      "epoch 145 iteration400, G Loss: 1.3287655115127563, D Loss: 1.2543890476226807, alpha: 0.9255320548339719\n",
      "epoch 145 iteration500, G Loss: 0.8991436958312988, D Loss: 1.2143313884735107, alpha: 0.9255320548339719\n",
      "epoch 145 iteration600, G Loss: 1.81635320186615, D Loss: 1.3579258918762207, alpha: 0.9255320548339719\n",
      "epoch 145 iteration700, G Loss: 1.067776083946228, D Loss: 1.1787993907928467, alpha: 0.9255320548339719\n",
      "epoch 145 iteration800, G Loss: 1.3465077877044678, D Loss: 1.2657116651535034, alpha: 0.9255320548339719\n",
      "Saving content.\n",
      "epoch 146 iteration0, G Loss: 0.84283447265625, D Loss: 1.248854398727417, alpha: 0.9238609289365459\n",
      "epoch 146 iteration100, G Loss: 1.0068657398223877, D Loss: 1.1438120603561401, alpha: 0.9238609289365459\n",
      "epoch 146 iteration200, G Loss: 1.2506486177444458, D Loss: 1.5423367023468018, alpha: 0.9238609289365459\n",
      "epoch 146 iteration300, G Loss: 0.8943902850151062, D Loss: 1.1743254661560059, alpha: 0.9238609289365459\n",
      "epoch 146 iteration400, G Loss: 1.1409822702407837, D Loss: 1.2934354543685913, alpha: 0.9238609289365459\n",
      "epoch 146 iteration500, G Loss: 1.0269756317138672, D Loss: 1.2203375101089478, alpha: 0.9238609289365459\n",
      "epoch 146 iteration600, G Loss: 0.8907127380371094, D Loss: 1.3114620447158813, alpha: 0.9238609289365459\n",
      "epoch 146 iteration700, G Loss: 0.8996701240539551, D Loss: 1.2852977514266968, alpha: 0.9238609289365459\n",
      "epoch 146 iteration800, G Loss: 1.6295759677886963, D Loss: 1.1793633699417114, alpha: 0.9238609289365459\n",
      "Saving content.\n",
      "epoch 147 iteration0, G Loss: 1.255980134010315, D Loss: 1.2271959781646729, alpha: 0.92215545568493\n",
      "epoch 147 iteration100, G Loss: 0.8456541895866394, D Loss: 1.2685312032699585, alpha: 0.92215545568493\n",
      "epoch 147 iteration200, G Loss: 1.139634370803833, D Loss: 1.173223614692688, alpha: 0.92215545568493\n",
      "epoch 147 iteration300, G Loss: 1.015809178352356, D Loss: 1.2631969451904297, alpha: 0.92215545568493\n",
      "epoch 147 iteration400, G Loss: 0.8685064911842346, D Loss: 1.2542665004730225, alpha: 0.92215545568493\n",
      "epoch 147 iteration500, G Loss: 1.2369074821472168, D Loss: 1.2630236148834229, alpha: 0.92215545568493\n",
      "epoch 147 iteration600, G Loss: 0.9157548546791077, D Loss: 1.254999041557312, alpha: 0.92215545568493\n",
      "epoch 147 iteration700, G Loss: 0.9981911778450012, D Loss: 1.217098593711853, alpha: 0.92215545568493\n",
      "epoch 147 iteration800, G Loss: 0.8925604820251465, D Loss: 1.2618024349212646, alpha: 0.92215545568493\n",
      "Saving content.\n",
      "epoch 148 iteration0, G Loss: 0.9219518899917603, D Loss: 1.1374235153198242, alpha: 0.9204150716097369\n",
      "epoch 148 iteration100, G Loss: 1.0814908742904663, D Loss: 1.2817797660827637, alpha: 0.9204150716097369\n",
      "epoch 148 iteration200, G Loss: 1.1005027294158936, D Loss: 1.1862846612930298, alpha: 0.9204150716097369\n",
      "epoch 148 iteration300, G Loss: 1.032438039779663, D Loss: 1.2626049518585205, alpha: 0.9204150716097369\n",
      "epoch 148 iteration400, G Loss: 0.895898163318634, D Loss: 1.2218520641326904, alpha: 0.9204150716097369\n",
      "epoch 148 iteration500, G Loss: 0.986544668674469, D Loss: 1.0985101461410522, alpha: 0.9204150716097369\n",
      "epoch 148 iteration600, G Loss: 1.0459141731262207, D Loss: 1.1216602325439453, alpha: 0.9204150716097369\n",
      "epoch 148 iteration700, G Loss: 0.9943901300430298, D Loss: 1.2110435962677002, alpha: 0.9204150716097369\n",
      "epoch 148 iteration800, G Loss: 0.7962672710418701, D Loss: 1.1204389333724976, alpha: 0.9204150716097369\n",
      "Saving content.\n",
      "epoch 149 iteration0, G Loss: 1.0198237895965576, D Loss: 1.2280949354171753, alpha: 0.918639210457691\n",
      "epoch 149 iteration100, G Loss: 1.0817556381225586, D Loss: 1.2056148052215576, alpha: 0.918639210457691\n",
      "epoch 149 iteration200, G Loss: 1.1063380241394043, D Loss: 1.2388055324554443, alpha: 0.918639210457691\n",
      "epoch 149 iteration300, G Loss: 0.9824164509773254, D Loss: 1.2553566694259644, alpha: 0.918639210457691\n",
      "epoch 149 iteration400, G Loss: 0.7738917469978333, D Loss: 1.3691433668136597, alpha: 0.918639210457691\n",
      "epoch 149 iteration500, G Loss: 0.9878143668174744, D Loss: 1.2373448610305786, alpha: 0.918639210457691\n",
      "epoch 149 iteration600, G Loss: 1.1362755298614502, D Loss: 1.3014121055603027, alpha: 0.918639210457691\n",
      "epoch 149 iteration700, G Loss: 0.9341179132461548, D Loss: 1.1494396924972534, alpha: 0.918639210457691\n",
      "epoch 149 iteration800, G Loss: 1.3540085554122925, D Loss: 1.2153211832046509, alpha: 0.918639210457691\n",
      "Saving content.\n",
      "epoch 150 iteration0, G Loss: 1.1578669548034668, D Loss: 1.2950836420059204, alpha: 0.9168273035060777\n",
      "epoch 150 iteration100, G Loss: 0.9819222688674927, D Loss: 1.2331504821777344, alpha: 0.9168273035060777\n",
      "epoch 150 iteration200, G Loss: 1.2784911394119263, D Loss: 1.2770719528198242, alpha: 0.9168273035060777\n",
      "epoch 150 iteration300, G Loss: 1.061588168144226, D Loss: 1.3221144676208496, alpha: 0.9168273035060777\n",
      "epoch 150 iteration400, G Loss: 0.9461145401000977, D Loss: 1.5337069034576416, alpha: 0.9168273035060777\n",
      "epoch 150 iteration500, G Loss: 0.9249774813652039, D Loss: 1.303346872329712, alpha: 0.9168273035060777\n",
      "epoch 150 iteration600, G Loss: 0.928215742111206, D Loss: 1.3468163013458252, alpha: 0.9168273035060777\n",
      "epoch 150 iteration700, G Loss: 1.059053659439087, D Loss: 1.346428632736206, alpha: 0.9168273035060777\n",
      "epoch 150 iteration800, G Loss: 0.9773384928703308, D Loss: 1.2653298377990723, alpha: 0.9168273035060777\n",
      "Saving content.\n",
      "epoch 151 iteration0, G Loss: 1.079969882965088, D Loss: 1.220607042312622, alpha: 0.9149787798944143\n",
      "epoch 151 iteration100, G Loss: 0.72300124168396, D Loss: 1.3535232543945312, alpha: 0.9149787798944143\n",
      "epoch 151 iteration200, G Loss: 1.1026866436004639, D Loss: 1.1935299634933472, alpha: 0.9149787798944143\n",
      "epoch 151 iteration300, G Loss: 1.0166624784469604, D Loss: 1.3611984252929688, alpha: 0.9149787798944143\n",
      "epoch 151 iteration400, G Loss: 1.0951189994812012, D Loss: 1.1672271490097046, alpha: 0.9149787798944143\n",
      "epoch 151 iteration500, G Loss: 1.405653953552246, D Loss: 1.4992352724075317, alpha: 0.9149787798944143\n",
      "epoch 151 iteration600, G Loss: 1.3469140529632568, D Loss: 1.1911380290985107, alpha: 0.9149787798944143\n",
      "epoch 151 iteration700, G Loss: 1.1676095724105835, D Loss: 1.208844780921936, alpha: 0.9149787798944143\n",
      "epoch 151 iteration800, G Loss: 0.7664936184883118, D Loss: 1.1671452522277832, alpha: 0.9149787798944143\n",
      "Saving content.\n",
      "epoch 152 iteration0, G Loss: 1.272641897201538, D Loss: 1.287344217300415, alpha: 0.91309306697374\n",
      "epoch 152 iteration100, G Loss: 1.053255558013916, D Loss: 1.3059213161468506, alpha: 0.91309306697374\n",
      "epoch 152 iteration200, G Loss: 1.1773059368133545, D Loss: 1.268803358078003, alpha: 0.91309306697374\n",
      "epoch 152 iteration300, G Loss: 1.0151413679122925, D Loss: 1.170525312423706, alpha: 0.91309306697374\n",
      "epoch 152 iteration400, G Loss: 0.9846090078353882, D Loss: 1.2478671073913574, alpha: 0.91309306697374\n",
      "epoch 152 iteration500, G Loss: 1.3611727952957153, D Loss: 1.168433427810669, alpha: 0.91309306697374\n",
      "epoch 152 iteration600, G Loss: 0.7432625889778137, D Loss: 1.2387583255767822, alpha: 0.91309306697374\n",
      "epoch 152 iteration700, G Loss: 1.0997323989868164, D Loss: 1.2200047969818115, alpha: 0.91309306697374\n",
      "epoch 152 iteration800, G Loss: 1.010030746459961, D Loss: 1.3319300413131714, alpha: 0.91309306697374\n",
      "Saving content.\n",
      "epoch 153 iteration0, G Loss: 0.781120777130127, D Loss: 1.3454389572143555, alpha: 0.9111695906739038\n",
      "epoch 153 iteration100, G Loss: 1.0709326267242432, D Loss: 1.216294765472412, alpha: 0.9111695906739038\n",
      "epoch 153 iteration200, G Loss: 1.2142311334609985, D Loss: 1.3720371723175049, alpha: 0.9111695906739038\n",
      "epoch 153 iteration300, G Loss: 0.872786819934845, D Loss: 1.1172765493392944, alpha: 0.9111695906739038\n",
      "epoch 153 iteration400, G Loss: 1.0188978910446167, D Loss: 1.3338018655776978, alpha: 0.9111695906739038\n",
      "epoch 153 iteration500, G Loss: 0.9153980016708374, D Loss: 1.24884831905365, alpha: 0.9111695906739038\n",
      "epoch 153 iteration600, G Loss: 1.426371693611145, D Loss: 1.5119553804397583, alpha: 0.9111695906739038\n",
      "epoch 153 iteration700, G Loss: 0.8522982597351074, D Loss: 1.2410746812820435, alpha: 0.9111695906739038\n",
      "epoch 153 iteration800, G Loss: 0.9626733064651489, D Loss: 1.2841358184814453, alpha: 0.9111695906739038\n",
      "Saving content.\n",
      "epoch 154 iteration0, G Loss: 1.1906849145889282, D Loss: 1.1857526302337646, alpha: 0.9092077758892203\n",
      "epoch 154 iteration100, G Loss: 1.1608442068099976, D Loss: 1.3131300210952759, alpha: 0.9092077758892203\n",
      "epoch 154 iteration200, G Loss: 0.7684731483459473, D Loss: 1.2765694856643677, alpha: 0.9092077758892203\n",
      "epoch 154 iteration300, G Loss: 1.0972487926483154, D Loss: 1.1611922979354858, alpha: 0.9092077758892203\n",
      "epoch 154 iteration400, G Loss: 1.1842074394226074, D Loss: 1.2697701454162598, alpha: 0.9092077758892203\n",
      "epoch 154 iteration500, G Loss: 1.154309868812561, D Loss: 1.119009017944336, alpha: 0.9092077758892203\n",
      "epoch 154 iteration600, G Loss: 1.0888631343841553, D Loss: 1.2091338634490967, alpha: 0.9092077758892203\n",
      "epoch 154 iteration700, G Loss: 0.8614062070846558, D Loss: 1.2266249656677246, alpha: 0.9092077758892203\n",
      "epoch 154 iteration800, G Loss: 1.0061917304992676, D Loss: 1.2314826250076294, alpha: 0.9092077758892203\n",
      "Saving content.\n",
      "epoch 155 iteration0, G Loss: 1.3573276996612549, D Loss: 1.243481159210205, alpha: 0.9072070468828429\n",
      "epoch 155 iteration100, G Loss: 0.7374047040939331, D Loss: 1.438778281211853, alpha: 0.9072070468828429\n",
      "epoch 155 iteration200, G Loss: 1.0136452913284302, D Loss: 1.21189546585083, alpha: 0.9072070468828429\n",
      "epoch 155 iteration300, G Loss: 0.8594644069671631, D Loss: 1.3005250692367554, alpha: 0.9072070468828429\n",
      "epoch 155 iteration400, G Loss: 1.0630031824111938, D Loss: 1.235288381576538, alpha: 0.9072070468828429\n",
      "epoch 155 iteration500, G Loss: 1.004045844078064, D Loss: 1.2040860652923584, alpha: 0.9072070468828429\n",
      "epoch 155 iteration600, G Loss: 0.8592398166656494, D Loss: 1.3556233644485474, alpha: 0.9072070468828429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 155 iteration700, G Loss: 1.2305614948272705, D Loss: 1.2609200477600098, alpha: 0.9072070468828429\n",
      "epoch 155 iteration800, G Loss: 1.073967456817627, D Loss: 1.2029306888580322, alpha: 0.9072070468828429\n",
      "Saving content.\n",
      "epoch 156 iteration0, G Loss: 0.8704122304916382, D Loss: 1.2882537841796875, alpha: 0.9051668277101852\n",
      "epoch 156 iteration100, G Loss: 1.4672540426254272, D Loss: 1.2180233001708984, alpha: 0.9051668277101852\n",
      "epoch 156 iteration200, G Loss: 0.8976019620895386, D Loss: 1.2259025573730469, alpha: 0.9051668277101852\n",
      "epoch 156 iteration300, G Loss: 1.0476582050323486, D Loss: 1.3700791597366333, alpha: 0.9051668277101852\n",
      "epoch 156 iteration400, G Loss: 0.8293524384498596, D Loss: 1.246575117111206, alpha: 0.9051668277101852\n",
      "epoch 156 iteration500, G Loss: 0.9098622798919678, D Loss: 1.1667323112487793, alpha: 0.9051668277101852\n",
      "epoch 156 iteration600, G Loss: 1.4235045909881592, D Loss: 1.3198227882385254, alpha: 0.9051668277101852\n",
      "epoch 156 iteration700, G Loss: 1.0153471231460571, D Loss: 1.2489583492279053, alpha: 0.9051668277101852\n",
      "epoch 156 iteration800, G Loss: 0.9870510697364807, D Loss: 1.167876958847046, alpha: 0.9051668277101852\n",
      "Saving content.\n",
      "epoch 157 iteration0, G Loss: 0.9527829885482788, D Loss: 1.2891180515289307, alpha: 0.9030865426616987\n",
      "epoch 157 iteration100, G Loss: 1.1500107049942017, D Loss: 1.3192096948623657, alpha: 0.9030865426616987\n",
      "epoch 157 iteration200, G Loss: 1.052140474319458, D Loss: 1.2043194770812988, alpha: 0.9030865426616987\n",
      "epoch 157 iteration300, G Loss: 0.8818596005439758, D Loss: 1.1518336534500122, alpha: 0.9030865426616987\n",
      "epoch 157 iteration400, G Loss: 1.0347453355789185, D Loss: 1.168576717376709, alpha: 0.9030865426616987\n",
      "epoch 157 iteration500, G Loss: 0.8913131952285767, D Loss: 1.1821117401123047, alpha: 0.9030865426616987\n",
      "epoch 157 iteration600, G Loss: 0.9354321360588074, D Loss: 1.2821171283721924, alpha: 0.9030865426616987\n",
      "epoch 157 iteration700, G Loss: 0.9867627620697021, D Loss: 1.2092150449752808, alpha: 0.9030865426616987\n",
      "epoch 157 iteration800, G Loss: 0.884015679359436, D Loss: 1.1071832180023193, alpha: 0.9030865426616987\n",
      "Saving content.\n",
      "epoch 158 iteration0, G Loss: 1.2816158533096313, D Loss: 1.2173476219177246, alpha: 0.9009656167252947\n",
      "epoch 158 iteration100, G Loss: 0.9770213961601257, D Loss: 1.2524113655090332, alpha: 0.9009656167252947\n",
      "epoch 158 iteration200, G Loss: 1.267197847366333, D Loss: 1.227818250656128, alpha: 0.9009656167252947\n",
      "epoch 158 iteration300, G Loss: 0.7719342708587646, D Loss: 1.2754977941513062, alpha: 0.9009656167252947\n",
      "epoch 158 iteration400, G Loss: 0.7226592302322388, D Loss: 1.13165283203125, alpha: 0.9009656167252947\n",
      "epoch 158 iteration500, G Loss: 0.8829532861709595, D Loss: 1.3157655000686646, alpha: 0.9009656167252947\n",
      "epoch 158 iteration600, G Loss: 0.892905056476593, D Loss: 1.1823993921279907, alpha: 0.9009656167252947\n",
      "epoch 158 iteration700, G Loss: 0.8943907022476196, D Loss: 1.3373773097991943, alpha: 0.9009656167252947\n",
      "epoch 158 iteration800, G Loss: 1.195158839225769, D Loss: 1.2298487424850464, alpha: 0.9009656167252947\n",
      "Saving content.\n",
      "epoch 159 iteration0, G Loss: 0.8423931002616882, D Loss: 1.3222657442092896, alpha: 0.8988034760686675\n",
      "epoch 159 iteration100, G Loss: 0.576128363609314, D Loss: 1.4729056358337402, alpha: 0.8988034760686675\n",
      "epoch 159 iteration200, G Loss: 1.1356769800186157, D Loss: 1.2541632652282715, alpha: 0.8988034760686675\n",
      "epoch 159 iteration300, G Loss: 1.0447885990142822, D Loss: 1.1977739334106445, alpha: 0.8988034760686675\n",
      "epoch 159 iteration400, G Loss: 1.131422519683838, D Loss: 1.2602707147598267, alpha: 0.8988034760686675\n",
      "epoch 159 iteration500, G Loss: 0.9125934839248657, D Loss: 1.2842354774475098, alpha: 0.8988034760686675\n",
      "epoch 159 iteration600, G Loss: 1.0793920755386353, D Loss: 1.1492722034454346, alpha: 0.8988034760686675\n",
      "epoch 159 iteration700, G Loss: 0.838446319103241, D Loss: 1.175654411315918, alpha: 0.8988034760686675\n",
      "epoch 159 iteration800, G Loss: 1.1482868194580078, D Loss: 1.25303316116333, alpha: 0.8988034760686675\n",
      "Saving content.\n",
      "epoch 160 iteration0, G Loss: 1.9328919649124146, D Loss: 1.217943787574768, alpha: 0.8965995485417504\n",
      "epoch 160 iteration100, G Loss: 0.9295490384101868, D Loss: 1.3058123588562012, alpha: 0.8965995485417504\n",
      "epoch 160 iteration200, G Loss: 1.0999712944030762, D Loss: 1.1762622594833374, alpha: 0.8965995485417504\n",
      "epoch 160 iteration300, G Loss: 1.1129083633422852, D Loss: 1.2736822366714478, alpha: 0.8965995485417504\n",
      "epoch 160 iteration400, G Loss: 1.029646635055542, D Loss: 1.271865963935852, alpha: 0.8965995485417504\n",
      "epoch 160 iteration500, G Loss: 1.2816473245620728, D Loss: 1.205167531967163, alpha: 0.8965995485417504\n",
      "epoch 160 iteration600, G Loss: 1.3088722229003906, D Loss: 1.279081106185913, alpha: 0.8965995485417504\n",
      "epoch 160 iteration700, G Loss: 1.0980206727981567, D Loss: 1.244014024734497, alpha: 0.8965995485417504\n",
      "epoch 160 iteration800, G Loss: 1.215857982635498, D Loss: 1.19255793094635, alpha: 0.8965995485417504\n",
      "Saving content.\n",
      "epoch 161 iteration0, G Loss: 1.0320143699645996, D Loss: 1.310713291168213, alpha: 0.8943532641995036\n",
      "epoch 161 iteration100, G Loss: 1.0599207878112793, D Loss: 1.2257624864578247, alpha: 0.8943532641995036\n",
      "epoch 161 iteration200, G Loss: 0.8895625472068787, D Loss: 1.2057634592056274, alpha: 0.8943532641995036\n",
      "epoch 161 iteration300, G Loss: 0.9157161116600037, D Loss: 1.2563257217407227, alpha: 0.8943532641995036\n",
      "epoch 161 iteration400, G Loss: 0.9585850238800049, D Loss: 1.2456210851669312, alpha: 0.8943532641995036\n",
      "epoch 161 iteration500, G Loss: 1.0947860479354858, D Loss: 1.198842167854309, alpha: 0.8943532641995036\n",
      "epoch 161 iteration600, G Loss: 1.0498753786087036, D Loss: 1.1802586317062378, alpha: 0.8943532641995036\n",
      "epoch 161 iteration700, G Loss: 0.9070762991905212, D Loss: 1.156717300415039, alpha: 0.8943532641995036\n",
      "epoch 161 iteration800, G Loss: 0.9270840883255005, D Loss: 1.244165062904358, alpha: 0.8943532641995036\n",
      "Saving content.\n",
      "epoch 162 iteration0, G Loss: 1.0129209756851196, D Loss: 1.111684799194336, alpha: 0.8920640558451981\n",
      "epoch 162 iteration100, G Loss: 1.036636233329773, D Loss: 1.171133041381836, alpha: 0.8920640558451981\n",
      "epoch 162 iteration200, G Loss: 0.9224962592124939, D Loss: 1.195948600769043, alpha: 0.8920640558451981\n",
      "epoch 162 iteration300, G Loss: 1.0868080854415894, D Loss: 1.092880368232727, alpha: 0.8920640558451981\n",
      "epoch 162 iteration400, G Loss: 1.1985440254211426, D Loss: 1.2464463710784912, alpha: 0.8920640558451981\n",
      "epoch 162 iteration500, G Loss: 0.9674861431121826, D Loss: 1.153461217880249, alpha: 0.8920640558451981\n",
      "epoch 162 iteration600, G Loss: 1.0628278255462646, D Loss: 1.2739447355270386, alpha: 0.8920640558451981\n",
      "epoch 162 iteration700, G Loss: 0.8268077969551086, D Loss: 1.1360950469970703, alpha: 0.8920640558451981\n",
      "epoch 162 iteration800, G Loss: 0.8948357105255127, D Loss: 1.2991137504577637, alpha: 0.8920640558451981\n",
      "Saving content.\n",
      "epoch 163 iteration0, G Loss: 0.928108811378479, D Loss: 1.1564750671386719, alpha: 0.8897313595943259\n",
      "epoch 163 iteration100, G Loss: 1.1510288715362549, D Loss: 1.1938188076019287, alpha: 0.8897313595943259\n",
      "epoch 163 iteration200, G Loss: 1.1658570766448975, D Loss: 1.2615500688552856, alpha: 0.8897313595943259\n",
      "epoch 163 iteration300, G Loss: 0.9995932579040527, D Loss: 1.182774543762207, alpha: 0.8897313595943259\n",
      "epoch 163 iteration400, G Loss: 1.0621875524520874, D Loss: 1.2684509754180908, alpha: 0.8897313595943259\n",
      "epoch 163 iteration500, G Loss: 1.354347586631775, D Loss: 1.2942006587982178, alpha: 0.8897313595943259\n",
      "epoch 163 iteration600, G Loss: 1.1037757396697998, D Loss: 1.3350152969360352, alpha: 0.8897313595943259\n",
      "epoch 163 iteration700, G Loss: 1.0224369764328003, D Loss: 1.2563493251800537, alpha: 0.8897313595943259\n",
      "epoch 163 iteration800, G Loss: 2.11283540725708, D Loss: 1.4308271408081055, alpha: 0.8897313595943259\n",
      "Saving content.\n",
      "epoch 164 iteration0, G Loss: 0.8154245615005493, D Loss: 1.1055262088775635, alpha: 0.8873546154592227\n",
      "epoch 164 iteration100, G Loss: 1.0398086309432983, D Loss: 1.2046737670898438, alpha: 0.8873546154592227\n",
      "epoch 164 iteration200, G Loss: 1.0851212739944458, D Loss: 1.265805959701538, alpha: 0.8873546154592227\n",
      "epoch 164 iteration300, G Loss: 0.8716954588890076, D Loss: 1.2351738214492798, alpha: 0.8873546154592227\n",
      "epoch 164 iteration400, G Loss: 0.8921704292297363, D Loss: 1.247344732284546, alpha: 0.8873546154592227\n",
      "epoch 164 iteration500, G Loss: 1.226802945137024, D Loss: 1.2405283451080322, alpha: 0.8873546154592227\n",
      "epoch 164 iteration600, G Loss: 1.1001050472259521, D Loss: 1.077289342880249, alpha: 0.8873546154592227\n",
      "epoch 164 iteration700, G Loss: 1.0129551887512207, D Loss: 1.2643530368804932, alpha: 0.8873546154592227\n",
      "epoch 164 iteration800, G Loss: 0.9262022376060486, D Loss: 1.2549142837524414, alpha: 0.8873546154592227\n",
      "Saving content.\n",
      "epoch 165 iteration0, G Loss: 1.295147180557251, D Loss: 1.2536208629608154, alpha: 0.8849332679544502\n",
      "epoch 165 iteration100, G Loss: 0.8059719800949097, D Loss: 1.1862671375274658, alpha: 0.8849332679544502\n",
      "epoch 165 iteration200, G Loss: 0.97469562292099, D Loss: 1.2414765357971191, alpha: 0.8849332679544502\n",
      "epoch 165 iteration300, G Loss: 0.9914910793304443, D Loss: 1.2295153141021729, alpha: 0.8849332679544502\n",
      "epoch 165 iteration400, G Loss: 1.109557867050171, D Loss: 1.291969656944275, alpha: 0.8849332679544502\n",
      "epoch 165 iteration500, G Loss: 1.2992939949035645, D Loss: 1.1035003662109375, alpha: 0.8849332679544502\n",
      "epoch 165 iteration600, G Loss: 1.4148836135864258, D Loss: 1.105926275253296, alpha: 0.8849332679544502\n",
      "epoch 165 iteration700, G Loss: 1.282729148864746, D Loss: 1.094635009765625, alpha: 0.8849332679544502\n",
      "epoch 165 iteration800, G Loss: 1.2709977626800537, D Loss: 1.2473646402359009, alpha: 0.8849332679544502\n",
      "Saving content.\n",
      "epoch 166 iteration0, G Loss: 0.8576850295066833, D Loss: 1.2119777202606201, alpha: 0.88246676672294\n",
      "epoch 166 iteration100, G Loss: 0.8339443802833557, D Loss: 1.246342658996582, alpha: 0.88246676672294\n",
      "epoch 166 iteration200, G Loss: 1.0647077560424805, D Loss: 1.2076809406280518, alpha: 0.88246676672294\n",
      "epoch 166 iteration300, G Loss: 1.1974272727966309, D Loss: 1.1809828281402588, alpha: 0.88246676672294\n",
      "epoch 166 iteration400, G Loss: 1.1606005430221558, D Loss: 1.2077462673187256, alpha: 0.88246676672294\n",
      "epoch 166 iteration500, G Loss: 0.9464956521987915, D Loss: 1.2024552822113037, alpha: 0.88246676672294\n",
      "epoch 166 iteration600, G Loss: 0.8413152098655701, D Loss: 1.2816736698150635, alpha: 0.88246676672294\n",
      "epoch 166 iteration700, G Loss: 1.324351191520691, D Loss: 1.2883317470550537, alpha: 0.88246676672294\n",
      "epoch 166 iteration800, G Loss: 0.8783165812492371, D Loss: 1.2011265754699707, alpha: 0.88246676672294\n",
      "Saving content.\n",
      "epoch 167 iteration0, G Loss: 1.00980806350708, D Loss: 1.253716230392456, alpha: 0.8799545671828493\n",
      "epoch 167 iteration100, G Loss: 0.915677547454834, D Loss: 1.6622440814971924, alpha: 0.8799545671828493\n",
      "epoch 167 iteration200, G Loss: 0.992195188999176, D Loss: 1.2622201442718506, alpha: 0.8799545671828493\n",
      "epoch 167 iteration300, G Loss: 1.1890411376953125, D Loss: 1.2085877656936646, alpha: 0.8799545671828493\n",
      "epoch 167 iteration400, G Loss: 0.7920949459075928, D Loss: 1.3271974325180054, alpha: 0.8799545671828493\n",
      "epoch 167 iteration500, G Loss: 0.6603202819824219, D Loss: 1.3629206418991089, alpha: 0.8799545671828493\n",
      "epoch 167 iteration600, G Loss: 0.7511613965034485, D Loss: 1.587368369102478, alpha: 0.8799545671828493\n",
      "epoch 167 iteration700, G Loss: 0.7896128296852112, D Loss: 1.337356448173523, alpha: 0.8799545671828493\n",
      "epoch 167 iteration800, G Loss: 1.0566346645355225, D Loss: 1.2391927242279053, alpha: 0.8799545671828493\n",
      "Saving content.\n",
      "epoch 168 iteration0, G Loss: 0.8926374316215515, D Loss: 1.1956032514572144, alpha: 0.8773961311950302\n",
      "epoch 168 iteration100, G Loss: 0.8959742188453674, D Loss: 1.1845815181732178, alpha: 0.8773961311950302\n",
      "epoch 168 iteration200, G Loss: 1.095426321029663, D Loss: 1.274256706237793, alpha: 0.8773961311950302\n",
      "epoch 168 iteration300, G Loss: 0.8677883148193359, D Loss: 1.1908390522003174, alpha: 0.8773961311950302\n",
      "epoch 168 iteration400, G Loss: 1.0756200551986694, D Loss: 1.1810625791549683, alpha: 0.8773961311950302\n",
      "epoch 168 iteration500, G Loss: 0.8364847898483276, D Loss: 1.2442916631698608, alpha: 0.8773961311950302\n",
      "epoch 168 iteration600, G Loss: 0.9300453662872314, D Loss: 1.2776602506637573, alpha: 0.8773961311950302\n",
      "epoch 168 iteration700, G Loss: 1.4524869918823242, D Loss: 1.3260713815689087, alpha: 0.8773961311950302\n",
      "epoch 168 iteration800, G Loss: 1.027543067932129, D Loss: 1.1862132549285889, alpha: 0.8773961311950302\n",
      "Saving content.\n",
      "epoch 169 iteration0, G Loss: 1.2229559421539307, D Loss: 1.2062580585479736, alpha: 0.8747909277509601\n",
      "epoch 169 iteration100, G Loss: 1.145698070526123, D Loss: 1.0888289213180542, alpha: 0.8747909277509601\n",
      "epoch 169 iteration200, G Loss: 0.8607778549194336, D Loss: 1.1952632665634155, alpha: 0.8747909277509601\n",
      "epoch 169 iteration300, G Loss: 1.2281911373138428, D Loss: 1.1955565214157104, alpha: 0.8747909277509601\n",
      "epoch 169 iteration400, G Loss: 1.120538353919983, D Loss: 1.3178415298461914, alpha: 0.8747909277509601\n",
      "epoch 169 iteration500, G Loss: 1.382060170173645, D Loss: 1.132683277130127, alpha: 0.8747909277509601\n",
      "epoch 169 iteration600, G Loss: 0.85858553647995, D Loss: 1.1992859840393066, alpha: 0.8747909277509601\n",
      "epoch 169 iteration700, G Loss: 0.9710506200790405, D Loss: 1.1554310321807861, alpha: 0.8747909277509601\n",
      "epoch 169 iteration800, G Loss: 0.7087512612342834, D Loss: 1.1597139835357666, alpha: 0.8747909277509601\n",
      "Saving content.\n",
      "epoch 170 iteration0, G Loss: 0.9958323240280151, D Loss: 1.0278980731964111, alpha: 0.8721384336809187\n",
      "epoch 170 iteration100, G Loss: 1.0321693420410156, D Loss: 1.3127201795578003, alpha: 0.8721384336809187\n",
      "epoch 170 iteration200, G Loss: 1.2307637929916382, D Loss: 1.298008918762207, alpha: 0.8721384336809187\n",
      "epoch 170 iteration300, G Loss: 1.0862693786621094, D Loss: 1.1721959114074707, alpha: 0.8721384336809187\n",
      "epoch 170 iteration400, G Loss: 0.9644047617912292, D Loss: 1.3675059080123901, alpha: 0.8721384336809187\n",
      "epoch 170 iteration500, G Loss: 1.2016901969909668, D Loss: 1.1651442050933838, alpha: 0.8721384336809187\n",
      "epoch 170 iteration600, G Loss: 1.2487680912017822, D Loss: 1.2085736989974976, alpha: 0.8721384336809187\n",
      "epoch 170 iteration700, G Loss: 1.3724321126937866, D Loss: 1.0977033376693726, alpha: 0.8721384336809187\n",
      "epoch 170 iteration800, G Loss: 1.2605130672454834, D Loss: 1.1295355558395386, alpha: 0.8721384336809187\n",
      "Saving content.\n",
      "epoch 171 iteration0, G Loss: 0.9209040999412537, D Loss: 1.1999156475067139, alpha: 0.869438134382144\n",
      "epoch 171 iteration100, G Loss: 1.0293924808502197, D Loss: 1.1899847984313965, alpha: 0.869438134382144\n",
      "epoch 171 iteration200, G Loss: 1.0821051597595215, D Loss: 1.1090822219848633, alpha: 0.869438134382144\n",
      "epoch 171 iteration300, G Loss: 0.9178130626678467, D Loss: 1.2701833248138428, alpha: 0.869438134382144\n",
      "epoch 171 iteration400, G Loss: 0.9910586476325989, D Loss: 1.1121673583984375, alpha: 0.869438134382144\n",
      "epoch 171 iteration500, G Loss: 0.9319965243339539, D Loss: 1.1951746940612793, alpha: 0.869438134382144\n",
      "epoch 171 iteration600, G Loss: 0.8978344202041626, D Loss: 1.2575565576553345, alpha: 0.869438134382144\n",
      "epoch 171 iteration700, G Loss: 1.0547879934310913, D Loss: 1.1853580474853516, alpha: 0.869438134382144\n",
      "epoch 171 iteration800, G Loss: 1.0993967056274414, D Loss: 1.2717370986938477, alpha: 0.869438134382144\n",
      "Saving content.\n",
      "epoch 172 iteration0, G Loss: 0.9281793236732483, D Loss: 1.2280218601226807, alpha: 0.8666895245666295\n",
      "epoch 172 iteration100, G Loss: 0.7187807559967041, D Loss: 1.2541704177856445, alpha: 0.8666895245666295\n",
      "epoch 172 iteration200, G Loss: 0.8425112962722778, D Loss: 1.260872483253479, alpha: 0.8666895245666295\n",
      "epoch 172 iteration300, G Loss: 0.8823985457420349, D Loss: 1.1906847953796387, alpha: 0.8666895245666295\n",
      "epoch 172 iteration400, G Loss: 0.9090421795845032, D Loss: 1.3702269792556763, alpha: 0.8666895245666295\n",
      "epoch 172 iteration500, G Loss: 1.950954556465149, D Loss: 1.335730791091919, alpha: 0.8666895245666295\n",
      "epoch 172 iteration600, G Loss: 1.2943294048309326, D Loss: 1.1524901390075684, alpha: 0.8666895245666295\n",
      "epoch 172 iteration700, G Loss: 1.5096466541290283, D Loss: 1.394923210144043, alpha: 0.8666895245666295\n",
      "epoch 172 iteration800, G Loss: 1.1357028484344482, D Loss: 1.1328306198120117, alpha: 0.8666895245666295\n",
      "Saving content.\n",
      "epoch 173 iteration0, G Loss: 0.925520122051239, D Loss: 1.1717381477355957, alpha: 0.8638921090281604\n",
      "epoch 173 iteration100, G Loss: 1.6842052936553955, D Loss: 1.2405407428741455, alpha: 0.8638921090281604\n",
      "epoch 173 iteration200, G Loss: 1.4416203498840332, D Loss: 1.227689266204834, alpha: 0.8638921090281604\n",
      "epoch 173 iteration300, G Loss: 1.0966901779174805, D Loss: 1.180833101272583, alpha: 0.8638921090281604\n",
      "epoch 173 iteration400, G Loss: 0.9253698587417603, D Loss: 1.1384141445159912, alpha: 0.8638921090281604\n",
      "epoch 173 iteration500, G Loss: 0.967983603477478, D Loss: 1.2997273206710815, alpha: 0.8638921090281604\n",
      "epoch 173 iteration600, G Loss: 1.2054474353790283, D Loss: 1.2607712745666504, alpha: 0.8638921090281604\n",
      "epoch 173 iteration700, G Loss: 1.508320689201355, D Loss: 1.3594168424606323, alpha: 0.8638921090281604\n",
      "epoch 173 iteration800, G Loss: 0.8808643817901611, D Loss: 1.174485206604004, alpha: 0.8638921090281604\n",
      "Saving content.\n",
      "epoch 174 iteration0, G Loss: 0.8511769771575928, D Loss: 1.1972593069076538, alpha: 0.8610454034281185\n",
      "epoch 174 iteration100, G Loss: 0.9000968933105469, D Loss: 1.2638152837753296, alpha: 0.8610454034281185\n",
      "epoch 174 iteration200, G Loss: 0.9250278472900391, D Loss: 1.257829189300537, alpha: 0.8610454034281185\n",
      "epoch 174 iteration300, G Loss: 0.8859760761260986, D Loss: 1.2616689205169678, alpha: 0.8610454034281185\n",
      "epoch 174 iteration400, G Loss: 1.2010293006896973, D Loss: 1.2167922258377075, alpha: 0.8610454034281185\n",
      "epoch 174 iteration500, G Loss: 0.9123488664627075, D Loss: 1.1822841167449951, alpha: 0.8610454034281185\n",
      "epoch 174 iteration600, G Loss: 1.1340878009796143, D Loss: 1.2136929035186768, alpha: 0.8610454034281185\n",
      "epoch 174 iteration700, G Loss: 1.4883389472961426, D Loss: 1.1937906742095947, alpha: 0.8610454034281185\n",
      "epoch 174 iteration800, G Loss: 0.9468346834182739, D Loss: 1.1985982656478882, alpha: 0.8610454034281185\n",
      "Saving content.\n",
      "epoch 175 iteration0, G Loss: 0.8083868622779846, D Loss: 1.1448322534561157, alpha: 0.8581489350995122\n",
      "epoch 175 iteration100, G Loss: 0.9848774075508118, D Loss: 1.210402488708496, alpha: 0.8581489350995122\n",
      "epoch 175 iteration200, G Loss: 0.9573172926902771, D Loss: 1.1905890703201294, alpha: 0.8581489350995122\n",
      "epoch 175 iteration300, G Loss: 1.0205329656600952, D Loss: 1.233736276626587, alpha: 0.8581489350995122\n",
      "epoch 175 iteration400, G Loss: 1.2838393449783325, D Loss: 1.396822452545166, alpha: 0.8581489350995122\n",
      "epoch 175 iteration500, G Loss: 0.8414068222045898, D Loss: 1.2002414464950562, alpha: 0.8581489350995122\n",
      "epoch 175 iteration600, G Loss: 0.885816752910614, D Loss: 1.2996912002563477, alpha: 0.8581489350995122\n",
      "epoch 175 iteration700, G Loss: 1.1959182024002075, D Loss: 1.2300622463226318, alpha: 0.8581489350995122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 175 iteration800, G Loss: 1.0954028367996216, D Loss: 1.1819000244140625, alpha: 0.8581489350995122\n",
      "Saving content.\n",
      "epoch 176 iteration0, G Loss: 0.7270182371139526, D Loss: 1.2078862190246582, alpha: 0.8552022438686094\n",
      "epoch 176 iteration100, G Loss: 0.6252414584159851, D Loss: 1.2672843933105469, alpha: 0.8552022438686094\n",
      "epoch 176 iteration200, G Loss: 1.1838597059249878, D Loss: 1.2244588136672974, alpha: 0.8552022438686094\n",
      "epoch 176 iteration300, G Loss: 1.1736974716186523, D Loss: 1.206209421157837, alpha: 0.8552022438686094\n",
      "epoch 176 iteration400, G Loss: 1.0426397323608398, D Loss: 1.3224244117736816, alpha: 0.8552022438686094\n",
      "epoch 176 iteration500, G Loss: 1.3381019830703735, D Loss: 1.2024857997894287, alpha: 0.8552022438686094\n",
      "epoch 176 iteration600, G Loss: 0.9728567600250244, D Loss: 1.1656066179275513, alpha: 0.8552022438686094\n",
      "epoch 176 iteration700, G Loss: 1.0217760801315308, D Loss: 1.1141490936279297, alpha: 0.8552022438686094\n",
      "epoch 176 iteration800, G Loss: 0.8541347980499268, D Loss: 1.2919946908950806, alpha: 0.8552022438686094\n",
      "Saving content.\n",
      "epoch 177 iteration0, G Loss: 0.8838173151016235, D Loss: 1.1419309377670288, alpha: 0.852204882893481\n",
      "epoch 177 iteration100, G Loss: 0.7847638130187988, D Loss: 1.1605769395828247, alpha: 0.852204882893481\n",
      "epoch 177 iteration200, G Loss: 1.3654673099517822, D Loss: 1.555303692817688, alpha: 0.852204882893481\n",
      "epoch 177 iteration300, G Loss: 1.0078121423721313, D Loss: 1.278794527053833, alpha: 0.852204882893481\n",
      "epoch 177 iteration400, G Loss: 1.0621979236602783, D Loss: 1.1644952297210693, alpha: 0.852204882893481\n",
      "epoch 177 iteration500, G Loss: 0.9851586818695068, D Loss: 1.2821574211120605, alpha: 0.852204882893481\n",
      "epoch 177 iteration600, G Loss: 0.7536405324935913, D Loss: 1.278510570526123, alpha: 0.852204882893481\n",
      "epoch 177 iteration700, G Loss: 0.8773217797279358, D Loss: 1.2653119564056396, alpha: 0.852204882893481\n",
      "epoch 177 iteration800, G Loss: 0.9386064410209656, D Loss: 1.3177673816680908, alpha: 0.852204882893481\n",
      "Saving content.\n",
      "epoch 178 iteration0, G Loss: 0.8560925722122192, D Loss: 1.2238812446594238, alpha: 0.8491564195186722\n",
      "epoch 178 iteration100, G Loss: 1.3127915859222412, D Loss: 1.3492320775985718, alpha: 0.8491564195186722\n",
      "epoch 178 iteration200, G Loss: 1.1801636219024658, D Loss: 1.1503691673278809, alpha: 0.8491564195186722\n",
      "epoch 178 iteration300, G Loss: 1.1067039966583252, D Loss: 1.2838188409805298, alpha: 0.8491564195186722\n",
      "epoch 178 iteration400, G Loss: 0.7860899567604065, D Loss: 1.1676139831542969, alpha: 0.8491564195186722\n",
      "epoch 178 iteration500, G Loss: 1.237945556640625, D Loss: 1.1995021104812622, alpha: 0.8491564195186722\n",
      "epoch 178 iteration600, G Loss: 1.1092472076416016, D Loss: 1.3603897094726562, alpha: 0.8491564195186722\n",
      "epoch 178 iteration700, G Loss: 0.7865808010101318, D Loss: 1.189788579940796, alpha: 0.8491564195186722\n",
      "epoch 178 iteration800, G Loss: 0.8856915831565857, D Loss: 1.2623308897018433, alpha: 0.8491564195186722\n",
      "Saving content.\n",
      "epoch 179 iteration0, G Loss: 1.0471851825714111, D Loss: 1.264899492263794, alpha: 0.8460564361451456\n",
      "epoch 179 iteration100, G Loss: 0.8428043127059937, D Loss: 1.3324341773986816, alpha: 0.8460564361451456\n",
      "epoch 179 iteration200, G Loss: 1.0254184007644653, D Loss: 1.3016929626464844, alpha: 0.8460564361451456\n",
      "epoch 179 iteration300, G Loss: 0.8488415479660034, D Loss: 1.1083343029022217, alpha: 0.8460564361451456\n",
      "epoch 179 iteration400, G Loss: 0.9739325046539307, D Loss: 1.319662094116211, alpha: 0.8460564361451456\n",
      "epoch 179 iteration500, G Loss: 0.9825745820999146, D Loss: 1.167248249053955, alpha: 0.8460564361451456\n",
      "epoch 179 iteration600, G Loss: 1.401045799255371, D Loss: 1.3084046840667725, alpha: 0.8460564361451456\n",
      "epoch 179 iteration700, G Loss: 0.9457567930221558, D Loss: 1.175834059715271, alpha: 0.8460564361451456\n",
      "epoch 179 iteration800, G Loss: 1.0753449201583862, D Loss: 1.113223910331726, alpha: 0.8460564361451456\n",
      "Saving content.\n",
      "epoch 180 iteration0, G Loss: 0.998380720615387, D Loss: 1.3856759071350098, alpha: 0.8429045311145472\n",
      "epoch 180 iteration100, G Loss: 1.3629846572875977, D Loss: 1.4304834604263306, alpha: 0.8429045311145472\n",
      "epoch 180 iteration200, G Loss: 1.069183111190796, D Loss: 1.2104636430740356, alpha: 0.8429045311145472\n",
      "epoch 180 iteration300, G Loss: 1.0735697746276855, D Loss: 1.2274318933486938, alpha: 0.8429045311145472\n",
      "epoch 180 iteration400, G Loss: 1.2693312168121338, D Loss: 1.1488544940948486, alpha: 0.8429045311145472\n",
      "epoch 180 iteration500, G Loss: 0.8770761489868164, D Loss: 1.1873115301132202, alpha: 0.8429045311145472\n",
      "epoch 180 iteration600, G Loss: 1.0273984670639038, D Loss: 1.2087043523788452, alpha: 0.8429045311145472\n",
      "epoch 180 iteration700, G Loss: 0.9034257531166077, D Loss: 1.1669901609420776, alpha: 0.8429045311145472\n",
      "epoch 180 iteration800, G Loss: 1.3294209241867065, D Loss: 1.2291076183319092, alpha: 0.8429045311145472\n",
      "Saving content.\n",
      "epoch 181 iteration0, G Loss: 0.9268868565559387, D Loss: 1.275046944618225, alpha: 0.8397003196067644\n",
      "epoch 181 iteration100, G Loss: 0.6410169005393982, D Loss: 1.2907613515853882, alpha: 0.8397003196067644\n",
      "epoch 181 iteration200, G Loss: 0.9989524483680725, D Loss: 1.3292901515960693, alpha: 0.8397003196067644\n",
      "epoch 181 iteration300, G Loss: 1.039435625076294, D Loss: 1.1885135173797607, alpha: 0.8397003196067644\n",
      "epoch 181 iteration400, G Loss: 1.1512000560760498, D Loss: 1.1794003248214722, alpha: 0.8397003196067644\n",
      "epoch 181 iteration500, G Loss: 0.7945935130119324, D Loss: 1.1722872257232666, alpha: 0.8397003196067644\n",
      "epoch 181 iteration600, G Loss: 1.0013140439987183, D Loss: 1.1261587142944336, alpha: 0.8397003196067644\n",
      "epoch 181 iteration700, G Loss: 1.2328824996948242, D Loss: 1.231203556060791, alpha: 0.8397003196067644\n",
      "epoch 181 iteration800, G Loss: 0.9490014314651489, D Loss: 1.2061011791229248, alpha: 0.8397003196067644\n",
      "Saving content.\n",
      "epoch 182 iteration0, G Loss: 1.4085134267807007, D Loss: 1.1760776042938232, alpha: 0.8364434345496531\n",
      "epoch 182 iteration100, G Loss: 0.8316811323165894, D Loss: 1.1403073072433472, alpha: 0.8364434345496531\n",
      "epoch 182 iteration200, G Loss: 0.8297617435455322, D Loss: 1.1782960891723633, alpha: 0.8364434345496531\n",
      "epoch 182 iteration300, G Loss: 0.9998642802238464, D Loss: 1.2169508934020996, alpha: 0.8364434345496531\n",
      "epoch 182 iteration400, G Loss: 1.0243045091629028, D Loss: 1.2506132125854492, alpha: 0.8364434345496531\n",
      "epoch 182 iteration500, G Loss: 1.0707156658172607, D Loss: 1.1914029121398926, alpha: 0.8364434345496531\n",
      "epoch 182 iteration600, G Loss: 1.0482600927352905, D Loss: 1.207763433456421, alpha: 0.8364434345496531\n",
      "epoch 182 iteration700, G Loss: 0.7938290238380432, D Loss: 1.2257843017578125, alpha: 0.8364434345496531\n",
      "epoch 182 iteration800, G Loss: 1.0628266334533691, D Loss: 1.1252861022949219, alpha: 0.8364434345496531\n",
      "Saving content.\n",
      "epoch 183 iteration0, G Loss: 0.8861580491065979, D Loss: 1.1910290718078613, alpha: 0.8331335275397229\n",
      "epoch 183 iteration100, G Loss: 0.9671786427497864, D Loss: 1.4648228883743286, alpha: 0.8331335275397229\n",
      "epoch 183 iteration200, G Loss: 0.7848008871078491, D Loss: 1.2401032447814941, alpha: 0.8331335275397229\n",
      "epoch 183 iteration300, G Loss: 0.762459397315979, D Loss: 1.2019362449645996, alpha: 0.8331335275397229\n",
      "epoch 183 iteration400, G Loss: 0.8794549107551575, D Loss: 1.9124675989151, alpha: 0.8331335275397229\n",
      "epoch 183 iteration500, G Loss: 0.6025716662406921, D Loss: 1.2538015842437744, alpha: 0.8331335275397229\n",
      "epoch 183 iteration600, G Loss: 1.021052360534668, D Loss: 1.1721974611282349, alpha: 0.8331335275397229\n",
      "epoch 183 iteration700, G Loss: 1.027834177017212, D Loss: 1.2623345851898193, alpha: 0.8331335275397229\n",
      "epoch 183 iteration800, G Loss: 1.0131752490997314, D Loss: 1.155754804611206, alpha: 0.8331335275397229\n",
      "Saving content.\n",
      "epoch 184 iteration0, G Loss: 0.949198842048645, D Loss: 1.1239800453186035, alpha: 0.8297702697724747\n",
      "epoch 184 iteration100, G Loss: 1.0371434688568115, D Loss: 1.2841699123382568, alpha: 0.8297702697724747\n",
      "epoch 184 iteration200, G Loss: 1.0196269750595093, D Loss: 1.2395418882369995, alpha: 0.8297702697724747\n",
      "epoch 184 iteration300, G Loss: 1.1527891159057617, D Loss: 1.2045499086380005, alpha: 0.8297702697724747\n",
      "epoch 184 iteration400, G Loss: 0.7798335552215576, D Loss: 1.2721037864685059, alpha: 0.8297702697724747\n",
      "epoch 184 iteration500, G Loss: 0.9147080183029175, D Loss: 1.2057784795761108, alpha: 0.8297702697724747\n",
      "epoch 184 iteration600, G Loss: 0.9513242244720459, D Loss: 1.249802827835083, alpha: 0.8297702697724747\n",
      "epoch 184 iteration700, G Loss: 0.9622382521629333, D Loss: 1.2862915992736816, alpha: 0.8297702697724747\n",
      "epoch 184 iteration800, G Loss: 0.8652523756027222, D Loss: 1.160858392715454, alpha: 0.8297702697724747\n",
      "Saving content.\n",
      "epoch 185 iteration0, G Loss: 0.8181982636451721, D Loss: 1.1117644309997559, alpha: 0.8263533529809949\n",
      "epoch 185 iteration100, G Loss: 0.7855867147445679, D Loss: 1.1307494640350342, alpha: 0.8263533529809949\n",
      "epoch 185 iteration200, G Loss: 1.1326136589050293, D Loss: 1.209157943725586, alpha: 0.8263533529809949\n",
      "epoch 185 iteration300, G Loss: 1.0776835680007935, D Loss: 1.1345078945159912, alpha: 0.8263533529809949\n",
      "epoch 185 iteration400, G Loss: 1.019540548324585, D Loss: 1.223407506942749, alpha: 0.8263533529809949\n",
      "epoch 185 iteration500, G Loss: 0.7887322306632996, D Loss: 1.184114933013916, alpha: 0.8263533529809949\n",
      "epoch 185 iteration600, G Loss: 1.2442232370376587, D Loss: 1.241983413696289, alpha: 0.8263533529809949\n",
      "epoch 185 iteration700, G Loss: 0.9424522519111633, D Loss: 1.163827657699585, alpha: 0.8263533529809949\n",
      "epoch 185 iteration800, G Loss: 0.8179417848587036, D Loss: 1.3223342895507812, alpha: 0.8263533529809949\n",
      "Saving content.\n",
      "epoch 186 iteration0, G Loss: 0.9267808794975281, D Loss: 1.1964093446731567, alpha: 0.8228824903813154\n",
      "epoch 186 iteration100, G Loss: 0.6616907119750977, D Loss: 1.1247608661651611, alpha: 0.8228824903813154\n",
      "epoch 186 iteration200, G Loss: 0.870473325252533, D Loss: 1.2204937934875488, alpha: 0.8228824903813154\n",
      "epoch 186 iteration300, G Loss: 1.5631729364395142, D Loss: 1.0918701887130737, alpha: 0.8228824903813154\n",
      "epoch 186 iteration400, G Loss: 0.7990943193435669, D Loss: 1.4829379320144653, alpha: 0.8228824903813154\n",
      "epoch 186 iteration500, G Loss: 0.6848526000976562, D Loss: 1.3130412101745605, alpha: 0.8228824903813154\n",
      "epoch 186 iteration600, G Loss: 0.8165128231048584, D Loss: 1.1788206100463867, alpha: 0.8228824903813154\n",
      "epoch 186 iteration700, G Loss: 1.1378170251846313, D Loss: 1.2494378089904785, alpha: 0.8228824903813154\n",
      "epoch 186 iteration800, G Loss: 0.7688033580780029, D Loss: 1.1663403511047363, alpha: 0.8228824903813154\n",
      "Saving content.\n",
      "epoch 187 iteration0, G Loss: 0.9600638151168823, D Loss: 1.2541050910949707, alpha: 0.8193574176229559\n",
      "epoch 187 iteration100, G Loss: 0.8007190227508545, D Loss: 1.2934048175811768, alpha: 0.8193574176229559\n",
      "epoch 187 iteration200, G Loss: 0.87839674949646, D Loss: 1.25602388381958, alpha: 0.8193574176229559\n",
      "epoch 187 iteration300, G Loss: 0.7893292307853699, D Loss: 1.3498015403747559, alpha: 0.8193574176229559\n",
      "epoch 187 iteration400, G Loss: 0.7730910181999207, D Loss: 1.2876280546188354, alpha: 0.8193574176229559\n",
      "epoch 187 iteration500, G Loss: 0.8803604245185852, D Loss: 1.271040439605713, alpha: 0.8193574176229559\n",
      "epoch 187 iteration600, G Loss: 0.8184954524040222, D Loss: 1.2318052053451538, alpha: 0.8193574176229559\n",
      "epoch 187 iteration700, G Loss: 0.6642174124717712, D Loss: 1.373265266418457, alpha: 0.8193574176229559\n",
      "epoch 187 iteration800, G Loss: 1.1655759811401367, D Loss: 1.0733672380447388, alpha: 0.8193574176229559\n",
      "Saving content.\n",
      "epoch 188 iteration0, G Loss: 1.2221249341964722, D Loss: 1.1551616191864014, alpha: 0.8157778937429687\n",
      "epoch 188 iteration100, G Loss: 0.7830648422241211, D Loss: 1.2528265714645386, alpha: 0.8157778937429687\n",
      "epoch 188 iteration200, G Loss: 1.315699577331543, D Loss: 1.2190107107162476, alpha: 0.8157778937429687\n",
      "epoch 188 iteration300, G Loss: 1.1226725578308105, D Loss: 1.185333490371704, alpha: 0.8157778937429687\n",
      "epoch 188 iteration400, G Loss: 0.8199149370193481, D Loss: 1.2761484384536743, alpha: 0.8157778937429687\n",
      "epoch 188 iteration500, G Loss: 1.164881944656372, D Loss: 1.3317735195159912, alpha: 0.8157778937429687\n",
      "epoch 188 iteration600, G Loss: 0.7463139891624451, D Loss: 1.2575567960739136, alpha: 0.8157778937429687\n",
      "epoch 188 iteration700, G Loss: 1.0550553798675537, D Loss: 1.1839234828948975, alpha: 0.8157778937429687\n",
      "epoch 188 iteration800, G Loss: 0.8476290702819824, D Loss: 1.2131038904190063, alpha: 0.8157778937429687\n",
      "Saving content.\n",
      "epoch 189 iteration0, G Loss: 1.2210495471954346, D Loss: 1.2925002574920654, alpha: 0.812143702121719\n",
      "epoch 189 iteration100, G Loss: 1.0096267461776733, D Loss: 1.1751831769943237, alpha: 0.812143702121719\n",
      "epoch 189 iteration200, G Loss: 0.9880454540252686, D Loss: 1.30182683467865, alpha: 0.812143702121719\n",
      "epoch 189 iteration300, G Loss: 1.3561137914657593, D Loss: 1.1201670169830322, alpha: 0.812143702121719\n",
      "epoch 189 iteration400, G Loss: 1.2107949256896973, D Loss: 1.3176530599594116, alpha: 0.812143702121719\n",
      "epoch 189 iteration500, G Loss: 1.2743430137634277, D Loss: 1.1736111640930176, alpha: 0.812143702121719\n",
      "epoch 189 iteration600, G Loss: 0.6688723564147949, D Loss: 1.2533681392669678, alpha: 0.812143702121719\n",
      "epoch 189 iteration700, G Loss: 1.0968340635299683, D Loss: 1.2155154943466187, alpha: 0.812143702121719\n",
      "epoch 189 iteration800, G Loss: 0.8905746936798096, D Loss: 1.3319425582885742, alpha: 0.812143702121719\n",
      "Saving content.\n",
      "epoch 190 iteration0, G Loss: 1.5552865266799927, D Loss: 1.3692946434020996, alpha: 0.8084546514385325\n",
      "epoch 190 iteration100, G Loss: 1.0533427000045776, D Loss: 1.3021025657653809, alpha: 0.8084546514385325\n",
      "epoch 190 iteration200, G Loss: 1.2311359643936157, D Loss: 1.2767879962921143, alpha: 0.8084546514385325\n",
      "epoch 190 iteration300, G Loss: 1.3847790956497192, D Loss: 1.1866390705108643, alpha: 0.8084546514385325\n",
      "epoch 190 iteration400, G Loss: 1.0485464334487915, D Loss: 1.2443426847457886, alpha: 0.8084546514385325\n",
      "epoch 190 iteration500, G Loss: 1.0605132579803467, D Loss: 1.2397717237472534, alpha: 0.8084546514385325\n",
      "epoch 190 iteration600, G Loss: 0.7450246214866638, D Loss: 1.3066790103912354, alpha: 0.8084546514385325\n",
      "epoch 190 iteration700, G Loss: 1.1558581590652466, D Loss: 1.2676273584365845, alpha: 0.8084546514385325\n",
      "epoch 190 iteration800, G Loss: 1.068244457244873, D Loss: 1.2030073404312134, alpha: 0.8084546514385325\n",
      "Saving content.\n",
      "epoch 191 iteration0, G Loss: 0.9539646506309509, D Loss: 1.1111645698547363, alpha: 0.8047105766252601\n",
      "epoch 191 iteration100, G Loss: 1.2529423236846924, D Loss: 1.2979243993759155, alpha: 0.8047105766252601\n",
      "epoch 191 iteration200, G Loss: 0.7579139471054077, D Loss: 1.2215756177902222, alpha: 0.8047105766252601\n",
      "epoch 191 iteration300, G Loss: 1.0450385808944702, D Loss: 1.2124955654144287, alpha: 0.8047105766252601\n",
      "epoch 191 iteration400, G Loss: 1.0696618556976318, D Loss: 1.1819522380828857, alpha: 0.8047105766252601\n",
      "epoch 191 iteration500, G Loss: 0.9741696715354919, D Loss: 1.172529697418213, alpha: 0.8047105766252601\n",
      "epoch 191 iteration600, G Loss: 1.1094970703125, D Loss: 1.1850769519805908, alpha: 0.8047105766252601\n",
      "epoch 191 iteration700, G Loss: 0.7838895320892334, D Loss: 1.276533842086792, alpha: 0.8047105766252601\n",
      "epoch 191 iteration800, G Loss: 1.007867455482483, D Loss: 1.1696875095367432, alpha: 0.8047105766252601\n",
      "Saving content.\n",
      "epoch 192 iteration0, G Loss: 0.8810095191001892, D Loss: 1.1746129989624023, alpha: 0.8009113398157162\n",
      "epoch 192 iteration100, G Loss: 1.1163079738616943, D Loss: 1.2265430688858032, alpha: 0.8009113398157162\n",
      "epoch 192 iteration200, G Loss: 0.9195974469184875, D Loss: 1.1725878715515137, alpha: 0.8009113398157162\n",
      "epoch 192 iteration300, G Loss: 1.0681179761886597, D Loss: 1.3341271877288818, alpha: 0.8009113398157162\n",
      "epoch 192 iteration400, G Loss: 1.2105062007904053, D Loss: 1.2599174976348877, alpha: 0.8009113398157162\n",
      "epoch 192 iteration500, G Loss: 0.8066914081573486, D Loss: 1.2614821195602417, alpha: 0.8009113398157162\n",
      "epoch 192 iteration600, G Loss: 0.9861329793930054, D Loss: 1.091497778892517, alpha: 0.8009113398157162\n",
      "epoch 192 iteration700, G Loss: 0.8354170322418213, D Loss: 1.1876479387283325, alpha: 0.8009113398157162\n",
      "epoch 192 iteration800, G Loss: 1.141807198524475, D Loss: 1.131575107574463, alpha: 0.8009113398157162\n",
      "Saving content.\n",
      "epoch 193 iteration0, G Loss: 1.1934748888015747, D Loss: 1.3672633171081543, alpha: 0.7970568312888584\n",
      "epoch 193 iteration100, G Loss: 0.8950058221817017, D Loss: 1.2630417346954346, alpha: 0.7970568312888584\n",
      "epoch 193 iteration200, G Loss: 0.7594850063323975, D Loss: 1.2453742027282715, alpha: 0.7970568312888584\n",
      "epoch 193 iteration300, G Loss: 0.81741863489151, D Loss: 1.1804689168930054, alpha: 0.7970568312888584\n",
      "epoch 193 iteration400, G Loss: 1.09253990650177, D Loss: 1.333722472190857, alpha: 0.7970568312888584\n",
      "epoch 193 iteration500, G Loss: 1.399633526802063, D Loss: 1.2702429294586182, alpha: 0.7970568312888584\n",
      "epoch 193 iteration600, G Loss: 1.5125696659088135, D Loss: 1.2177512645721436, alpha: 0.7970568312888584\n",
      "epoch 193 iteration700, G Loss: 0.7411532998085022, D Loss: 1.3237485885620117, alpha: 0.7970568312888584\n",
      "epoch 193 iteration800, G Loss: 1.93254554271698, D Loss: 1.310258150100708, alpha: 0.7970568312888584\n",
      "Saving content.\n",
      "epoch 194 iteration0, G Loss: 1.1110200881958008, D Loss: 1.1569050550460815, alpha: 0.7931469704034991\n",
      "epoch 194 iteration100, G Loss: 1.1462997198104858, D Loss: 1.3356516361236572, alpha: 0.7931469704034991\n",
      "epoch 194 iteration200, G Loss: 0.9009202718734741, D Loss: 1.260979413986206, alpha: 0.7931469704034991\n",
      "epoch 194 iteration300, G Loss: 1.3652361631393433, D Loss: 1.3505375385284424, alpha: 0.7931469704034991\n",
      "epoch 194 iteration400, G Loss: 1.2166401147842407, D Loss: 1.2486989498138428, alpha: 0.7931469704034991\n",
      "epoch 194 iteration500, G Loss: 1.0085394382476807, D Loss: 1.3038265705108643, alpha: 0.7931469704034991\n",
      "epoch 194 iteration600, G Loss: 0.7786694765090942, D Loss: 1.3314493894577026, alpha: 0.7931469704034991\n",
      "epoch 194 iteration700, G Loss: 0.9270285367965698, D Loss: 1.3163118362426758, alpha: 0.7931469704034991\n",
      "epoch 194 iteration800, G Loss: 1.2281415462493896, D Loss: 1.2611087560653687, alpha: 0.7931469704034991\n",
      "Saving content.\n",
      "epoch 195 iteration0, G Loss: 1.0937800407409668, D Loss: 1.2747981548309326, alpha: 0.789181706522253\n",
      "epoch 195 iteration100, G Loss: 0.7970009446144104, D Loss: 1.346751093864441, alpha: 0.789181706522253\n",
      "epoch 195 iteration200, G Loss: 0.7361145615577698, D Loss: 1.090476393699646, alpha: 0.789181706522253\n",
      "epoch 195 iteration300, G Loss: 1.0723576545715332, D Loss: 1.2162665128707886, alpha: 0.789181706522253\n",
      "epoch 195 iteration400, G Loss: 1.5254695415496826, D Loss: 1.1295382976531982, alpha: 0.789181706522253\n",
      "epoch 195 iteration500, G Loss: 0.6581392884254456, D Loss: 1.194101333618164, alpha: 0.789181706522253\n",
      "epoch 195 iteration600, G Loss: 0.6087281703948975, D Loss: 1.2335935831069946, alpha: 0.789181706522253\n",
      "epoch 195 iteration700, G Loss: 1.0376896858215332, D Loss: 1.2276540994644165, alpha: 0.789181706522253\n",
      "epoch 195 iteration800, G Loss: 0.8373817205429077, D Loss: 1.2591825723648071, alpha: 0.789181706522253\n",
      "Saving content.\n",
      "epoch 196 iteration0, G Loss: 0.8279790878295898, D Loss: 1.324234962463379, alpha: 0.7851610199223515\n",
      "epoch 196 iteration100, G Loss: 0.7165522575378418, D Loss: 1.4033093452453613, alpha: 0.7851610199223515\n",
      "epoch 196 iteration200, G Loss: 0.9447627663612366, D Loss: 1.2359135150909424, alpha: 0.7851610199223515\n",
      "epoch 196 iteration300, G Loss: 0.9544798731803894, D Loss: 1.1345032453536987, alpha: 0.7851610199223515\n",
      "epoch 196 iteration400, G Loss: 1.1659398078918457, D Loss: 1.203214406967163, alpha: 0.7851610199223515\n",
      "epoch 196 iteration500, G Loss: 1.3300706148147583, D Loss: 1.3655385971069336, alpha: 0.7851610199223515\n",
      "epoch 196 iteration600, G Loss: 0.9212727546691895, D Loss: 1.2188224792480469, alpha: 0.7851610199223515\n",
      "epoch 196 iteration700, G Loss: 0.8794418573379517, D Loss: 1.3093626499176025, alpha: 0.7851610199223515\n",
      "epoch 196 iteration800, G Loss: 0.769788920879364, D Loss: 1.2915987968444824, alpha: 0.7851610199223515\n",
      "Saving content.\n",
      "epoch 197 iteration0, G Loss: 1.1926658153533936, D Loss: 1.1088660955429077, alpha: 0.7810849226908843\n",
      "epoch 197 iteration100, G Loss: 0.9348525404930115, D Loss: 1.2008098363876343, alpha: 0.7810849226908843\n",
      "epoch 197 iteration200, G Loss: 0.9130334854125977, D Loss: 1.642324686050415, alpha: 0.7810849226908843\n",
      "epoch 197 iteration300, G Loss: 1.4747734069824219, D Loss: 1.275928020477295, alpha: 0.7810849226908843\n",
      "epoch 197 iteration400, G Loss: 0.8328911662101746, D Loss: 1.3560054302215576, alpha: 0.7810849226908843\n",
      "epoch 197 iteration500, G Loss: 0.9986475706100464, D Loss: 1.2684632539749146, alpha: 0.7810849226908843\n",
      "epoch 197 iteration600, G Loss: 0.9673659801483154, D Loss: 1.2687489986419678, alpha: 0.7810849226908843\n",
      "epoch 197 iteration700, G Loss: 0.8111832141876221, D Loss: 1.195675015449524, alpha: 0.7810849226908843\n",
      "epoch 197 iteration800, G Loss: 0.9144700765609741, D Loss: 1.2336194515228271, alpha: 0.7810849226908843\n",
      "Saving content.\n",
      "epoch 198 iteration0, G Loss: 0.8526549935340881, D Loss: 1.2789807319641113, alpha: 0.7769534596019573\n",
      "epoch 198 iteration100, G Loss: 1.2846102714538574, D Loss: 1.088795781135559, alpha: 0.7769534596019573\n",
      "epoch 198 iteration200, G Loss: 1.0507688522338867, D Loss: 1.2002501487731934, alpha: 0.7769534596019573\n",
      "epoch 198 iteration300, G Loss: 0.9452766180038452, D Loss: 1.1721594333648682, alpha: 0.7769534596019573\n",
      "epoch 198 iteration400, G Loss: 1.0375956296920776, D Loss: 1.219007968902588, alpha: 0.7769534596019573\n",
      "epoch 198 iteration500, G Loss: 0.8195980191230774, D Loss: 1.2444679737091064, alpha: 0.7769534596019573\n",
      "epoch 198 iteration600, G Loss: 0.7782413363456726, D Loss: 1.282048225402832, alpha: 0.7769534596019573\n",
      "epoch 198 iteration700, G Loss: 0.962516725063324, D Loss: 1.2630845308303833, alpha: 0.7769534596019573\n",
      "epoch 198 iteration800, G Loss: 0.8969627618789673, D Loss: 1.2175087928771973, alpha: 0.7769534596019573\n",
      "Saving content.\n",
      "epoch 199 iteration0, G Loss: 1.2165658473968506, D Loss: 1.173525094985962, alpha: 0.7727667089732031\n",
      "epoch 199 iteration100, G Loss: 1.079959750175476, D Loss: 1.2113735675811768, alpha: 0.7727667089732031\n",
      "epoch 199 iteration200, G Loss: 0.800524115562439, D Loss: 1.2835901975631714, alpha: 0.7727667089732031\n",
      "epoch 199 iteration300, G Loss: 0.9105043411254883, D Loss: 1.1687016487121582, alpha: 0.7727667089732031\n",
      "epoch 199 iteration400, G Loss: 0.9802170991897583, D Loss: 1.083514928817749, alpha: 0.7727667089732031\n",
      "epoch 199 iteration500, G Loss: 0.705001950263977, D Loss: 1.3111379146575928, alpha: 0.7727667089732031\n",
      "epoch 199 iteration600, G Loss: 1.0153961181640625, D Loss: 1.3205320835113525, alpha: 0.7727667089732031\n",
      "epoch 199 iteration700, G Loss: 0.8675291538238525, D Loss: 1.1754380464553833, alpha: 0.7727667089732031\n",
      "epoch 199 iteration800, G Loss: 1.344764232635498, D Loss: 1.2702640295028687, alpha: 0.7727667089732031\n",
      "Saving content.\n",
      "epoch 200 iteration0, G Loss: 0.9451653957366943, D Loss: 1.3288406133651733, alpha: 0.7685247834990176\n",
      "epoch 200 iteration100, G Loss: 1.072553038597107, D Loss: 1.1594452857971191, alpha: 0.7685247834990176\n",
      "epoch 200 iteration200, G Loss: 0.8472175598144531, D Loss: 1.2654335498809814, alpha: 0.7685247834990176\n",
      "epoch 200 iteration300, G Loss: 1.2132872343063354, D Loss: 1.2981855869293213, alpha: 0.7685247834990176\n",
      "epoch 200 iteration400, G Loss: 0.7724109888076782, D Loss: 1.212738275527954, alpha: 0.7685247834990176\n",
      "epoch 200 iteration500, G Loss: 1.027848482131958, D Loss: 1.288710594177246, alpha: 0.7685247834990176\n",
      "epoch 200 iteration600, G Loss: 1.7013994455337524, D Loss: 1.3457764387130737, alpha: 0.7685247834990176\n",
      "epoch 200 iteration700, G Loss: 1.2192707061767578, D Loss: 1.1452794075012207, alpha: 0.7685247834990176\n",
      "epoch 200 iteration800, G Loss: 1.0207273960113525, D Loss: 1.0927948951721191, alpha: 0.7685247834990176\n",
      "Saving content.\n",
      "epoch 201 iteration0, G Loss: 1.1774728298187256, D Loss: 1.15232515335083, alpha: 0.7642278310578563\n",
      "epoch 201 iteration100, G Loss: 0.970646321773529, D Loss: 1.1880699396133423, alpha: 0.7642278310578563\n",
      "epoch 201 iteration200, G Loss: 1.0642313957214355, D Loss: 1.2818264961242676, alpha: 0.7642278310578563\n",
      "epoch 201 iteration300, G Loss: 1.4392458200454712, D Loss: 1.215523362159729, alpha: 0.7642278310578563\n",
      "epoch 201 iteration400, G Loss: 1.1001521348953247, D Loss: 1.2157793045043945, alpha: 0.7642278310578563\n",
      "epoch 201 iteration500, G Loss: 0.7875850200653076, D Loss: 1.3970229625701904, alpha: 0.7642278310578563\n",
      "epoch 201 iteration600, G Loss: 1.1208338737487793, D Loss: 1.248091697692871, alpha: 0.7642278310578563\n",
      "epoch 201 iteration700, G Loss: 1.3020793199539185, D Loss: 1.0775561332702637, alpha: 0.7642278310578563\n",
      "epoch 201 iteration800, G Loss: 0.9104546904563904, D Loss: 1.227474570274353, alpha: 0.7642278310578563\n",
      "Saving content.\n",
      "epoch 202 iteration0, G Loss: 0.9540892243385315, D Loss: 1.1319397687911987, alpha: 0.7598760354908782\n",
      "epoch 202 iteration100, G Loss: 0.9793413281440735, D Loss: 1.356292724609375, alpha: 0.7598760354908782\n",
      "epoch 202 iteration200, G Loss: 1.1038012504577637, D Loss: 1.1864545345306396, alpha: 0.7598760354908782\n",
      "epoch 202 iteration300, G Loss: 0.9121555089950562, D Loss: 1.1829311847686768, alpha: 0.7598760354908782\n",
      "epoch 202 iteration400, G Loss: 0.9311892986297607, D Loss: 1.257756233215332, alpha: 0.7598760354908782\n",
      "epoch 202 iteration500, G Loss: 0.9399396181106567, D Loss: 1.254270076751709, alpha: 0.7598760354908782\n",
      "epoch 202 iteration600, G Loss: 1.01002037525177, D Loss: 1.2243599891662598, alpha: 0.7598760354908782\n",
      "epoch 202 iteration700, G Loss: 0.558710515499115, D Loss: 1.3558900356292725, alpha: 0.7598760354908782\n",
      "epoch 202 iteration800, G Loss: 0.9804110527038574, D Loss: 1.1077539920806885, alpha: 0.7598760354908782\n",
      "Saving content.\n",
      "epoch 203 iteration0, G Loss: 0.9068918824195862, D Loss: 1.36280357837677, alpha: 0.7554696173492006\n",
      "epoch 203 iteration100, G Loss: 1.195671558380127, D Loss: 1.4279301166534424, alpha: 0.7554696173492006\n",
      "epoch 203 iteration200, G Loss: 0.8570403456687927, D Loss: 1.2931604385375977, alpha: 0.7554696173492006\n",
      "epoch 203 iteration300, G Loss: 1.3814442157745361, D Loss: 1.2121620178222656, alpha: 0.7554696173492006\n",
      "epoch 203 iteration400, G Loss: 1.0548036098480225, D Loss: 1.2971880435943604, alpha: 0.7554696173492006\n",
      "epoch 203 iteration500, G Loss: 1.0507093667984009, D Loss: 1.1964659690856934, alpha: 0.7554696173492006\n",
      "epoch 203 iteration600, G Loss: 1.0408464670181274, D Loss: 1.1895923614501953, alpha: 0.7554696173492006\n",
      "epoch 203 iteration700, G Loss: 0.7634135484695435, D Loss: 1.1153837442398071, alpha: 0.7554696173492006\n",
      "epoch 203 iteration800, G Loss: 1.2221933603286743, D Loss: 1.4855462312698364, alpha: 0.7554696173492006\n",
      "Saving content.\n",
      "epoch 204 iteration0, G Loss: 0.7017930150032043, D Loss: 1.1885111331939697, alpha: 0.7510088346069963\n",
      "epoch 204 iteration100, G Loss: 1.244417428970337, D Loss: 1.3002424240112305, alpha: 0.7510088346069963\n",
      "epoch 204 iteration200, G Loss: 0.9432548880577087, D Loss: 1.2217700481414795, alpha: 0.7510088346069963\n",
      "epoch 204 iteration300, G Loss: 0.8047268390655518, D Loss: 1.2003718614578247, alpha: 0.7510088346069963\n",
      "epoch 204 iteration400, G Loss: 0.9496709704399109, D Loss: 1.2879729270935059, alpha: 0.7510088346069963\n",
      "epoch 204 iteration500, G Loss: 0.9592506289482117, D Loss: 1.2529890537261963, alpha: 0.7510088346069963\n",
      "epoch 204 iteration600, G Loss: 1.3345763683319092, D Loss: 1.1678380966186523, alpha: 0.7510088346069963\n",
      "epoch 204 iteration700, G Loss: 0.768272876739502, D Loss: 1.2626489400863647, alpha: 0.7510088346069963\n",
      "epoch 204 iteration800, G Loss: 0.9583516716957092, D Loss: 1.1763882637023926, alpha: 0.7510088346069963\n",
      "Saving content.\n",
      "epoch 205 iteration0, G Loss: 0.8996418714523315, D Loss: 1.257908582687378, alpha: 0.7464939833376623\n",
      "epoch 205 iteration100, G Loss: 0.9604171514511108, D Loss: 1.174919843673706, alpha: 0.7464939833376623\n",
      "epoch 205 iteration200, G Loss: 1.599869966506958, D Loss: 1.1792683601379395, alpha: 0.7464939833376623\n",
      "epoch 205 iteration300, G Loss: 0.9809041023254395, D Loss: 1.2307484149932861, alpha: 0.7464939833376623\n",
      "epoch 205 iteration400, G Loss: 0.9013792872428894, D Loss: 1.2699733972549438, alpha: 0.7464939833376623\n",
      "epoch 205 iteration500, G Loss: 0.7660967111587524, D Loss: 1.2451517581939697, alpha: 0.7464939833376623\n",
      "epoch 205 iteration600, G Loss: 0.8761277794837952, D Loss: 1.1784491539001465, alpha: 0.7464939833376623\n",
      "epoch 205 iteration700, G Loss: 0.8966036438941956, D Loss: 1.1648855209350586, alpha: 0.7464939833376623\n",
      "epoch 205 iteration800, G Loss: 1.37418794631958, D Loss: 1.2459015846252441, alpha: 0.7464939833376623\n",
      "Saving content.\n",
      "epoch 206 iteration0, G Loss: 1.2198455333709717, D Loss: 1.1794286966323853, alpha: 0.7419253983502743\n",
      "epoch 206 iteration100, G Loss: 1.1808953285217285, D Loss: 1.2224833965301514, alpha: 0.7419253983502743\n",
      "epoch 206 iteration200, G Loss: 0.874545693397522, D Loss: 1.2061233520507812, alpha: 0.7419253983502743\n",
      "epoch 206 iteration300, G Loss: 0.7415415644645691, D Loss: 1.3375321626663208, alpha: 0.7419253983502743\n",
      "epoch 206 iteration400, G Loss: 0.8732825517654419, D Loss: 1.1876168251037598, alpha: 0.7419253983502743\n",
      "epoch 206 iteration500, G Loss: 0.7694858312606812, D Loss: 1.2542695999145508, alpha: 0.7419253983502743\n",
      "epoch 206 iteration600, G Loss: 0.8254735469818115, D Loss: 1.6645933389663696, alpha: 0.7419253983502743\n",
      "epoch 206 iteration700, G Loss: 0.9168493151664734, D Loss: 1.1924972534179688, alpha: 0.7419253983502743\n",
      "epoch 206 iteration800, G Loss: 0.9244279265403748, D Loss: 1.2113232612609863, alpha: 0.7419253983502743\n",
      "Saving content.\n",
      "epoch 207 iteration0, G Loss: 0.9640312194824219, D Loss: 1.4214826822280884, alpha: 0.7373034537835592\n",
      "epoch 207 iteration100, G Loss: 0.993483304977417, D Loss: 1.1596183776855469, alpha: 0.7373034537835592\n",
      "epoch 207 iteration200, G Loss: 1.1250900030136108, D Loss: 1.2629245519638062, alpha: 0.7373034537835592\n",
      "epoch 207 iteration300, G Loss: 0.9484935402870178, D Loss: 1.2544667720794678, alpha: 0.7373034537835592\n",
      "epoch 207 iteration400, G Loss: 0.8818445801734924, D Loss: 1.1658055782318115, alpha: 0.7373034537835592\n",
      "epoch 207 iteration500, G Loss: 0.8788231611251831, D Loss: 1.2523144483566284, alpha: 0.7373034537835592\n",
      "epoch 207 iteration600, G Loss: 0.7863264083862305, D Loss: 1.2483868598937988, alpha: 0.7373034537835592\n",
      "epoch 207 iteration700, G Loss: 0.9031235575675964, D Loss: 1.2640806436538696, alpha: 0.7373034537835592\n",
      "epoch 207 iteration800, G Loss: 0.7412723302841187, D Loss: 1.233695387840271, alpha: 0.7373034537835592\n",
      "Saving content.\n",
      "epoch 208 iteration0, G Loss: 0.9597561955451965, D Loss: 1.0988409519195557, alpha: 0.7326285636546261\n",
      "epoch 208 iteration100, G Loss: 0.8823880553245544, D Loss: 1.1751585006713867, alpha: 0.7326285636546261\n",
      "epoch 208 iteration200, G Loss: 1.1905022859573364, D Loss: 1.1560075283050537, alpha: 0.7326285636546261\n",
      "epoch 208 iteration300, G Loss: 0.8761566877365112, D Loss: 1.2193944454193115, alpha: 0.7326285636546261\n",
      "epoch 208 iteration400, G Loss: 0.8848634958267212, D Loss: 1.2041800022125244, alpha: 0.7326285636546261\n",
      "epoch 208 iteration500, G Loss: 1.2281380891799927, D Loss: 1.2385878562927246, alpha: 0.7326285636546261\n",
      "epoch 208 iteration600, G Loss: 0.9858300685882568, D Loss: 1.3136351108551025, alpha: 0.7326285636546261\n",
      "epoch 208 iteration700, G Loss: 0.9845285415649414, D Loss: 1.3505709171295166, alpha: 0.7326285636546261\n",
      "epoch 208 iteration800, G Loss: 1.1418497562408447, D Loss: 1.1674280166625977, alpha: 0.7326285636546261\n",
      "Saving content.\n",
      "epoch 209 iteration0, G Loss: 1.0363287925720215, D Loss: 1.1694276332855225, alpha: 0.7279011823597308\n",
      "epoch 209 iteration100, G Loss: 1.1937097311019897, D Loss: 1.2687420845031738, alpha: 0.7279011823597308\n",
      "epoch 209 iteration200, G Loss: 1.100663423538208, D Loss: 1.2180461883544922, alpha: 0.7279011823597308\n",
      "epoch 209 iteration300, G Loss: 1.074916124343872, D Loss: 1.256462812423706, alpha: 0.7279011823597308\n",
      "epoch 209 iteration400, G Loss: 1.1017171144485474, D Loss: 1.2173881530761719, alpha: 0.7279011823597308\n",
      "epoch 209 iteration500, G Loss: 1.113120675086975, D Loss: 1.219003438949585, alpha: 0.7279011823597308\n",
      "epoch 209 iteration600, G Loss: 0.8246095776557922, D Loss: 1.2231342792510986, alpha: 0.7279011823597308\n",
      "epoch 209 iteration700, G Loss: 1.2103053331375122, D Loss: 1.2917691469192505, alpha: 0.7279011823597308\n",
      "epoch 209 iteration800, G Loss: 0.5154203176498413, D Loss: 1.3273072242736816, alpha: 0.7279011823597308\n",
      "Saving content.\n",
      "epoch 210 iteration0, G Loss: 1.0905946493148804, D Loss: 1.2039437294006348, alpha: 0.7231218051243897\n",
      "epoch 210 iteration100, G Loss: 1.125137209892273, D Loss: 1.1905226707458496, alpha: 0.7231218051243897\n",
      "epoch 210 iteration200, G Loss: 1.3249478340148926, D Loss: 1.2242004871368408, alpha: 0.7231218051243897\n",
      "epoch 210 iteration300, G Loss: 0.8786382079124451, D Loss: 1.22042715549469, alpha: 0.7231218051243897\n",
      "epoch 210 iteration400, G Loss: 0.9883032441139221, D Loss: 1.1655384302139282, alpha: 0.7231218051243897\n",
      "epoch 210 iteration500, G Loss: 1.3487110137939453, D Loss: 1.27178955078125, alpha: 0.7231218051243897\n",
      "epoch 210 iteration600, G Loss: 1.0753209590911865, D Loss: 1.1118944883346558, alpha: 0.7231218051243897\n",
      "epoch 210 iteration700, G Loss: 0.9508227109909058, D Loss: 1.5098042488098145, alpha: 0.7231218051243897\n",
      "epoch 210 iteration800, G Loss: 0.7766849994659424, D Loss: 1.2911779880523682, alpha: 0.7231218051243897\n",
      "Saving content.\n",
      "epoch 211 iteration0, G Loss: 0.9276764988899231, D Loss: 1.2668240070343018, alpha: 0.7182909684002099\n",
      "epoch 211 iteration100, G Loss: 1.1957987546920776, D Loss: 1.162164330482483, alpha: 0.7182909684002099\n",
      "epoch 211 iteration200, G Loss: 0.9812607765197754, D Loss: 1.0878691673278809, alpha: 0.7182909684002099\n",
      "epoch 211 iteration300, G Loss: 1.2304884195327759, D Loss: 1.1735246181488037, alpha: 0.7182909684002099\n",
      "epoch 211 iteration400, G Loss: 0.894967257976532, D Loss: 1.2595276832580566, alpha: 0.7182909684002099\n",
      "epoch 211 iteration500, G Loss: 1.0163843631744385, D Loss: 1.2125320434570312, alpha: 0.7182909684002099\n",
      "epoch 211 iteration600, G Loss: 1.0831974744796753, D Loss: 1.2440943717956543, alpha: 0.7182909684002099\n",
      "epoch 211 iteration700, G Loss: 1.169630765914917, D Loss: 1.273517370223999, alpha: 0.7182909684002099\n",
      "epoch 211 iteration800, G Loss: 1.038591980934143, D Loss: 1.160165786743164, alpha: 0.7182909684002099\n",
      "Saving content.\n",
      "epoch 212 iteration0, G Loss: 0.9949625730514526, D Loss: 1.2934668064117432, alpha: 0.7134092502058658\n",
      "epoch 212 iteration100, G Loss: 1.2022708654403687, D Loss: 1.2822608947753906, alpha: 0.7134092502058658\n",
      "epoch 212 iteration200, G Loss: 1.032875657081604, D Loss: 1.1885266304016113, alpha: 0.7134092502058658\n",
      "epoch 212 iteration300, G Loss: 0.9143849015235901, D Loss: 1.2574225664138794, alpha: 0.7134092502058658\n",
      "epoch 212 iteration400, G Loss: 0.749333381652832, D Loss: 1.2227389812469482, alpha: 0.7134092502058658\n",
      "epoch 212 iteration500, G Loss: 1.8543068170547485, D Loss: 1.3794736862182617, alpha: 0.7134092502058658\n",
      "epoch 212 iteration600, G Loss: 0.9883710145950317, D Loss: 1.2573368549346924, alpha: 0.7134092502058658\n",
      "epoch 212 iteration700, G Loss: 0.8383992910385132, D Loss: 1.5115883350372314, alpha: 0.7134092502058658\n",
      "epoch 212 iteration800, G Loss: 1.079206943511963, D Loss: 1.2249197959899902, alpha: 0.7134092502058658\n",
      "Saving content.\n",
      "epoch 213 iteration0, G Loss: 0.7451199293136597, D Loss: 1.200961709022522, alpha: 0.708477270409738\n",
      "epoch 213 iteration100, G Loss: 0.9591752290725708, D Loss: 1.340548038482666, alpha: 0.708477270409738\n",
      "epoch 213 iteration200, G Loss: 1.3545351028442383, D Loss: 1.5000550746917725, alpha: 0.708477270409738\n",
      "epoch 213 iteration300, G Loss: 1.8145262002944946, D Loss: 1.4464422464370728, alpha: 0.708477270409738\n",
      "epoch 213 iteration400, G Loss: 1.4249906539916992, D Loss: 1.116477608680725, alpha: 0.708477270409738\n",
      "epoch 213 iteration500, G Loss: 0.9078994393348694, D Loss: 1.127894639968872, alpha: 0.708477270409738\n",
      "epoch 213 iteration600, G Loss: 1.366763710975647, D Loss: 1.3955507278442383, alpha: 0.708477270409738\n",
      "epoch 213 iteration700, G Loss: 1.0540974140167236, D Loss: 1.2203795909881592, alpha: 0.708477270409738\n",
      "epoch 213 iteration800, G Loss: 1.0039961338043213, D Loss: 1.2879881858825684, alpha: 0.708477270409738\n",
      "Saving content.\n",
      "epoch 214 iteration0, G Loss: 1.0110337734222412, D Loss: 1.2428185939788818, alpha: 0.7034956909518111\n",
      "epoch 214 iteration100, G Loss: 1.0965244770050049, D Loss: 1.2494885921478271, alpha: 0.7034956909518111\n",
      "epoch 214 iteration200, G Loss: 1.0693626403808594, D Loss: 1.3010923862457275, alpha: 0.7034956909518111\n",
      "epoch 214 iteration300, G Loss: 0.9551323056221008, D Loss: 1.2138220071792603, alpha: 0.7034956909518111\n",
      "epoch 214 iteration400, G Loss: 0.8785179257392883, D Loss: 1.1442631483078003, alpha: 0.7034956909518111\n",
      "epoch 214 iteration500, G Loss: 1.1364694833755493, D Loss: 1.2769831418991089, alpha: 0.7034956909518111\n",
      "epoch 214 iteration600, G Loss: 0.9662777781486511, D Loss: 1.1268126964569092, alpha: 0.7034956909518111\n",
      "epoch 214 iteration700, G Loss: 0.7408691644668579, D Loss: 1.3307147026062012, alpha: 0.7034956909518111\n",
      "epoch 214 iteration800, G Loss: 0.8232011198997498, D Loss: 1.1996158361434937, alpha: 0.7034956909518111\n",
      "Saving content.\n",
      "epoch 215 iteration0, G Loss: 1.4412810802459717, D Loss: 1.2749544382095337, alpha: 0.6984652160025386\n",
      "epoch 215 iteration100, G Loss: 0.8799416422843933, D Loss: 1.2082191705703735, alpha: 0.6984652160025386\n",
      "epoch 215 iteration200, G Loss: 0.9534512758255005, D Loss: 1.2458301782608032, alpha: 0.6984652160025386\n",
      "epoch 215 iteration300, G Loss: 0.979057252407074, D Loss: 1.2624284029006958, alpha: 0.6984652160025386\n",
      "epoch 215 iteration400, G Loss: 1.463561773300171, D Loss: 1.342299461364746, alpha: 0.6984652160025386\n",
      "epoch 215 iteration500, G Loss: 0.8438759446144104, D Loss: 1.3839176893234253, alpha: 0.6984652160025386\n",
      "epoch 215 iteration600, G Loss: 1.124546766281128, D Loss: 1.2130181789398193, alpha: 0.6984652160025386\n",
      "epoch 215 iteration700, G Loss: 0.7948898673057556, D Loss: 1.2757484912872314, alpha: 0.6984652160025386\n",
      "epoch 215 iteration800, G Loss: 0.9236161708831787, D Loss: 1.253684401512146, alpha: 0.6984652160025386\n",
      "Saving content.\n",
      "epoch 216 iteration0, G Loss: 0.9782130718231201, D Loss: 1.2514170408248901, alpha: 0.6933865920564957\n",
      "epoch 216 iteration100, G Loss: 0.967793881893158, D Loss: 1.1197587251663208, alpha: 0.6933865920564957\n",
      "epoch 216 iteration200, G Loss: 0.8649468421936035, D Loss: 1.361682653427124, alpha: 0.6933865920564957\n",
      "epoch 216 iteration300, G Loss: 0.7936457991600037, D Loss: 1.2940891981124878, alpha: 0.6933865920564957\n",
      "epoch 216 iteration400, G Loss: 0.9209105372428894, D Loss: 1.185431957244873, alpha: 0.6933865920564957\n",
      "epoch 216 iteration500, G Loss: 0.8422754406929016, D Loss: 1.3316631317138672, alpha: 0.6933865920564957\n",
      "epoch 216 iteration600, G Loss: 0.9262853860855103, D Loss: 1.220950961112976, alpha: 0.6933865920564957\n",
      "epoch 216 iteration700, G Loss: 1.3421015739440918, D Loss: 1.444211483001709, alpha: 0.6933865920564957\n",
      "epoch 216 iteration800, G Loss: 1.1213653087615967, D Loss: 1.1970112323760986, alpha: 0.6933865920564957\n",
      "Saving content.\n",
      "epoch 217 iteration0, G Loss: 0.7066239714622498, D Loss: 1.4583144187927246, alpha: 0.6882606079587694\n",
      "epoch 217 iteration100, G Loss: 0.9207583665847778, D Loss: 1.3082196712493896, alpha: 0.6882606079587694\n",
      "epoch 217 iteration200, G Loss: 1.1073319911956787, D Loss: 1.2314794063568115, alpha: 0.6882606079587694\n",
      "epoch 217 iteration300, G Loss: 1.1281417608261108, D Loss: 1.1416475772857666, alpha: 0.6882606079587694\n",
      "epoch 217 iteration400, G Loss: 0.6547722220420837, D Loss: 1.1774263381958008, alpha: 0.6882606079587694\n",
      "epoch 217 iteration500, G Loss: 1.0006647109985352, D Loss: 1.192676305770874, alpha: 0.6882606079587694\n",
      "epoch 217 iteration600, G Loss: 1.5320065021514893, D Loss: 1.3084475994110107, alpha: 0.6882606079587694\n",
      "epoch 217 iteration700, G Loss: 1.1494078636169434, D Loss: 1.1790928840637207, alpha: 0.6882606079587694\n",
      "epoch 217 iteration800, G Loss: 1.092422366142273, D Loss: 1.106869101524353, alpha: 0.6882606079587694\n",
      "Saving content.\n",
      "epoch 218 iteration0, G Loss: 1.1257632970809937, D Loss: 1.2657427787780762, alpha: 0.683088094862185\n",
      "epoch 218 iteration100, G Loss: 0.7274557948112488, D Loss: 1.4872180223464966, alpha: 0.683088094862185\n",
      "epoch 218 iteration200, G Loss: 1.0653294324874878, D Loss: 1.170181155204773, alpha: 0.683088094862185\n",
      "epoch 218 iteration300, G Loss: 1.2287251949310303, D Loss: 1.2722430229187012, alpha: 0.683088094862185\n",
      "epoch 218 iteration400, G Loss: 0.9430321455001831, D Loss: 1.2064273357391357, alpha: 0.683088094862185\n",
      "epoch 218 iteration500, G Loss: 0.9326309561729431, D Loss: 1.2050871849060059, alpha: 0.683088094862185\n",
      "epoch 218 iteration600, G Loss: 0.911719560623169, D Loss: 1.2259283065795898, alpha: 0.683088094862185\n",
      "epoch 218 iteration700, G Loss: 0.9366084933280945, D Loss: 1.1596977710723877, alpha: 0.683088094862185\n",
      "epoch 218 iteration800, G Loss: 1.0488265752792358, D Loss: 1.2801194190979004, alpha: 0.683088094862185\n",
      "Saving content.\n",
      "epoch 219 iteration0, G Loss: 0.8618844747543335, D Loss: 1.238106369972229, alpha: 0.6778699261136105\n",
      "epoch 219 iteration100, G Loss: 0.9553911685943604, D Loss: 1.2255538702011108, alpha: 0.6778699261136105\n",
      "epoch 219 iteration200, G Loss: 0.8541001081466675, D Loss: 1.1898267269134521, alpha: 0.6778699261136105\n",
      "epoch 219 iteration300, G Loss: 0.9186018109321594, D Loss: 1.1781800985336304, alpha: 0.6778699261136105\n",
      "epoch 219 iteration400, G Loss: 0.7496770024299622, D Loss: 1.1606409549713135, alpha: 0.6778699261136105\n",
      "epoch 219 iteration500, G Loss: 0.8053915500640869, D Loss: 1.226041316986084, alpha: 0.6778699261136105\n",
      "epoch 219 iteration600, G Loss: 0.8618955612182617, D Loss: 1.2869081497192383, alpha: 0.6778699261136105\n",
      "epoch 219 iteration700, G Loss: 0.7158594131469727, D Loss: 1.3660565614700317, alpha: 0.6778699261136105\n",
      "epoch 219 iteration800, G Loss: 0.898564875125885, D Loss: 1.2091834545135498, alpha: 0.6778699261136105\n",
      "Saving content.\n",
      "epoch 220 iteration0, G Loss: 1.270723581314087, D Loss: 1.2375366687774658, alpha: 0.6726070170677604\n",
      "epoch 220 iteration100, G Loss: 1.1143969297409058, D Loss: 1.3138401508331299, alpha: 0.6726070170677604\n",
      "epoch 220 iteration200, G Loss: 1.724915623664856, D Loss: 1.1795144081115723, alpha: 0.6726070170677604\n",
      "epoch 220 iteration300, G Loss: 1.0382943153381348, D Loss: 1.249942421913147, alpha: 0.6726070170677604\n",
      "epoch 220 iteration400, G Loss: 0.7828958630561829, D Loss: 1.2307144403457642, alpha: 0.6726070170677604\n",
      "epoch 220 iteration500, G Loss: 1.2378895282745361, D Loss: 1.176743507385254, alpha: 0.6726070170677604\n",
      "epoch 220 iteration600, G Loss: 1.0601110458374023, D Loss: 1.1372170448303223, alpha: 0.6726070170677604\n",
      "epoch 220 iteration700, G Loss: 1.124284029006958, D Loss: 1.1806557178497314, alpha: 0.6726070170677604\n",
      "epoch 220 iteration800, G Loss: 1.00858473777771, D Loss: 1.2251777648925781, alpha: 0.6726070170677604\n",
      "Saving content.\n",
      "epoch 221 iteration0, G Loss: 0.894282877445221, D Loss: 1.266589879989624, alpha: 0.6673003248270916\n",
      "epoch 221 iteration100, G Loss: 1.1853336095809937, D Loss: 1.0647838115692139, alpha: 0.6673003248270916\n",
      "epoch 221 iteration200, G Loss: 1.2338217496871948, D Loss: 1.20112943649292, alpha: 0.6673003248270916\n",
      "epoch 221 iteration300, G Loss: 1.1766365766525269, D Loss: 1.1751842498779297, alpha: 0.6673003248270916\n",
      "epoch 221 iteration400, G Loss: 1.2439117431640625, D Loss: 1.0708565711975098, alpha: 0.6673003248270916\n",
      "epoch 221 iteration500, G Loss: 1.044333577156067, D Loss: 1.1652154922485352, alpha: 0.6673003248270916\n",
      "epoch 221 iteration600, G Loss: 1.1118676662445068, D Loss: 1.2171745300292969, alpha: 0.6673003248270916\n",
      "epoch 221 iteration700, G Loss: 1.0754995346069336, D Loss: 1.184117317199707, alpha: 0.6673003248270916\n",
      "epoch 221 iteration800, G Loss: 0.8913905620574951, D Loss: 1.279923439025879, alpha: 0.6673003248270916\n",
      "Saving content.\n",
      "epoch 222 iteration0, G Loss: 0.9380190372467041, D Loss: 1.205031156539917, alpha: 0.6619508479065779\n",
      "epoch 222 iteration100, G Loss: 1.1339781284332275, D Loss: 1.244389295578003, alpha: 0.6619508479065779\n",
      "epoch 222 iteration200, G Loss: 0.729381263256073, D Loss: 1.2623753547668457, alpha: 0.6619508479065779\n",
      "epoch 222 iteration300, G Loss: 0.9238296151161194, D Loss: 1.1558864116668701, alpha: 0.6619508479065779\n",
      "epoch 222 iteration400, G Loss: 1.088877558708191, D Loss: 1.1516172885894775, alpha: 0.6619508479065779\n",
      "epoch 222 iteration500, G Loss: 1.0146806240081787, D Loss: 1.0736267566680908, alpha: 0.6619508479065779\n",
      "epoch 222 iteration600, G Loss: 1.3124580383300781, D Loss: 1.2455744743347168, alpha: 0.6619508479065779\n",
      "epoch 222 iteration700, G Loss: 1.0701334476470947, D Loss: 1.1610000133514404, alpha: 0.6619508479065779\n",
      "epoch 222 iteration800, G Loss: 0.8860065340995789, D Loss: 1.1607451438903809, alpha: 0.6619508479065779\n",
      "Saving content.\n",
      "epoch 223 iteration0, G Loss: 0.9594539403915405, D Loss: 1.1757910251617432, alpha: 0.6565596258223503\n",
      "epoch 223 iteration100, G Loss: 0.7656744718551636, D Loss: 1.4072680473327637, alpha: 0.6565596258223503\n",
      "epoch 223 iteration200, G Loss: 0.9961716532707214, D Loss: 1.112998604774475, alpha: 0.6565596258223503\n",
      "epoch 223 iteration300, G Loss: 0.8766718506813049, D Loss: 1.0873674154281616, alpha: 0.6565596258223503\n",
      "epoch 223 iteration400, G Loss: 0.9449995160102844, D Loss: 1.2350493669509888, alpha: 0.6565596258223503\n",
      "epoch 223 iteration500, G Loss: 0.8152269124984741, D Loss: 1.371366024017334, alpha: 0.6565596258223503\n",
      "epoch 223 iteration600, G Loss: 0.9100186228752136, D Loss: 1.215510368347168, alpha: 0.6565596258223503\n",
      "epoch 223 iteration700, G Loss: 1.5911108255386353, D Loss: 1.2494606971740723, alpha: 0.6565596258223503\n",
      "epoch 223 iteration800, G Loss: 0.9168552160263062, D Loss: 1.1635229587554932, alpha: 0.6565596258223503\n",
      "Saving content.\n",
      "epoch 224 iteration0, G Loss: 1.0303233861923218, D Loss: 1.3054475784301758, alpha: 0.6511277386034049\n",
      "epoch 224 iteration100, G Loss: 0.7856177091598511, D Loss: 1.2700862884521484, alpha: 0.6511277386034049\n",
      "epoch 224 iteration200, G Loss: 1.0141957998275757, D Loss: 1.1784098148345947, alpha: 0.6511277386034049\n",
      "epoch 224 iteration300, G Loss: 0.7788946628570557, D Loss: 1.2122033834457397, alpha: 0.6511277386034049\n",
      "epoch 224 iteration400, G Loss: 0.9838290810585022, D Loss: 1.2042062282562256, alpha: 0.6511277386034049\n",
      "epoch 224 iteration500, G Loss: 0.8519988656044006, D Loss: 1.2141295671463013, alpha: 0.6511277386034049\n",
      "epoch 224 iteration600, G Loss: 1.003023386001587, D Loss: 1.1726691722869873, alpha: 0.6511277386034049\n",
      "epoch 224 iteration700, G Loss: 1.1392316818237305, D Loss: 1.314415693283081, alpha: 0.6511277386034049\n",
      "epoch 224 iteration800, G Loss: 0.946727454662323, D Loss: 1.1889128684997559, alpha: 0.6511277386034049\n",
      "Saving content.\n",
      "epoch 225 iteration0, G Loss: 1.13387930393219, D Loss: 1.1638623476028442, alpha: 0.6456563062257954\n",
      "epoch 225 iteration100, G Loss: 1.3141931295394897, D Loss: 1.2100023031234741, alpha: 0.6456563062257954\n",
      "epoch 225 iteration200, G Loss: 0.7732672095298767, D Loss: 1.2369110584259033, alpha: 0.6456563062257954\n",
      "epoch 225 iteration300, G Loss: 1.0952104330062866, D Loss: 1.2445040941238403, alpha: 0.6456563062257954\n",
      "epoch 225 iteration400, G Loss: 1.1680293083190918, D Loss: 1.2850685119628906, alpha: 0.6456563062257954\n",
      "epoch 225 iteration500, G Loss: 1.052331805229187, D Loss: 1.2166377305984497, alpha: 0.6456563062257954\n",
      "epoch 225 iteration600, G Loss: 0.9297052621841431, D Loss: 1.2037622928619385, alpha: 0.6456563062257954\n",
      "epoch 225 iteration700, G Loss: 1.0334089994430542, D Loss: 1.1338797807693481, alpha: 0.6456563062257954\n",
      "epoch 225 iteration800, G Loss: 0.9338819980621338, D Loss: 1.1446973085403442, alpha: 0.6456563062257954\n",
      "Saving content.\n",
      "epoch 226 iteration0, G Loss: 0.8773170113563538, D Loss: 1.1552284955978394, alpha: 0.6401464879689667\n",
      "epoch 226 iteration100, G Loss: 0.9583402872085571, D Loss: 1.1487900018692017, alpha: 0.6401464879689667\n",
      "epoch 226 iteration200, G Loss: 1.0828684568405151, D Loss: 1.2603213787078857, alpha: 0.6401464879689667\n",
      "epoch 226 iteration300, G Loss: 0.7562260031700134, D Loss: 1.3049581050872803, alpha: 0.6401464879689667\n",
      "epoch 226 iteration400, G Loss: 1.052182912826538, D Loss: 1.2189950942993164, alpha: 0.6401464879689667\n",
      "epoch 226 iteration500, G Loss: 0.9105811715126038, D Loss: 1.228771448135376, alpha: 0.6401464879689667\n",
      "epoch 226 iteration600, G Loss: 0.955054521560669, D Loss: 1.16605544090271, alpha: 0.6401464879689667\n",
      "epoch 226 iteration700, G Loss: 1.2148587703704834, D Loss: 1.2525689601898193, alpha: 0.6401464879689667\n",
      "epoch 226 iteration800, G Loss: 1.1587363481521606, D Loss: 1.2002143859863281, alpha: 0.6401464879689667\n",
      "Saving content.\n",
      "epoch 227 iteration0, G Loss: 0.8467592000961304, D Loss: 1.2782928943634033, alpha: 0.6345994816941167\n",
      "epoch 227 iteration100, G Loss: 0.9452967047691345, D Loss: 1.2171880006790161, alpha: 0.6345994816941167\n",
      "epoch 227 iteration200, G Loss: 0.9286919236183167, D Loss: 1.2184255123138428, alpha: 0.6345994816941167\n",
      "epoch 227 iteration300, G Loss: 0.7491517663002014, D Loss: 1.2349408864974976, alpha: 0.6345994816941167\n",
      "epoch 227 iteration400, G Loss: 0.6792742609977722, D Loss: 1.3082637786865234, alpha: 0.6345994816941167\n",
      "epoch 227 iteration500, G Loss: 0.7339978814125061, D Loss: 1.2062132358551025, alpha: 0.6345994816941167\n",
      "epoch 227 iteration600, G Loss: 0.8609915375709534, D Loss: 1.2565340995788574, alpha: 0.6345994816941167\n",
      "epoch 227 iteration700, G Loss: 0.9071145057678223, D Loss: 1.2617602348327637, alpha: 0.6345994816941167\n",
      "epoch 227 iteration800, G Loss: 0.799491822719574, D Loss: 1.1757113933563232, alpha: 0.6345994816941167\n",
      "Saving content.\n",
      "epoch 228 iteration0, G Loss: 0.9314703941345215, D Loss: 1.2090580463409424, alpha: 0.6290165230447224\n",
      "epoch 228 iteration100, G Loss: 1.00814950466156, D Loss: 1.2329277992248535, alpha: 0.6290165230447224\n",
      "epoch 228 iteration200, G Loss: 0.791466236114502, D Loss: 1.2651574611663818, alpha: 0.6290165230447224\n",
      "epoch 228 iteration300, G Loss: 1.0325679779052734, D Loss: 1.1012582778930664, alpha: 0.6290165230447224\n",
      "epoch 228 iteration400, G Loss: 0.5908516049385071, D Loss: 1.32237708568573, alpha: 0.6290165230447224\n",
      "epoch 228 iteration500, G Loss: 0.9391942024230957, D Loss: 1.3248960971832275, alpha: 0.6290165230447224\n",
      "epoch 228 iteration600, G Loss: 1.3680824041366577, D Loss: 1.2354059219360352, alpha: 0.6290165230447224\n",
      "epoch 228 iteration700, G Loss: 0.8874048590660095, D Loss: 1.2263948917388916, alpha: 0.6290165230447224\n",
      "epoch 228 iteration800, G Loss: 0.9189354777336121, D Loss: 1.2430171966552734, alpha: 0.6290165230447224\n",
      "Saving content.\n",
      "epoch 229 iteration0, G Loss: 0.7056624293327332, D Loss: 1.3421168327331543, alpha: 0.6233988845696152\n",
      "epoch 229 iteration100, G Loss: 1.3117047548294067, D Loss: 1.2102404832839966, alpha: 0.6233988845696152\n",
      "epoch 229 iteration200, G Loss: 0.7942260503768921, D Loss: 1.154658317565918, alpha: 0.6233988845696152\n",
      "epoch 229 iteration300, G Loss: 0.6222610473632812, D Loss: 1.2412244081497192, alpha: 0.6233988845696152\n",
      "epoch 229 iteration400, G Loss: 1.1886950731277466, D Loss: 1.1186110973358154, alpha: 0.6233988845696152\n",
      "epoch 229 iteration500, G Loss: 0.7163225412368774, D Loss: 1.5914618968963623, alpha: 0.6233988845696152\n",
      "epoch 229 iteration600, G Loss: 0.9328190088272095, D Loss: 1.2250417470932007, alpha: 0.6233988845696152\n",
      "epoch 229 iteration700, G Loss: 1.2246813774108887, D Loss: 1.4019484519958496, alpha: 0.6233988845696152\n",
      "epoch 229 iteration800, G Loss: 0.9463122487068176, D Loss: 1.162902593612671, alpha: 0.6233988845696152\n",
      "Saving content.\n",
      "epoch 230 iteration0, G Loss: 0.87525475025177, D Loss: 1.1445660591125488, alpha: 0.6177478747692489\n",
      "epoch 230 iteration100, G Loss: 1.1173213720321655, D Loss: 1.1226835250854492, alpha: 0.6177478747692489\n",
      "epoch 230 iteration200, G Loss: 0.9316782355308533, D Loss: 1.0829684734344482, alpha: 0.6177478747692489\n",
      "epoch 230 iteration300, G Loss: 0.990519106388092, D Loss: 1.1292376518249512, alpha: 0.6177478747692489\n",
      "epoch 230 iteration400, G Loss: 1.121111273765564, D Loss: 1.3330367803573608, alpha: 0.6177478747692489\n",
      "epoch 230 iteration500, G Loss: 1.1349114179611206, D Loss: 1.162792682647705, alpha: 0.6177478747692489\n",
      "epoch 230 iteration600, G Loss: 1.1438344717025757, D Loss: 1.2537124156951904, alpha: 0.6177478747692489\n",
      "epoch 230 iteration700, G Loss: 0.9873894453048706, D Loss: 1.2058923244476318, alpha: 0.6177478747692489\n",
      "epoch 230 iteration800, G Loss: 1.2968926429748535, D Loss: 1.1122270822525024, alpha: 0.6177478747692489\n",
      "Saving content.\n",
      "epoch 231 iteration0, G Loss: 1.3358367681503296, D Loss: 1.251117467880249, alpha: 0.6120648370660613\n",
      "epoch 231 iteration100, G Loss: 1.2266143560409546, D Loss: 1.2033803462982178, alpha: 0.6120648370660613\n",
      "epoch 231 iteration200, G Loss: 0.9419599771499634, D Loss: 1.1048859357833862, alpha: 0.6120648370660613\n",
      "epoch 231 iteration300, G Loss: 0.9654263854026794, D Loss: 1.2604050636291504, alpha: 0.6120648370660613\n",
      "epoch 231 iteration400, G Loss: 0.866402268409729, D Loss: 1.2862309217453003, alpha: 0.6120648370660613\n",
      "epoch 231 iteration500, G Loss: 0.7775758504867554, D Loss: 1.295752763748169, alpha: 0.6120648370660613\n",
      "epoch 231 iteration600, G Loss: 0.926393985748291, D Loss: 1.2076849937438965, alpha: 0.6120648370660613\n",
      "epoch 231 iteration700, G Loss: 1.0980300903320312, D Loss: 1.2122235298156738, alpha: 0.6120648370660613\n",
      "epoch 231 iteration800, G Loss: 0.7671718597412109, D Loss: 1.257932424545288, alpha: 0.6120648370660613\n",
      "Saving content.\n",
      "epoch 232 iteration0, G Loss: 0.7677714824676514, D Loss: 1.28855299949646, alpha: 0.6063511487000908\n",
      "epoch 232 iteration100, G Loss: 0.9934507608413696, D Loss: 1.1982643604278564, alpha: 0.6063511487000908\n",
      "epoch 232 iteration200, G Loss: 0.8547405004501343, D Loss: 1.2432870864868164, alpha: 0.6063511487000908\n",
      "epoch 232 iteration300, G Loss: 1.1577486991882324, D Loss: 1.198195219039917, alpha: 0.6063511487000908\n",
      "epoch 232 iteration400, G Loss: 0.7294872999191284, D Loss: 1.3172986507415771, alpha: 0.6063511487000908\n",
      "epoch 232 iteration500, G Loss: 1.2513508796691895, D Loss: 1.3471033573150635, alpha: 0.6063511487000908\n",
      "epoch 232 iteration600, G Loss: 0.9199907779693604, D Loss: 1.2414871454238892, alpha: 0.6063511487000908\n",
      "epoch 232 iteration700, G Loss: 1.052713394165039, D Loss: 1.304155945777893, alpha: 0.6063511487000908\n",
      "epoch 232 iteration800, G Loss: 1.0353916883468628, D Loss: 1.2421839237213135, alpha: 0.6063511487000908\n",
      "Saving content.\n",
      "epoch 233 iteration0, G Loss: 0.9362480640411377, D Loss: 1.1510138511657715, alpha: 0.6006082195512743\n",
      "epoch 233 iteration100, G Loss: 0.9414006471633911, D Loss: 1.2539392709732056, alpha: 0.6006082195512743\n",
      "epoch 233 iteration200, G Loss: 0.7781441807746887, D Loss: 1.2588849067687988, alpha: 0.6006082195512743\n",
      "epoch 233 iteration300, G Loss: 1.0107693672180176, D Loss: 1.250226378440857, alpha: 0.6006082195512743\n",
      "epoch 233 iteration400, G Loss: 0.9870721697807312, D Loss: 1.4091490507125854, alpha: 0.6006082195512743\n",
      "epoch 233 iteration500, G Loss: 0.8842646479606628, D Loss: 1.1806429624557495, alpha: 0.6006082195512743\n",
      "epoch 233 iteration600, G Loss: 0.9503195881843567, D Loss: 1.181560754776001, alpha: 0.6006082195512743\n",
      "epoch 233 iteration700, G Loss: 0.8451589941978455, D Loss: 1.3328850269317627, alpha: 0.6006082195512743\n",
      "epoch 233 iteration800, G Loss: 0.8053225874900818, D Loss: 1.1599369049072266, alpha: 0.6006082195512743\n",
      "Saving content.\n",
      "epoch 234 iteration0, G Loss: 0.9816817045211792, D Loss: 1.2224578857421875, alpha: 0.594837490890115\n",
      "epoch 234 iteration100, G Loss: 0.9309481382369995, D Loss: 1.2024755477905273, alpha: 0.594837490890115\n",
      "epoch 234 iteration200, G Loss: 1.0873093605041504, D Loss: 1.252880573272705, alpha: 0.594837490890115\n",
      "epoch 234 iteration300, G Loss: 0.8233414888381958, D Loss: 1.25283682346344, alpha: 0.594837490890115\n",
      "epoch 234 iteration400, G Loss: 0.9823880791664124, D Loss: 1.1162768602371216, alpha: 0.594837490890115\n",
      "epoch 234 iteration500, G Loss: 0.9176835417747498, D Loss: 1.1949892044067383, alpha: 0.594837490890115\n",
      "epoch 234 iteration600, G Loss: 1.0882459878921509, D Loss: 1.2087023258209229, alpha: 0.594837490890115\n",
      "epoch 234 iteration700, G Loss: 1.0049248933792114, D Loss: 1.2014645338058472, alpha: 0.594837490890115\n",
      "epoch 234 iteration800, G Loss: 1.3657310009002686, D Loss: 1.291630506515503, alpha: 0.594837490890115\n",
      "Saving content.\n",
      "epoch 235 iteration0, G Loss: 0.7753154635429382, D Loss: 1.1112278699874878, alpha: 0.5890404340586652\n",
      "epoch 235 iteration100, G Loss: 0.8460568785667419, D Loss: 1.1442906856536865, alpha: 0.5890404340586652\n",
      "epoch 235 iteration200, G Loss: 1.0057814121246338, D Loss: 1.139031171798706, alpha: 0.5890404340586652\n",
      "epoch 235 iteration300, G Loss: 0.8495868444442749, D Loss: 1.1723006963729858, alpha: 0.5890404340586652\n",
      "epoch 235 iteration400, G Loss: 0.739260196685791, D Loss: 1.2247649431228638, alpha: 0.5890404340586652\n",
      "epoch 235 iteration500, G Loss: 1.0813822746276855, D Loss: 1.2729663848876953, alpha: 0.5890404340586652\n",
      "epoch 235 iteration600, G Loss: 0.8083341121673584, D Loss: 1.1453371047973633, alpha: 0.5890404340586652\n",
      "epoch 235 iteration700, G Loss: 0.9123554229736328, D Loss: 1.036537766456604, alpha: 0.5890404340586652\n",
      "epoch 235 iteration800, G Loss: 0.8724104166030884, D Loss: 1.2749173641204834, alpha: 0.5890404340586652\n",
      "Saving content.\n",
      "epoch 236 iteration0, G Loss: 1.0109435319900513, D Loss: 1.311692714691162, alpha: 0.5832185490840334\n",
      "epoch 236 iteration100, G Loss: 1.0953022241592407, D Loss: 1.1609325408935547, alpha: 0.5832185490840334\n",
      "epoch 236 iteration200, G Loss: 1.0101639032363892, D Loss: 1.1815896034240723, alpha: 0.5832185490840334\n",
      "epoch 236 iteration300, G Loss: 0.9694082736968994, D Loss: 1.1975104808807373, alpha: 0.5832185490840334\n",
      "epoch 236 iteration400, G Loss: 1.020738124847412, D Loss: 1.121096134185791, alpha: 0.5832185490840334\n",
      "epoch 236 iteration500, G Loss: 1.0745902061462402, D Loss: 1.307642936706543, alpha: 0.5832185490840334\n",
      "epoch 236 iteration600, G Loss: 0.9619704484939575, D Loss: 1.1059490442276, alpha: 0.5832185490840334\n"
     ]
    }
   ],
   "source": [
    "train(0, 0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3938a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iddgan",
   "language": "python",
   "name": "iddgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
