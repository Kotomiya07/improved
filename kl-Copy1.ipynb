{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e150ff-a647-4d14-b4d5-ab6d46fe22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_args import get_args\n",
    "args = [\"--dataset\", \"cifar10\", \"--exp\", \"kl-f2-3\", \"--num_channels\", \"4\", \"--num_channels_dae\", \"128\", \"--num_timesteps\", \"4\", \n",
    "\t\t\t\"--num_res_blocks\", \"2\", \"--batch_size\", \"256\", \"--num_epoch\", \"2000\", \"--ngf\", \"64\", \"--nz\", \"50\", \"--z_emb_dim\", \"256\", \"--n_mlp\", \"4\", \"--embedding_type\", \"positional\", \n",
    "\t\t\t\"--use_ema\", \"--ema_decay\", \"0.9999\", \"--r1_gamma\", \"0.02\", \"--lr_d\", \"1.0e-4\", \"--lr_g\", \"2.0e-4\", \"--lazy_reg\", \"15\", \n",
    "\t\t\t\"--ch_mult\", \"1\", \"2\", \"2\", \"--save_content\", \"--datadir\", \"./data/cifar-10\", \n",
    "\t\t\t\"--master_port\", \"6084\", \"--num_process_per_node\", \"1\", \"--save_ckpt_every\", \"25\", \n",
    "\t\t\t\"--current_resolution\", \"16\", \"--attn_resolutions\", \"32\", \"--num_disc_layers\", \"3\",  \"--scale_factor\", \"105.0\", \n",
    "\t\t\t\"--no_lr_decay\", \n",
    "            \"--AutoEncoder_config\", \"autoencoder/config/kl-f2.yaml\", \n",
    "            \"--AutoEncoder_ckpt\", \"autoencoder/weight/kl-f2.ckpt\", \n",
    "\t\t\t\"--rec_loss\",\n",
    "\t\t\t\"--sigmoid_learning\", \n",
    "       ]\n",
    "\n",
    "args = get_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414e48d4-68ae-49af-a4ba-86470b9cf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10', 'afhq_cat']:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models',\n",
    "                            os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.sample().detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            wandb.log({\"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch]})\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior.sample())\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103c482e-774c-4f91-a209-aa2e8e6bf643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/std/2021/21k0005/improved-ddgan/wandb/run-20240917_230743-si37tvsr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kotomiya07/TEST/runs/si37tvsr' target=\"_blank\">kl-f2-2</a></strong> to <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kotomiya07/TEST' target=\"_blank\">https://wandb.ai/kotomiya07/TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kotomiya07/TEST/runs/si37tvsr' target=\"_blank\">https://wandb.ai/kotomiya07/TEST/runs/si37tvsr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kotomiya07/TEST/runs/si37tvsr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdcaa99f3a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"TEST\",\n",
    "            name=args.exp,\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"channels_dae\": args.num_channels_dae,\n",
    "                \"ch_nult\": args.ch_mult,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"res_blocks\": args.num_res_blocks,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"embedding_type\": args.embedding_type,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"z_emb_dim\": args.z_emb_dim,\n",
    "                \"attn_resolutions\": args.attn_resolutions,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17debf4-678d-48e4-94c3-015065223e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting in debug mode\n",
      "Files already downloaded and verified\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 4, 16, 16) = 1024 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/users/std/2021/21k0005/improved-ddgan/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "=> loaded checkpoint (epoch 1651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1651 iteration0, G Loss: 1.6408634185791016, D Loss: 1.1352837085723877, alpha: 0.019723961258662492\n",
      "epoch 1651 iteration100, G Loss: 1.1703264713287354, D Loss: 1.2506437301635742, alpha: 0.019723961258662492\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1652 iteration0, G Loss: 1.1393433809280396, D Loss: 1.2534799575805664, alpha: 0.019608285384129065\n",
      "epoch 1652 iteration100, G Loss: 1.3516526222229004, D Loss: 1.08108651638031, alpha: 0.019608285384129065\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1653 iteration0, G Loss: 1.2912336587905884, D Loss: 1.141887903213501, alpha: 0.0194932744278542\n",
      "epoch 1653 iteration100, G Loss: 0.9180908203125, D Loss: 1.2528327703475952, alpha: 0.0194932744278542\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1654 iteration0, G Loss: 1.1937958002090454, D Loss: 1.2514281272888184, alpha: 0.019378924725629187\n",
      "epoch 1654 iteration100, G Loss: 1.1035701036453247, D Loss: 1.1587164402008057, alpha: 0.019378924725629187\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1655 iteration0, G Loss: 1.1264539957046509, D Loss: 1.1803619861602783, alpha: 0.019265232631591744\n",
      "epoch 1655 iteration100, G Loss: 1.1086177825927734, D Loss: 1.1035263538360596, alpha: 0.019265232631591744\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1656 iteration0, G Loss: 0.8042656183242798, D Loss: 1.2045246362686157, alpha: 0.01915219451815342\n",
      "epoch 1656 iteration100, G Loss: 1.275681972503662, D Loss: 1.1994119882583618, alpha: 0.01915219451815342\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1657 iteration0, G Loss: 1.2170531749725342, D Loss: 1.2008330821990967, alpha: 0.019039806775931756\n",
      "epoch 1657 iteration100, G Loss: 0.9766892790794373, D Loss: 1.1397783756256104, alpha: 0.019039806775931756\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1658 iteration0, G Loss: 1.3003432750701904, D Loss: 1.1043180227279663, alpha: 0.018928065813680117\n",
      "epoch 1658 iteration100, G Loss: 1.209255337715149, D Loss: 1.1805250644683838, alpha: 0.018928065813680117\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1659 iteration0, G Loss: 1.0955952405929565, D Loss: 1.097030520439148, alpha: 0.018816968058216532\n",
      "epoch 1659 iteration100, G Loss: 1.1157971620559692, D Loss: 1.1055233478546143, alpha: 0.018816968058216532\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1660 iteration0, G Loss: 1.0125701427459717, D Loss: 1.0929077863693237, alpha: 0.018706509954354522\n",
      "epoch 1660 iteration100, G Loss: 0.9978473782539368, D Loss: 1.3051021099090576, alpha: 0.018706509954354522\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1661 iteration0, G Loss: 1.373602032661438, D Loss: 1.1688803434371948, alpha: 0.018596687964833936\n",
      "epoch 1661 iteration100, G Loss: 0.9864253997802734, D Loss: 1.0836663246154785, alpha: 0.018596687964833936\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1662 iteration0, G Loss: 1.408687949180603, D Loss: 1.1701717376708984, alpha: 0.018487498570249672\n",
      "epoch 1662 iteration100, G Loss: 1.0587455034255981, D Loss: 1.1216238737106323, alpha: 0.018487498570249672\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1663 iteration0, G Loss: 0.9564381837844849, D Loss: 1.1752288341522217, alpha: 0.018378938268982514\n",
      "epoch 1663 iteration100, G Loss: 1.2514135837554932, D Loss: 1.1252808570861816, alpha: 0.018378938268982514\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1664 iteration0, G Loss: 1.4257113933563232, D Loss: 1.2443548440933228, alpha: 0.018271003577129186\n",
      "epoch 1664 iteration100, G Loss: 0.8768638372421265, D Loss: 1.1560415029525757, alpha: 0.018271003577129186\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1665 iteration0, G Loss: 1.2648837566375732, D Loss: 1.1443088054656982, alpha: 0.018163691028432627\n",
      "epoch 1665 iteration100, G Loss: 1.3774499893188477, D Loss: 1.2036893367767334, alpha: 0.018163691028432627\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1666 iteration0, G Loss: 1.2079886198043823, D Loss: 1.053962230682373, alpha: 0.018056997174211276\n",
      "epoch 1666 iteration100, G Loss: 1.3393620252609253, D Loss: 1.2892770767211914, alpha: 0.018056997174211276\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1667 iteration0, G Loss: 1.2211899757385254, D Loss: 1.1274269819259644, alpha: 0.01795091858329123\n",
      "epoch 1667 iteration100, G Loss: 1.0039666891098022, D Loss: 1.1420906782150269, alpha: 0.01795091858329123\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1668 iteration0, G Loss: 0.9742446541786194, D Loss: 1.1280168294906616, alpha: 0.0178454518419342\n",
      "epoch 1668 iteration100, G Loss: 1.3647931814193726, D Loss: 1.162245273590088, alpha: 0.0178454518419342\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1669 iteration0, G Loss: 1.1351829767227173, D Loss: 1.1543388366699219, alpha: 0.017740593553769224\n",
      "epoch 1669 iteration100, G Loss: 1.0448797941207886, D Loss: 1.2354780435562134, alpha: 0.017740593553769224\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1670 iteration0, G Loss: 1.0372471809387207, D Loss: 1.166568398475647, alpha: 0.017636340339722723\n",
      "epoch 1670 iteration100, G Loss: 1.0293102264404297, D Loss: 1.1858373880386353, alpha: 0.017636340339722723\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1671 iteration0, G Loss: 0.9954976439476013, D Loss: 1.1649971008300781, alpha: 0.01753268883794845\n",
      "epoch 1671 iteration100, G Loss: 1.217705249786377, D Loss: 1.1295528411865234, alpha: 0.01753268883794845\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1672 iteration0, G Loss: 1.0623137950897217, D Loss: 1.177269458770752, alpha: 0.01742963570375866\n",
      "epoch 1672 iteration100, G Loss: 0.8005333542823792, D Loss: 1.1298418045043945, alpha: 0.01742963570375866\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1673 iteration0, G Loss: 1.0696016550064087, D Loss: 1.1473026275634766, alpha: 0.017327177609553934\n",
      "epoch 1673 iteration100, G Loss: 1.1163884401321411, D Loss: 1.191713809967041, alpha: 0.017327177609553934\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1674 iteration0, G Loss: 1.2906123399734497, D Loss: 1.1919376850128174, alpha: 0.017225311244753683\n",
      "epoch 1674 iteration100, G Loss: 1.073001503944397, D Loss: 1.1494512557983398, alpha: 0.017225311244753683\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1675 iteration0, G Loss: 1.2235389947891235, D Loss: 1.1058027744293213, alpha: 0.01712403331572765\n",
      "epoch 1675 iteration100, G Loss: 1.0866317749023438, D Loss: 1.1947963237762451, alpha: 0.01712403331572765\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1676 iteration0, G Loss: 0.987602710723877, D Loss: 1.191631555557251, alpha: 0.017023340545725296\n",
      "epoch 1676 iteration100, G Loss: 1.1029081344604492, D Loss: 1.1405504941940308, alpha: 0.017023340545725296\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1677 iteration0, G Loss: 1.1736774444580078, D Loss: 1.1811516284942627, alpha: 0.016923229674807527\n",
      "epoch 1677 iteration100, G Loss: 0.9907183051109314, D Loss: 1.2216753959655762, alpha: 0.016923229674807527\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1678 iteration0, G Loss: 1.259893774986267, D Loss: 1.058828353881836, alpha: 0.016823697459776854\n",
      "epoch 1678 iteration100, G Loss: 1.2801740169525146, D Loss: 1.1838101148605347, alpha: 0.016823697459776854\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1679 iteration0, G Loss: 1.3988711833953857, D Loss: 1.1174243688583374, alpha: 0.01672474067410845\n",
      "epoch 1679 iteration100, G Loss: 1.0408838987350464, D Loss: 1.0998659133911133, alpha: 0.01672474067410845\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1680 iteration0, G Loss: 1.3277729749679565, D Loss: 1.0767126083374023, alpha: 0.016626356107881657\n",
      "epoch 1680 iteration100, G Loss: 0.7997483611106873, D Loss: 1.1265743970870972, alpha: 0.016626356107881657\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1681 iteration0, G Loss: 1.1897871494293213, D Loss: 1.1350080966949463, alpha: 0.016528540567709915\n",
      "epoch 1681 iteration100, G Loss: 1.1142199039459229, D Loss: 1.131528615951538, alpha: 0.016528540567709915\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1682 iteration0, G Loss: 1.3660110235214233, D Loss: 1.1491857767105103, alpha: 0.0164312908766725\n",
      "epoch 1682 iteration100, G Loss: 1.1050657033920288, D Loss: 1.132675290107727, alpha: 0.0164312908766725\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1683 iteration0, G Loss: 1.5152405500411987, D Loss: 1.1169977188110352, alpha: 0.016334603874246345\n",
      "epoch 1683 iteration100, G Loss: 0.8865258693695068, D Loss: 1.1276068687438965, alpha: 0.016334603874246345\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1684 iteration0, G Loss: 1.1688944101333618, D Loss: 1.1586885452270508, alpha: 0.01623847641623566\n",
      "epoch 1684 iteration100, G Loss: 1.0266752243041992, D Loss: 1.1528832912445068, alpha: 0.01623847641623566\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1685 iteration0, G Loss: 1.14212965965271, D Loss: 1.217125654220581, alpha: 0.016142905374705308\n",
      "epoch 1685 iteration100, G Loss: 1.1104645729064941, D Loss: 1.172019600868225, alpha: 0.016142905374705308\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1686 iteration0, G Loss: 1.1904929876327515, D Loss: 1.1292500495910645, alpha: 0.016047887637909986\n",
      "epoch 1686 iteration100, G Loss: 1.2898865938186646, D Loss: 1.1461577415466309, alpha: 0.016047887637909986\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1687 iteration0, G Loss: 1.2182705402374268, D Loss: 1.136540412902832, alpha: 0.015953420110228267\n",
      "epoch 1687 iteration100, G Loss: 1.1130272150039673, D Loss: 1.1454901695251465, alpha: 0.015953420110228267\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1688 iteration0, G Loss: 1.1557564735412598, D Loss: 1.1176005601882935, alpha: 0.015859499712092218\n",
      "epoch 1688 iteration100, G Loss: 1.0433980226516724, D Loss: 1.20076584815979, alpha: 0.015859499712092218\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1689 iteration0, G Loss: 1.1147440671920776, D Loss: 1.2139886617660522, alpha: 0.015766123379920116\n",
      "epoch 1689 iteration100, G Loss: 0.8029956221580505, D Loss: 1.2109359502792358, alpha: 0.015766123379920116\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1690 iteration0, G Loss: 1.3894044160842896, D Loss: 1.1234219074249268, alpha: 0.01567328806604862\n",
      "epoch 1690 iteration100, G Loss: 0.7831586003303528, D Loss: 1.0963671207427979, alpha: 0.01567328806604862\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1691 iteration0, G Loss: 1.2107648849487305, D Loss: 1.1764531135559082, alpha: 0.01558099073866337\n",
      "epoch 1691 iteration100, G Loss: 0.7783493995666504, D Loss: 1.2425518035888672, alpha: 0.01558099073866337\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1692 iteration0, G Loss: 1.3875564336776733, D Loss: 1.1961498260498047, alpha: 0.015489228381733056\n",
      "epoch 1692 iteration100, G Loss: 1.1222983598709106, D Loss: 1.179187297821045, alpha: 0.015489228381733056\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1693 iteration0, G Loss: 1.2974146604537964, D Loss: 1.0891752243041992, alpha: 0.01539799799493946\n",
      "epoch 1693 iteration100, G Loss: 1.142220377922058, D Loss: 1.1845860481262207, alpha: 0.01539799799493946\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1694 iteration0, G Loss: 1.1391459703445435, D Loss: 1.1446163654327393, alpha: 0.015307296593611186\n",
      "epoch 1694 iteration100, G Loss: 0.7307196855545044, D Loss: 1.1547183990478516, alpha: 0.015307296593611186\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1695 iteration0, G Loss: 1.1174938678741455, D Loss: 1.0471630096435547, alpha: 0.01521712120865637\n",
      "epoch 1695 iteration100, G Loss: 0.9447397589683533, D Loss: 1.0996650457382202, alpha: 0.01521712120865637\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1696 iteration0, G Loss: 1.1645193099975586, D Loss: 1.1435941457748413, alpha: 0.015127468886493411\n",
      "epoch 1696 iteration100, G Loss: 1.0479766130447388, D Loss: 1.1900506019592285, alpha: 0.015127468886493411\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1697 iteration0, G Loss: 0.9291878938674927, D Loss: 1.1772515773773193, alpha: 0.015038336688985465\n",
      "epoch 1697 iteration100, G Loss: 1.139835238456726, D Loss: 1.1245779991149902, alpha: 0.015038336688985465\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1698 iteration0, G Loss: 0.973582923412323, D Loss: 1.159675121307373, alpha: 0.01494972169337272\n",
      "epoch 1698 iteration100, G Loss: 0.9415407180786133, D Loss: 1.1397454738616943, alpha: 0.01494972169337272\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1699 iteration0, G Loss: 0.983547568321228, D Loss: 1.2030608654022217, alpha: 0.01486162099220456\n",
      "epoch 1699 iteration100, G Loss: 0.8893378376960754, D Loss: 1.1154475212097168, alpha: 0.01486162099220456\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1700 iteration0, G Loss: 1.1119930744171143, D Loss: 1.1610538959503174, alpha: 0.01477403169327307\n",
      "epoch 1700 iteration100, G Loss: 0.9876257181167603, D Loss: 1.1551411151885986, alpha: 0.01477403169327307\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "Saving content.\n",
      "epoch 1701 iteration0, G Loss: 1.200576901435852, D Loss: 1.1382527351379395, alpha: 0.01468695091954697\n",
      "epoch 1701 iteration100, G Loss: 1.1116942167282104, D Loss: 1.1525886058807373, alpha: 0.01468695091954697\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1702 iteration0, G Loss: 0.7674337029457092, D Loss: 1.258955955505371, alpha: 0.01460037580910356\n",
      "epoch 1702 iteration100, G Loss: 0.8603652119636536, D Loss: 1.2417669296264648, alpha: 0.01460037580910356\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1703 iteration0, G Loss: 1.4303501844406128, D Loss: 1.2301669120788574, alpha: 0.014514303515063554\n",
      "epoch 1703 iteration100, G Loss: 0.7996726036071777, D Loss: 1.213822603225708, alpha: 0.014514303515063554\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1704 iteration0, G Loss: 1.1347898244857788, D Loss: 1.093340516090393, alpha: 0.014428731205522793\n",
      "epoch 1704 iteration100, G Loss: 0.8284000754356384, D Loss: 1.1231014728546143, alpha: 0.014428731205522793\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1705 iteration0, G Loss: 1.3059091567993164, D Loss: 1.1100255250930786, alpha: 0.014343656063488086\n",
      "epoch 1705 iteration100, G Loss: 1.0118231773376465, D Loss: 1.2036006450653076, alpha: 0.014343656063488086\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1706 iteration0, G Loss: 1.2069594860076904, D Loss: 1.102691411972046, alpha: 0.014259075286809586\n",
      "epoch 1706 iteration100, G Loss: 1.184607744216919, D Loss: 1.1583380699157715, alpha: 0.014259075286809586\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1707 iteration0, G Loss: 1.0997611284255981, D Loss: 1.205650806427002, alpha: 0.014174986088115071\n",
      "epoch 1707 iteration100, G Loss: 1.3019510507583618, D Loss: 1.1288241147994995, alpha: 0.014174986088115071\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1708 iteration0, G Loss: 1.32303786277771, D Loss: 1.0938054323196411, alpha: 0.014091385694744774\n",
      "epoch 1708 iteration100, G Loss: 1.1284085512161255, D Loss: 1.1466469764709473, alpha: 0.014091385694744774\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1709 iteration0, G Loss: 1.0182349681854248, D Loss: 1.2705357074737549, alpha: 0.01400827134868421\n",
      "epoch 1709 iteration100, G Loss: 1.2309962511062622, D Loss: 1.172387719154358, alpha: 0.01400827134868421\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1710 iteration0, G Loss: 1.196373701095581, D Loss: 1.170172929763794, alpha: 0.013925640306500453\n",
      "epoch 1710 iteration100, G Loss: 0.9886139631271362, D Loss: 1.11869215965271, alpha: 0.013925640306500453\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1711 iteration0, G Loss: 1.511169672012329, D Loss: 1.2023926973342896, alpha: 0.01384348983927497\n",
      "epoch 1711 iteration100, G Loss: 1.1478859186172485, D Loss: 1.193839192390442, alpha: 0.01384348983927497\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1712 iteration0, G Loss: 1.2664732933044434, D Loss: 1.1264055967330933, alpha: 0.01376181723253933\n",
      "epoch 1712 iteration100, G Loss: 0.9689289331436157, D Loss: 1.159777283668518, alpha: 0.01376181723253933\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1713 iteration0, G Loss: 0.8745846152305603, D Loss: 1.2509052753448486, alpha: 0.013680619786209824\n",
      "epoch 1713 iteration100, G Loss: 1.176209807395935, D Loss: 1.171112298965454, alpha: 0.013680619786209824\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1714 iteration0, G Loss: 1.435178518295288, D Loss: 1.146280288696289, alpha: 0.01359989481452295\n",
      "epoch 1714 iteration100, G Loss: 0.8660106658935547, D Loss: 1.1092352867126465, alpha: 0.01359989481452295\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1715 iteration0, G Loss: 0.8652658462524414, D Loss: 1.1169207096099854, alpha: 0.013519639645969583\n",
      "epoch 1715 iteration100, G Loss: 1.2124849557876587, D Loss: 1.1745944023132324, alpha: 0.013519639645969583\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1716 iteration0, G Loss: 1.2145235538482666, D Loss: 1.1048945188522339, alpha: 0.013439851623231136\n",
      "epoch 1716 iteration100, G Loss: 1.0768234729766846, D Loss: 1.1835498809814453, alpha: 0.013439851623231136\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1717 iteration0, G Loss: 0.8989245295524597, D Loss: 1.1319576501846313, alpha: 0.013360528103114278\n",
      "epoch 1717 iteration100, G Loss: 1.2664307355880737, D Loss: 1.1211872100830078, alpha: 0.013360528103114278\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1718 iteration0, G Loss: 0.9611595273017883, D Loss: 1.0984238386154175, alpha: 0.013281666456487984\n",
      "epoch 1718 iteration100, G Loss: 0.976434051990509, D Loss: 1.1500349044799805, alpha: 0.013281666456487984\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1719 iteration0, G Loss: 1.151479721069336, D Loss: 1.1653594970703125, alpha: 0.013203264068217813\n",
      "epoch 1719 iteration100, G Loss: 1.1903042793273926, D Loss: 1.1703197956085205, alpha: 0.013203264068217813\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1720 iteration0, G Loss: 1.0521527528762817, D Loss: 1.1592986583709717, alpha: 0.013125318337102843\n",
      "epoch 1720 iteration100, G Loss: 1.0489131212234497, D Loss: 1.253357172012329, alpha: 0.013125318337102843\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1721 iteration0, G Loss: 1.0156762599945068, D Loss: 1.1704967021942139, alpha: 0.01304782667581128\n",
      "epoch 1721 iteration100, G Loss: 1.3814040422439575, D Loss: 1.216924786567688, alpha: 0.01304782667581128\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1722 iteration0, G Loss: 1.1518934965133667, D Loss: 1.155043363571167, alpha: 0.012970786510817178\n",
      "epoch 1722 iteration100, G Loss: 1.2618671655654907, D Loss: 1.1489471197128296, alpha: 0.012970786510817178\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1723 iteration0, G Loss: 1.3749184608459473, D Loss: 1.1402220726013184, alpha: 0.012894195282336818\n",
      "epoch 1723 iteration100, G Loss: 1.3570531606674194, D Loss: 1.1789441108703613, alpha: 0.012894195282336818\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1724 iteration0, G Loss: 0.8921447992324829, D Loss: 1.2310360670089722, alpha: 0.012818050444264761\n",
      "epoch 1724 iteration100, G Loss: 1.092200756072998, D Loss: 1.0603108406066895, alpha: 0.012818050444264761\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1725 iteration0, G Loss: 1.1991878747940063, D Loss: 1.138902187347412, alpha: 0.012742349464111569\n",
      "epoch 1725 iteration100, G Loss: 1.063179612159729, D Loss: 1.0699706077575684, alpha: 0.012742349464111569\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1726 iteration0, G Loss: 1.2661808729171753, D Loss: 1.1256048679351807, alpha: 0.012667089822940625\n",
      "epoch 1726 iteration100, G Loss: 0.9449625015258789, D Loss: 1.1801915168762207, alpha: 0.012667089822940625\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1727 iteration0, G Loss: 1.069061040878296, D Loss: 1.2189767360687256, alpha: 0.012592269015304303\n",
      "epoch 1727 iteration100, G Loss: 0.9812734127044678, D Loss: 1.1192338466644287, alpha: 0.012592269015304303\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1728 iteration0, G Loss: 1.0214427709579468, D Loss: 1.1592310667037964, alpha: 0.012517884549182567\n",
      "epoch 1728 iteration100, G Loss: 1.1206482648849487, D Loss: 1.1525285243988037, alpha: 0.012517884549182567\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1729 iteration0, G Loss: 1.094416618347168, D Loss: 1.1362026929855347, alpha: 0.012443933945919694\n",
      "epoch 1729 iteration100, G Loss: 0.8583633899688721, D Loss: 1.2234609127044678, alpha: 0.012443933945919694\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1730 iteration0, G Loss: 1.090245008468628, D Loss: 1.1844565868377686, alpha: 0.012370414740162317\n",
      "epoch 1730 iteration100, G Loss: 0.9382767677307129, D Loss: 1.1223204135894775, alpha: 0.012370414740162317\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1731 iteration0, G Loss: 1.302655577659607, D Loss: 1.1444966793060303, alpha: 0.012297324479797256\n",
      "epoch 1731 iteration100, G Loss: 0.8244996666908264, D Loss: 1.1335117816925049, alpha: 0.012297324479797256\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1732 iteration0, G Loss: 1.0406701564788818, D Loss: 1.1391152143478394, alpha: 0.012224660725888903\n",
      "epoch 1732 iteration100, G Loss: 1.4466651678085327, D Loss: 1.123410701751709, alpha: 0.012224660725888903\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1733 iteration0, G Loss: 1.3092286586761475, D Loss: 1.137755036354065, alpha: 0.0121524210526186\n",
      "epoch 1733 iteration100, G Loss: 0.9272529482841492, D Loss: 1.177507758140564, alpha: 0.0121524210526186\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1734 iteration0, G Loss: 1.1607478857040405, D Loss: 1.066088080406189, alpha: 0.012080603047221694\n",
      "epoch 1734 iteration100, G Loss: 1.2510887384414673, D Loss: 1.174241304397583, alpha: 0.012080603047221694\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1735 iteration0, G Loss: 1.2281231880187988, D Loss: 1.1287367343902588, alpha: 0.012009204309927468\n",
      "epoch 1735 iteration100, G Loss: 1.005925178527832, D Loss: 1.075813889503479, alpha: 0.012009204309927468\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1736 iteration0, G Loss: 1.2988332509994507, D Loss: 1.1881399154663086, alpha: 0.01193822245389664\n",
      "epoch 1736 iteration100, G Loss: 1.2407652139663696, D Loss: 1.2090325355529785, alpha: 0.01193822245389664\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1737 iteration0, G Loss: 1.0211912393569946, D Loss: 1.1108837127685547, alpha: 0.011867655105161412\n",
      "epoch 1737 iteration100, G Loss: 1.1486377716064453, D Loss: 1.174330711364746, alpha: 0.011867655105161412\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1738 iteration0, G Loss: 1.0494167804718018, D Loss: 1.1693809032440186, alpha: 0.011797499902563291\n",
      "epoch 1738 iteration100, G Loss: 0.966180682182312, D Loss: 1.1958673000335693, alpha: 0.011797499902563291\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1739 iteration0, G Loss: 0.9480264186859131, D Loss: 1.1640928983688354, alpha: 0.01172775449769381\n",
      "epoch 1739 iteration100, G Loss: 0.9051570296287537, D Loss: 1.2208324670791626, alpha: 0.01172775449769381\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1740 iteration0, G Loss: 1.120485782623291, D Loss: 1.1391420364379883, alpha: 0.011658416554833018\n",
      "epoch 1740 iteration100, G Loss: 1.0231146812438965, D Loss: 1.1130290031433105, alpha: 0.011658416554833018\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1741 iteration0, G Loss: 1.535959005355835, D Loss: 1.252120018005371, alpha: 0.01158948375088964\n",
      "epoch 1741 iteration100, G Loss: 0.940035343170166, D Loss: 1.1604079008102417, alpha: 0.01158948375088964\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1742 iteration0, G Loss: 1.541251301765442, D Loss: 1.2484564781188965, alpha: 0.011520953775340015\n",
      "epoch 1742 iteration100, G Loss: 1.350174069404602, D Loss: 1.1471474170684814, alpha: 0.011520953775340015\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1743 iteration0, G Loss: 1.3015488386154175, D Loss: 1.1512281894683838, alpha: 0.01145282433017003\n",
      "epoch 1743 iteration100, G Loss: 0.729556679725647, D Loss: 1.1980035305023193, alpha: 0.01145282433017003\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1744 iteration0, G Loss: 1.238754391670227, D Loss: 1.1290791034698486, alpha: 0.011385093129813062\n",
      "epoch 1744 iteration100, G Loss: 0.8625110983848572, D Loss: 1.137727975845337, alpha: 0.011385093129813062\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1745 iteration0, G Loss: 0.9099048972129822, D Loss: 1.1561028957366943, alpha: 0.011317757901092351\n",
      "epoch 1745 iteration100, G Loss: 0.7678463459014893, D Loss: 1.17003333568573, alpha: 0.011317757901092351\n",
      "OUTTTT\n",
      "OUTTTT\n",
      "epoch 1746 iteration0, G Loss: 1.3481770753860474, D Loss: 1.1888586282730103, alpha: 0.011250816383160167\n",
      "epoch 1746 iteration100, G Loss: 0.8436294794082642, D Loss: 1.1624817848205566, alpha: 0.011250816383160167\n"
     ]
    }
   ],
   "source": [
    "print('starting in debug mode')\n",
    "init_processes(0, 1, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e13c6-9f5d-4b5d-8d40-fe862e1f0919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
